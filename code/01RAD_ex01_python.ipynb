{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3ba712",
   "metadata": {},
   "source": [
    "# 01RAD - Exercise 01: Simple Linear Regression\n",
    "\n",
    "This notebook revisits Galton's father-son height data to practise two core ideas from the lecture: small-sample t-tests and the mechanics of fitting a simple linear regression by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec74208",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Refresh exploratory summaries for a bivariate dataset.\n",
    "- Recap one-sample and paired-sample t-tests using both manual formulas and `scipy` helpers.\n",
    "- Derive the simple linear regression coefficients step by step and confirm the equivalence of several computational approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf038a2",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='deep')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446233fc",
   "metadata": {},
   "source": [
    "## 2. Load and inspect the dataset\n",
    "The file `fsdata.csv` contains Galton's measurements (in inches) for 1,078 father?son pairs. We keep a local copy for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fsdata = 'https://raw.githubusercontent.com/francji1/01RAD/main/data/fsdata.csv'\n",
    "fsdata = pd.read_csv(url_fsdata)\n",
    "print(f'Rows: {fsdata.shape[0]}, columns: {fsdata.shape[1]}')\n",
    "fsdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4846d",
   "metadata": {},
   "source": [
    "### Structural overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbc6f1",
   "metadata": {},
   "source": [
    "## 3. Exploring the distributions\n",
    "Visual checks help confirm that the marginal distributions look roughly symmetric and that father and son heights are positively related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e08142",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.histplot(fsdata, x='father', bins=20, color='#1f77b4', kde=True, stat='density', alpha=0.6, label='Father')\n",
    "sns.histplot(fsdata, x='son', bins=20, color='#d62728', kde=True, stat='density', alpha=0.6, label='Son')\n",
    "\n",
    "plt.title('Heights of fathers and sons')\n",
    "plt.xlabel('Height (inches)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87475946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "sns.scatterplot(data=fsdata, x='father', y='son', alpha=0.6)\n",
    "plt.title('Father vs. son height (scatter plot)')\n",
    "plt.xlabel('Father height (inches)')\n",
    "plt.ylabel('Son height (inches)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd3830",
   "metadata": {},
   "source": [
    "## 4. Recap: t-tests\n",
    "We revisit two classical tests covered in the lecture.\n",
    "1. A *paired* t-test to compare father and son means (the observations are naturally paired by family).\n",
    "2. A one-sample t-test to check whether the average son height exceeds 67 inches.\n",
    "\n",
    "Both are shown twice: first via manual formulas, then with `scipy` for confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23bc75",
   "metadata": {},
   "source": [
    "### 4.1 Paired t-test (sons vs fathers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = fsdata['son'] - fsdata['father']\n",
    "mean_diff = diff.mean()\n",
    "sd_diff = diff.std(ddof=1)\n",
    "n = len(diff)\n",
    "\n",
    "standard_error = sd_diff / np.sqrt(n)\n",
    "t_statistic = mean_diff / standard_error\n",
    "degrees_of_freedom = n - 1\n",
    "\n",
    "p_value_two_sided = 2 * (1 - stats.t.cdf(abs(t_statistic), df=degrees_of_freedom))\n",
    "\n",
    "print(f\"Mean difference (son - father): {mean_diff:.3f} inches\")\n",
    "print(f\"t statistic: {t_statistic:.3f}, df: {degrees_of_freedom}\")\n",
    "print(f\"Two-sided p-value: {p_value_two_sided:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2555e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_result = stats.ttest_rel(fsdata['son'], fsdata['father'])\n",
    "print(scipy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b3a28",
   "metadata": {},
   "source": [
    "### 4.2 One-sample t-test (is the mean son height > 67 inches?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 67\n",
    "sons = fsdata['son']\n",
    "mean_son = sons.mean()\n",
    "std_son = sons.std(ddof=1)\n",
    "n = len(sons)\n",
    "\n",
    "standard_error = std_son / np.sqrt(n)\n",
    "t_statistic = (mean_son - mu_0) / standard_error\n",
    "p_value_one_sided = 1 - stats.t.cdf(t_statistic, df=n - 1)\n",
    "\n",
    "print(f\"Sample mean: {mean_son:.3f} inches\")\n",
    "print(f\"t statistic: {t_statistic:.3f}, df: {n - 1}\")\n",
    "print(f\"One-sided p-value (H1: mean > 67): {p_value_one_sided:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_result = stats.ttest_1samp(sons, popmean=mu_0, alternative='greater')\n",
    "print(scipy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d21804",
   "metadata": {},
   "source": [
    "## 5. Manual simple linear regression\n",
    "The lecture derives estimators for the intercept $\\hat{\\beta}_0$ and slope $\\hat{\\beta}_1$ of the simple linear regression model\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i. $$\n",
    "\n",
    "We compute the estimates three different ways and show that they agree:\n",
    "1. Directly from the sums in the normal equations.\n",
    "2. Using covariance/variance relationships.\n",
    "3. Using a linear-algebra formulation (`np.linalg.lstsq`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca457728",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = fsdata['son'].to_numpy()\n",
    "X = fsdata['father'].to_numpy()\n",
    "\n",
    "x_mean = X.mean()\n",
    "y_mean = Y.mean()\n",
    "\n",
    "S_xy = ((X - x_mean) * (Y - y_mean)).sum()\n",
    "S_xx = ((X - x_mean) ** 2).sum()\n",
    "\n",
    "beta1_from_sums = S_xy / S_xx\n",
    "beta0_from_sums = y_mean - beta1_from_sums * x_mean\n",
    "\n",
    "print(f\"Beta1 (slope) from sums: {beta1_from_sums:.4f}\")\n",
    "print(f\"Beta0 (intercept) from sums: {beta0_from_sums:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c856c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance / variance formulation\n",
    "cov_xy = np.cov(X, Y, ddof=1)[0, 1]\n",
    "var_x = np.var(X, ddof=1)\n",
    "\n",
    "beta1_from_cov = cov_xy / var_x\n",
    "beta0_from_cov = y_mean - beta1_from_cov * x_mean\n",
    "\n",
    "print(f\"Beta1 (slope) from covariance: {beta1_from_cov:.4f}\")\n",
    "print(f\"Beta0 (intercept) from covariance: {beta0_from_cov:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra / least squares solution\n",
    "X_design = np.column_stack((np.ones_like(X), X))\n",
    "beta_hat, *_ = np.linalg.lstsq(X_design, Y, rcond=None)\n",
    "\n",
    "beta0_lstsq, beta1_lstsq = beta_hat\n",
    "print(f\"Beta0 from lstsq: {beta0_lstsq:.4f}\")\n",
    "print(f\"Beta1 from lstsq: {beta1_lstsq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'approach': ['normal equations', 'covariance ratio', 'least squares'],\n",
    "    'beta0': [beta0_from_sums, beta0_from_cov, beta0_lstsq],\n",
    "    'beta1': [beta1_from_sums, beta1_from_cov, beta1_lstsq]\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa4394",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Preview: residuals and sampling variability\n",
    "These quantities come up when we study sampling distributions of the OLS estimators. For now just note the formulas; we will unpack their meaning in the next lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535667f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(X)\n",
    "IX = np.column_stack([np.ones(n), X])\n",
    "\n",
    "b0_hat, b1_hat = beta0_from_sums, beta1_from_sums\n",
    "residuals = Y - (b0_hat + b1_hat * X)\n",
    "\n",
    "sigma_hat = np.sqrt(np.sum(residuals ** 2) / (n - 2))\n",
    "S_xx = np.sum((X - x_mean) ** 2)\n",
    "\n",
    "var_b1 = sigma_hat ** 2 / S_xx\n",
    "sd_b1 = np.sqrt(var_b1)\n",
    "\n",
    "var_b0 = sigma_hat ** 2 * (np.sum(X ** 2) / (n * S_xx))\n",
    "sd_b0 = np.sqrt(var_b0)\n",
    "\n",
    "summary = pd.Series({\n",
    "    'sigma_hat (residual sd)': sigma_hat,\n",
    "    'var(b1_hat)': var_b1,\n",
    "    'sd(b1_hat)': sd_b1,\n",
    "    'var(b0_hat)': var_b0,\n",
    "    'sd(b0_hat)': sd_b0\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3d581",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 Preview: solving the normal equations with `numpy.linalg`\n",
    "Here we reproduce the manual slope/intercept using matrix algebra. For the no-intercept model we solve a $1\times 1$ system; for the standard model we solve the $2\times 2$ system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_no_const = X.reshape(-1, 1)\n",
    "\n",
    "beta_hat_no_const = np.linalg.solve(X_no_const.T @ X_no_const, X_no_const.T @ Y)\n",
    "beta_hat_with_const = np.linalg.solve(IX.T @ IX, IX.T @ Y)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'model': ['no intercept', 'with intercept'],\n",
    "    'beta0': [0.0, beta_hat_with_const[0]],\n",
    "    'beta1': [beta_hat_no_const[0], beta_hat_with_const[1]]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d244e9",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3 Preview: `statsmodels` fits (with and without intercept)\n",
    "The `statsmodels` output will be our workhorse for inference later on. For now, focus on how the coefficients change when we force the line through the origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_no_intercept = sm.OLS(Y, X_no_const).fit()\n",
    "model_with_intercept = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "\n",
    "results_preview = pd.DataFrame({\n",
    "    'model': ['no intercept', 'with intercept'],\n",
    "    'beta0': [0.0, model_with_intercept.params[0]],\n",
    "    'beta1': [model_no_intercept.params[0], model_with_intercept.params[1]],\n",
    "    'R_squared': [model_no_intercept.rsquared, model_with_intercept.rsquared]\n",
    "})\n",
    "results_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37958",
   "metadata": {},
   "source": [
    "\n",
    "### 5.4 Preview: comparing regression lines\n",
    "Notice how the forced-zero line (blue) differs from the fitted intercept line (red), especially for shorter fathers. We will discuss the implications in the follow-up session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X, y=Y, alpha=0.5, label='Data')\n",
    "plt.plot(X, X * beta_hat_no_const[0], color='steelblue', label=f'No intercept: y = {beta_hat_no_const[0]:.2f}x')\n",
    "plt.plot(X, model_with_intercept.params[0] + model_with_intercept.params[1] * X,\n",
    "         color='darkred', label=f'With intercept: y = {model_with_intercept.params[0]:.2f} + {model_with_intercept.params[1]:.2f}x')\n",
    "plt.xlabel('Father height (inches)')\n",
    "plt.ylabel('Son height (inches)')\n",
    "plt.title('Comparison of regression lines (preview)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7273c",
   "metadata": {},
   "source": [
    "\n",
    "### Optional challenge: what happens with outliers?\n",
    "Try adding a couple of extreme points and re-running the fits above. How sensitive are the coefficients? (Code scaffold provided if you want to experiment.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ce4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fsdata_outliers = fsdata.copy()\n",
    "outliers = pd.DataFrame({'father': [70, 80], 'son': [10, 20]})\n",
    "fsdata_outliers = pd.concat([fsdata_outliers, outliers], ignore_index=True)\n",
    "\n",
    "model_no_intercept_out = smf.ols('son ~ -1 + father', data=fsdata_outliers).fit()\n",
    "model_with_intercept_out = smf.ols('son ~ father', data=fsdata_outliers).fit()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=fsdata_outliers, x='father', y='son', alpha=0.5)\n",
    "plt.plot(fsdata_outliers['father'], model_no_intercept_out.fittedvalues, color='steelblue', label='No intercept fit')\n",
    "plt.plot(fsdata_outliers['father'], model_with_intercept_out.fittedvalues, color='darkred', label='With intercept fit')\n",
    "plt.xlabel('Father height (inches)')\n",
    "plt.ylabel('Son height (inches)')\n",
    "plt.title('Outlier experiment (optional)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'model': ['no intercept (outliers)', 'with intercept (outliers)'],\n",
    "    'beta0': [0.0, model_with_intercept_out.params[0]],\n",
    "    'beta1': [model_no_intercept_out.params[0], model_with_intercept_out.params[1]]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845c1f4",
   "metadata": {},
   "source": [
    "## 6. What to remember\n",
    "- Paired t-tests operate on the differences; a positive mean difference supporting taller sons translates into a large positive t statistic.\n",
    "- One-sample t-tests compare a sample mean against a benchmark once you set a clear alternative hypothesis.\n",
    "- The simple regression slope can be derived from sums, from covariance/variance, or by solving the normal equations.\n",
    "- Preview take-away: forcing the line through the origin changes the slope appreciably; keep this in mind when we study model specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad7a97",
   "metadata": {},
   "source": [
    "## Homework 01 - Cars braking distance\n",
    "Investigate the relationship between speed and stopping distance using the classic `cars` dataset.\n",
    "\n",
    "1. Load the dataset (available via `statsmodels` or the shared GitHub URL) and inspect the first few rows.\n",
    "2. Plot histograms and density curves for `speed` and `dist`, plus the scatter plot.\n",
    "3. Fit simple linear regression models manually, both with and without an intercept (derive $\\hat{\\beta}_0$, $\\hat{\\beta}_1$).\n",
    "4. Compute the residual sum of squares and the error variance estimate for each model.\n",
    "5. Derive the variances (and standard errors) of the estimated parameters in both models.\n",
    "6. Plot the data together with both fitted regression lines on the same axes.\n",
    "7. Compare the two slopes ? why do they differ? Which model makes more sense for this context?\n",
    "8. Predict stopping distances at 20 mph and 30 mph using both models; discuss whether those predictions are plausible.\n",
    "9. Calculate and plot the residuals for both models. Which residual pattern looks healthier?\n",
    "10. Introduce an artificial outlier (e.g., high distance at low speed) and refit both models. How do the coefficients change?\n",
    "11. Reflect on whether a straight-line model is an adequate description. Suggest at least two possible next steps (transformations, additional variables, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}