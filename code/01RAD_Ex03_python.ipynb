{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNVlJ/OZzhXhYe9E/s92hnK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_Ex03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01RAD Exercise 03"
      ],
      "metadata": {
        "id": "5SL2gaKMqH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last exercise: simple linear regression + different approaches how to add categorical varaible"
      ],
      "metadata": {
        "id": "vmhNKTe8qJPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from statsmodels.datasets import get_rdataset\n",
        "from scipy.stats import t,norm\n",
        "\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "FNwFWm9ArpLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "cars = sns.load_dataset('mpg').dropna()  # Dropping rows with missing values\n",
        "\n",
        "# Check the first few rows\n",
        "print(cars.head())"
      ],
      "metadata": {
        "id": "LGUUC5HarrhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCnUk4WQrmZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLS model: mpg ~ weight (single slope and intercept)\n",
        "model1 = smf.ols('mpg ~ weight', data=cars)\n",
        "results1 = model1.fit()\n",
        "print(results1.summary())\n",
        "\n",
        "\n",
        "# Scatter plot + regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='weight', y='mpg', data=cars, color='blue')\n",
        "plt.plot(cars['weight'], results1.fittedvalues, color='red', label='Regression line')\n",
        "\n",
        "plt.title('Simple Linear Regression (mpg ~ weight)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TE08YXhZruvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLS model: mpg ~ weight + origin (three intercepts, one slope)\n",
        "model2 = smf.ols('mpg ~ weight + C(origin)', data=cars)\n",
        "results2 = model2.fit()\n",
        "print(results2.summary())\n",
        "\n",
        "# Scatter plot with points colored by origin\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = sns.scatterplot(x='weight', y='mpg', hue='origin', data=cars, palette='Set1')\n",
        "\n",
        "# Get the color palette used in the scatterplot\n",
        "palette = dict(zip(cars['origin'].unique(), scatter.legend_.get_texts()))\n",
        "\n",
        "# Plot regression lines for each origin group (same slope, different intercepts)\n",
        "for origin_level in cars['origin'].unique():\n",
        "    subset = cars[cars['origin'] == origin_level]\n",
        "    color = scatter.legend_.get_lines()[list(cars['origin'].unique()).index(origin_level)].get_color()\n",
        "    plt.plot(subset['weight'], results2.predict(subset), label=f'Origin {origin_level}', color=color)\n",
        "\n",
        "plt.title('Multiple Intercepts (mpg ~ weight + origin)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend(title='Origin')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5E3FAlF1sntk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLS model: mpg ~ weight + origin + weight:origin (three intercepts, three slopes)\n",
        "model3 = smf.ols('mpg ~ weight * C(origin)', data=cars)\n",
        "results3 = model3.fit()\n",
        "print(results3.summary())\n",
        "\n",
        "# Scatter plot with points colored by origin\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = sns.scatterplot(x='weight', y='mpg', hue='origin', data=cars, palette='Set1')\n",
        "\n",
        "# Plot regression lines for each origin group (different slopes and intercepts)\n",
        "for origin_level in cars['origin'].unique():\n",
        "    subset = cars[cars['origin'] == origin_level]\n",
        "    color = scatter.legend_.get_lines()[list(cars['origin'].unique()).index(origin_level)].get_color()\n",
        "    plt.plot(subset['weight'], results3.predict(subset), label=f'Origin {origin_level}', color=color)\n",
        "\n",
        "plt.title('Multiple Intercepts and Slopes (mpg ~ weight * origin)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend(title='Origin')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "39B_epW1snv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYTcZHrFvy_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The confidence interval for the mean predicted value** $\\hat{y}_i$ at a given value of $x_i$ is calculated as:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{y}_i)}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\hat{y}_i$ is the predicted value at $x_i$, computed from the regression equation $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$,\n",
        "- $z_{\\alpha/2,}$ is the critical value from the tnormal distribution with (based on the desired confidence level, typically 95%),\n",
        "- $\\text{Var}(\\hat{y}_i)$is the variance of the predicted value $\\hat{y}_i$, given by:\n",
        "\n",
        "$\n",
        "\\text{Var}(\\hat{y}_i) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} \\right)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\sigma^2$ is the residual variance,\n",
        "- $n$ is the number of observations,\n",
        "- $x_i$ is the specific value of the independent variable for which the confidence interval is being calculated,\n",
        "- $\\bar{x}$ is the mean of the independent variable values.\n",
        "\n",
        "\n",
        "**The prediction interval for an individual predicted value** $y_i$ at a given $x_i$ is computed as:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{y}_i) + \\sigma^2}\n",
        "$\n",
        "\n",
        "## Qeuestions:\n",
        "\n",
        "* Is this computation correct?\n",
        "* If so, can I use it in practice?\n",
        "* How can we derive the formula for $\\hat{y}_i$ in general?\n",
        "\n",
        "<!--\n",
        "\n",
        "\n",
        "The confidence interval for the mean predicted value with unknown $\\sigma$:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm t_{\\alpha/2, n-m-1} \\cdot \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} \\right)}\n",
        "$\n",
        "\n",
        "Where the unbiased estimate of the residual variance $\\hat{\\sigma}^2$ in a regression model is given by:\n",
        "\n",
        "$\n",
        "s_n^2 = \\hat{\\sigma}^2 = \\frac{1}{n - m - 1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$\n",
        "\n",
        "$\n",
        "\\text{Var}(\\hat{y}_i) = \\sigma^2 \\cdot \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i\n",
        "$\n",
        "\n",
        "-->\n"
      ],
      "metadata": {
        "id": "GXDihzH2wvMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "B2JmGcTD6M4h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PK3C7naa6L3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new data for weight (for a smooth line plot)\n",
        "weight_range = np.linspace(cars['weight'].min(), cars['weight'].max(), 100)\n",
        "new_data = pd.DataFrame({'weight': weight_range})\n",
        "\n",
        "# Predict the mean mpg and get confidence and prediction intervals\n",
        "predictions = results1.get_prediction(new_data)\n",
        "prediction_summary = predictions.summary_frame(alpha=0.05)  # 95% intervals\n",
        "prediction_summary.head()\n"
      ],
      "metadata": {
        "id": "QKdThixqvzHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Scatter plot of original data\n",
        "sns.scatterplot(x='weight', y='mpg', data=cars, color='blue', label='Data')\n",
        "\n",
        "# Plot the regression line (mean prediction)\n",
        "plt.plot(weight_range, prediction_summary['mean'], color='red', label='Regression line')\n",
        "\n",
        "# Plot the confidence interval\n",
        "plt.fill_between(weight_range,\n",
        "                 prediction_summary['mean_ci_lower'],\n",
        "                 prediction_summary['mean_ci_upper'],\n",
        "                 color='red', alpha=0.3, label='Confidence interval')\n",
        "\n",
        "# Plot the prediction interval\n",
        "plt.fill_between(weight_range,\n",
        "                 prediction_summary['obs_ci_lower'],\n",
        "                 prediction_summary['obs_ci_upper'],\n",
        "                 color='green', alpha=0.2, label='Prediction interval')\n",
        "\n",
        "plt.title('Regression Line with Confidence and Prediction Intervals')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yL2zpz1p6zKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual computation"
      ],
      "metadata": {
        "id": "eDmY7OAB69VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the regression coefficients and residuals from statsmodels\n",
        "intercept, slope = results1.params\n",
        "y_hat = results1.fittedvalues\n",
        "residuals = results1.resid\n",
        "n = len(residuals)\n",
        "\n",
        "# Estimate of sigma^2 (unbiased residual variance) using statsmodels result\n",
        "sigma_squared_hat = results1.mse_resid\n",
        "sigma_hat = np.sqrt(sigma_squared_hat)\n",
        "\n",
        "# Variance-covariance matrix of the coefficients\n",
        "var_beta_hat = results1.cov_params()\n",
        "\n",
        "# Generate new data for weight for smooth line plotting\n",
        "weight_range = np.linspace(cars['weight'].min(), cars['weight'].max(), 100)\n",
        "X_range_with_intercept = sm.add_constant(weight_range)\n",
        "\n",
        "# Predicted mean mpg for the new data (regression line)  - classic way\n",
        "y_hat_range = X_range_with_intercept @ results1.params\n",
        "\n",
        "# You can use predict function instead of manually calculating\n",
        "# y_hat_range = results1.predict(new_data)\n",
        "\n",
        "# Standard error of the predicted mean (for confidence interval)\n",
        "se_mean_prediction = np.sqrt(np.sum(X_range_with_intercept @ var_beta_hat * X_range_with_intercept, axis=1))\n",
        "\n",
        "# Confidence interval (95%)\n",
        "alpha = 0.05\n",
        "t_value = t.ppf(1 - alpha / 2, df=n - 2)  # Critical t-value for 95% confidence interval\n",
        "confidence_interval_lower = y_hat_range - t_value * se_mean_prediction\n",
        "confidence_interval_upper = y_hat_range + t_value * se_mean_prediction\n",
        "\n",
        "# Step 5: Standard error for the prediction interval (includes variance of errors)\n",
        "se_prediction_interval = np.sqrt(se_mean_prediction**2 + sigma_squared_hat)\n",
        "\n",
        "# Prediction interval (95%)\n",
        "prediction_interval_lower = y_hat_range - t_value * se_prediction_interval\n",
        "prediction_interval_upper = y_hat_range + t_value * se_prediction_interval\n",
        "\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Scatter plot of original data\n",
        "sns.scatterplot(x=cars['weight'], y=cars['mpg'], color='blue', label='Data')\n",
        "\n",
        "# Plot the regression line (mean prediction)\n",
        "plt.plot(weight_range, y_hat_range, color='red', label='Regression line')\n",
        "\n",
        "# Plot the confidence interval\n",
        "plt.fill_between(weight_range, confidence_interval_lower, confidence_interval_upper,\n",
        "                 color='red', alpha=0.3, label='Confidence interval')\n",
        "\n",
        "# Plot the prediction interval\n",
        "plt.fill_between(weight_range, prediction_interval_lower, prediction_interval_upper,\n",
        "                 color='green', alpha=0.2, label='Prediction interval')\n",
        "\n",
        "plt.title('Regression Line with Confidence and Prediction Intervals (Manual Calculation)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print estimated sigma and standard errors for reference\n",
        "print(f\"Estimated sigma^2 (residual variance): {sigma_squared_hat}\")\n",
        "print(f\"Estimated sigma (residual standard deviation): {sigma_hat}\")"
      ],
      "metadata": {
        "id": "vaKL8AOJvzJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FCAcx1FvzMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction into multivarible regression"
      ],
      "metadata": {
        "id": "vcX_wCVcuEKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap: $\\hat{\\beta}^{OLS} = argmin_{\\beta \\in \\mathrm{R^p}} \\sum_{i=1}^n (Y_i - X_i \\beta)^2$"
      ],
      "metadata": {
        "id": "NNHGqAO1uKji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Lecture: $\\hat{\\beta}^{OLS}  = (X^TX)^{-1}X^TY$\n",
        "\n",
        "Question\n",
        "* When it holds?\n",
        "* How many solutions do we have?\n",
        "* What should we check and how?\n",
        "* How do we compute $\\hat{\\beta}^{OLS}$ in practice?"
      ],
      "metadata": {
        "id": "Xw-UpWjWuvdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating multivariate data X with an intercept\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Generating random explanatory variables\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Adding intercept (column of ones)\n",
        "X_intercept = np.hstack((np.ones((n_samples, 1)), X))\n",
        "\n",
        "# True coefficients beta (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])\n",
        "\n",
        "# Generating noise epsilon\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# Calculating the response Y\n",
        "Y = X_intercept @ beta_true + epsilon\n",
        "\n",
        "# Applying various methods to estimate coefficients\n",
        "\n",
        "## a) Analytical solution using normal equations\n",
        "beta_hat_norm_eq = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "\n",
        "## b) Using numpy.linalg.lstsq\n",
        "beta_hat_lstsq, residuals, rank, s = np.linalg.lstsq(X_intercept, Y, rcond=None)\n",
        "\n",
        "## c) Linear regression using sklearn\n",
        "model_sk = LinearRegression(fit_intercept=False)\n",
        "model_sk.fit(X_intercept, Y)\n",
        "beta_hat_sk = model_sk.coef_\n",
        "\n",
        "## d) Linear regression using statsmodels\n",
        "model_sm = sm.OLS(Y, X_intercept).fit()\n",
        "beta_hat_sm = model_sm.params\n",
        "\n",
        "# 5. Comparing the results\n",
        "\n",
        "# Creating a DataFrame for comparison\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Normal equations': beta_hat_norm_eq,\n",
        "    'Numpy lstsq': beta_hat_lstsq,\n",
        "    'Sklearn': beta_hat_sk,\n",
        "    'Statsmodels': beta_hat_sm\n",
        "})\n",
        "\n",
        "print(df_results)\n",
        "\n",
        "# Graphical comparison\n",
        "methods = ['Normal equations', 'Numpy lstsq', 'Sklearn', 'Statsmodels']\n",
        "x = np.arange(len(beta_true))  # Indices of coefficients\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, beta_true, 'o-', label='True beta', linewidth=3)\n",
        "for method in methods:\n",
        "    plt.plot(x, df_results[method], 'x--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features+1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Comparison of Estimated Coefficients by Different Methods')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WlwpwBdJuuns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview:\n",
        "\n",
        "* NumPy (`numpy.linalg.lstsq`): Uses Singular Value Decomposition (SVD)\n",
        "\n",
        "* SciPy (`scipy.linalg.lstsq`): Offers methods using QR decomposition and SVD\n",
        "\n",
        "* scikit-learn (`LinearRegression`): Uses SVD via numpy.linalg.lstsq\n",
        "\n",
        "* statsmodels (`OLS`): Uses QR decomposition by default\n",
        "\n",
        "Methods:\n",
        "* **Cholesky Decomposition** $(X^T X = L L^T)$ is efficient but sensitive to data conditions. Use it when you are confident that $(X^TX)$ is positive definite.\n",
        "\n",
        "* **QR Decomposition** ($X = QR$) is a stable method suitable for most linear regression problems, especially when multicollinearity is a concern.\n",
        "\n",
        "* **SVD** ($X = U \\Sigma V^T$ and ($(X^TX)^{-1}X = X^{+}$) provides the most robust solution, particularly in the presence of multicollinearity or rank deficiency, higher computational cost."
      ],
      "metadata": {
        "id": "xQc8zs8u99hu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYOmXjdw91q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generating multivariate data X with an intercept\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Function to generate correlated features\n",
        "def generate_correlated_data(n_samples, n_features, correlation):\n",
        "    \"\"\"\n",
        "    Generates a dataset with specified correlation between features.\n",
        "\n",
        "    Parameters:\n",
        "    - n_samples: Number of samples\n",
        "    - n_features: Number of features (excluding intercept)\n",
        "    - correlation: Desired correlation between features (between 0 and 1)\n",
        "\n",
        "    Returns:\n",
        "    - X: Generated data matrix with an intercept column\n",
        "    \"\"\"\n",
        "    # Create a covariance matrix with the desired correlation\n",
        "    cov_matrix = np.full((n_features, n_features), correlation)\n",
        "    np.fill_diagonal(cov_matrix, 1)\n",
        "\n",
        "    # Generate multivariate normal data\n",
        "    mean = np.zeros(n_features)\n",
        "    X_no_intercept = np.random.multivariate_normal(mean, cov_matrix, size=n_samples)\n",
        "\n",
        "    # Add intercept (column of ones)\n",
        "    X = np.hstack((np.ones((n_samples, 1)), X_no_intercept))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Set the desired correlation level (e.g., 0.9 for high correlation)\n",
        "correlation_level = 0.99\n",
        "\n",
        "# Generate correlated data\n",
        "X = generate_correlated_data(n_samples, n_features, correlation_level)\n",
        "\n",
        "# True coefficients beta (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])  # Length should be n_features + 1\n",
        "\n",
        "# 2. Generating noise epsilon\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# 3. Calculating the response Y\n",
        "Y = X @ beta_true + epsilon\n",
        "\n",
        "# 4. Calculating the condition number of X^T X\n",
        "condition_number = np.linalg.cond(X.T @ X)\n",
        "print(f\"Condition number of X^T X: {condition_number:.2e}\")\n",
        "\n",
        "# 5. Applying various methods to estimate coefficients\n",
        "\n",
        "## a) Analytical solution using normal equations\n",
        "try:\n",
        "    beta_hat_norm_eq = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "except np.linalg.LinAlgError:\n",
        "    beta_hat_norm_eq = np.linalg.pinv(X.T @ X) @ X.T @ Y\n",
        "    print(\"Used pseudo-inverse due to singular matrix in normal equations.\")\n",
        "\n",
        "## b) Using numpy.linalg.lstsq\n",
        "beta_hat_lstsq, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "## c) Linear regression using sklearn\n",
        "model_sk = LinearRegression(fit_intercept=False)\n",
        "model_sk.fit(X, Y)\n",
        "beta_hat_sk = model_sk.coef_\n",
        "\n",
        "## d) Linear regression using statsmodels\n",
        "model_sm = sm.OLS(Y, X).fit()\n",
        "beta_hat_sm = model_sm.params\n",
        "\n",
        "# 6. Comparing the results\n",
        "\n",
        "# Creating a DataFrame for comparison\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Normal equations': beta_hat_norm_eq,\n",
        "    'Numpy lstsq': beta_hat_lstsq,\n",
        "    'Sklearn': beta_hat_sk,\n",
        "    'Statsmodels': beta_hat_sm\n",
        "})\n",
        "\n",
        "print(df_results)\n",
        "\n",
        "# Graphical comparison\n",
        "methods = ['Normal equations', 'Numpy lstsq', 'Sklearn', 'Statsmodels']\n",
        "x = np.arange(len(beta_true))  # Indices of coefficients\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, beta_true, 'o-', label='True beta', linewidth=3)\n",
        "for method in methods:\n",
        "    plt.plot(x, df_results[method], 'x--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features+1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title(f'Comparison of Estimated Coefficients (Correlation={correlation_level})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UT91CGeQuJhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jzFsvQCFd50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of samples and features\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Function to generate correlated features\n",
        "def generate_correlated_data(n_samples, n_features, correlation):\n",
        "    \"\"\"\n",
        "    Generates a dataset with specified correlation between features.\n",
        "\n",
        "    Parameters:\n",
        "    - n_samples: Number of samples\n",
        "    - n_features: Number of features (excluding intercept)\n",
        "    - correlation: Desired correlation between features (between 0 and 1)\n",
        "\n",
        "    Returns:\n",
        "    - X: Generated data matrix with an intercept column\n",
        "    \"\"\"\n",
        "    # Mean and covariance matrix\n",
        "    mean = np.zeros(n_features)\n",
        "    cov = np.full((n_features, n_features), correlation)\n",
        "    np.fill_diagonal(cov, 1)\n",
        "\n",
        "    # Generate multivariate normal data\n",
        "    X_no_intercept = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
        "\n",
        "    # Add intercept (column of ones)\n",
        "    X = np.hstack((np.ones((n_samples, 1)), X_no_intercept))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Generate data with high correlation to test ill-conditioned scenarios\n",
        "correlation_level = 0.99\n",
        "X = generate_correlated_data(n_samples, n_features, correlation_level)\n",
        "\n",
        "# True coefficients (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])  # Length should be n_features + 1\n",
        "\n",
        "# Generate noise\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# Calculate response variable\n",
        "Y = X @ beta_true + epsilon\n"
      ],
      "metadata": {
        "id": "yjB9StveFd85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_cholesky(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using normal equations and Cholesky decomposition.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute X^T X and X^T Y\n",
        "    XtX = X.T @ X\n",
        "    XtY = X.T @ Y\n",
        "\n",
        "    # Perform Cholesky decomposition of XtX\n",
        "    try:\n",
        "        L = np.linalg.cholesky(XtX)\n",
        "    except np.linalg.LinAlgError:\n",
        "        raise np.linalg.LinAlgError(\"Matrix X^T X is not positive definite.\")\n",
        "\n",
        "    # Solve L * z = X^T Y\n",
        "    z = np.linalg.solve(L, XtY)\n",
        "\n",
        "    # Solve L^T * beta_hat = z\n",
        "    beta_hat = np.linalg.solve(L.T, z)\n",
        "\n",
        "    return beta_hat\n"
      ],
      "metadata": {
        "id": "-JZrJwHpFd_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_qr(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using QR decomposition.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the QR decomposition of X\n",
        "    Q, R = np.linalg.qr(X)\n",
        "\n",
        "    # Compute Q^T Y\n",
        "    QtY = Q.T @ Y\n",
        "\n",
        "    # Solve R * beta_hat = Q^T Y\n",
        "    beta_hat = np.linalg.solve(R, QtY)\n",
        "\n",
        "    return beta_hat\n"
      ],
      "metadata": {
        "id": "pGpOpGOUFeCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_svd(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using Singular Value Decomposition (SVD).\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the SVD of X\n",
        "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "    # Compute beta_hat = V * S_inv * U^T * Y\n",
        "    S_inv = np.diag(1 / S)\n",
        "    beta_hat = Vt.T @ S_inv @ U.T @ Y\n",
        "\n",
        "    return beta_hat\n"
      ],
      "metadata": {
        "id": "YNixULy_FeE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_pinv(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using the Moore-Penrose pseudoinverse.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the pseudoinverse of X\n",
        "    X_pinv = np.linalg.pinv(X)\n",
        "\n",
        "    # Compute beta_hat = X_pinv * Y\n",
        "    beta_hat = X_pinv @ Y\n",
        "\n",
        "    return beta_hat\n"
      ],
      "metadata": {
        "id": "mhf8_2qiFeHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate coefficients using Cholesky decomposition\n",
        "try:\n",
        "    beta_cholesky = linear_regression_cholesky(X, Y)\n",
        "except np.linalg.LinAlgError as e:\n",
        "    print(f\"Cholesky method failed: {e}\")\n",
        "    beta_cholesky = np.full(beta_true.shape, np.nan)\n",
        "\n",
        "# Estimate coefficients using QR decomposition\n",
        "beta_qr = linear_regression_qr(X, Y)\n",
        "\n",
        "# Estimate coefficients using SVD\n",
        "beta_svd = linear_regression_svd(X, Y)\n",
        "\n",
        "# Estimate coefficients using the pseudoinverse\n",
        "beta_pinv = linear_regression_pinv(X, Y)\n",
        "\n",
        "# For reference, use NumPy's lstsq method\n",
        "beta_lstsq, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Cholesky': beta_cholesky,\n",
        "    'QR Decomposition': beta_qr,\n",
        "    'SVD': beta_svd,\n",
        "    'Pseudoinverse': beta_pinv,\n",
        "    'NumPy lstsq': beta_lstsq\n",
        "})\n",
        "\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "4-8Aw-QzFeJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQmLaOUjFeMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9majNCOhm1uY"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from numpy.linalg import eigvals, svd, inv, cholesky\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# 1. VIF (Variance Inflation Factor)\n",
        "X_df = pd.DataFrame(X, columns=['Intercept'] + [f'X{i}' for i in range(1, n_features + 1)])\n",
        "vif_data = pd.DataFrame({\n",
        "    'Feature': X_df.columns,\n",
        "    'VIF': [variance_inflation_factor(X_df.values, i) for i in range(X_df.shape[1])]\n",
        "})\n",
        "print(\"VIF:\")\n",
        "print(vif_data)\n",
        "\n",
        "# 2. Condition Number\n",
        "cond_number = np.linalg.cond(X)\n",
        "print(f\"\\nCondition number of X: {cond_number:.2e}\")\n",
        "\n",
        "# 3. Condition Indices\n",
        "def condition_indices(X):\n",
        "    U, s, Vt = svd(X, full_matrices=False)\n",
        "    cond_indices = s[0] / s\n",
        "    return cond_indices\n",
        "\n",
        "cond_indices = condition_indices(X)\n",
        "print(\"\\nCondition Indices:\")\n",
        "for i, ci in enumerate(cond_indices):\n",
        "    print(f\"Index {i+1}: {ci:.2f}\")\n",
        "\n",
        "# 4. Eigenvalues of X^T X\n",
        "eigenvalues = eigvals(X.T @ X)\n",
        "print(\"\\nEigenvalues of X^T X:\")\n",
        "print(eigenvalues.real)\n",
        "\n",
        "# 5. Correlation Matrix\n",
        "corr_matrix = X_df.iloc[:, 1:].corr()\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(corr_matrix)\n",
        "\n",
        "# 6. Normal Equations using Cholesky Decomposition\n",
        "try:\n",
        "    XtX = X.T @ X\n",
        "    XtY = X.T @ Y\n",
        "    L = cholesky(XtX)\n",
        "    z = np.linalg.solve(L, XtY)\n",
        "    beta_cholesky = np.linalg.solve(L.T, z)\n",
        "    print(\"\\nCoefficients using Cholesky Decomposition:\")\n",
        "    print(beta_cholesky)\n",
        "except np.linalg.LinAlgError as e:\n",
        "    print(f\"\\nCholesky decomposition failed: {e}\")\n",
        "\n",
        "# 7. Linear Regression using QR Decomposition\n",
        "def linear_regression_qr(X, Y):\n",
        "    Q, R = np.linalg.qr(X)\n",
        "    QtY = Q.T @ Y\n",
        "    beta_hat = np.linalg.solve(R, QtY)\n",
        "    return beta_hat\n",
        "\n",
        "beta_qr = linear_regression_qr(X, Y)\n",
        "print(\"\\nCoefficients using QR Decomposition:\")\n",
        "print(beta_qr)\n",
        "\n",
        "# 8. Linear Regression using Singular Value Decomposition (SVD)\n",
        "def linear_regression_svd(X, Y):\n",
        "    U, S, Vt = svd(X, full_matrices=False)\n",
        "    S_inv = np.diag(1 / S)\n",
        "    beta_hat = Vt.T @ S_inv @ U.T @ Y\n",
        "    return beta_hat\n",
        "\n",
        "beta_svd = linear_regression_svd(X, Y)\n",
        "print(\"\\nCoefficients using SVD:\")\n",
        "print(beta_svd)\n",
        "\n",
        "# 9. Linear Regression using Pseudoinverse\n",
        "X_pinv = np.linalg.pinv(X)\n",
        "beta_pinv = X_pinv @ Y\n",
        "print(\"\\nCoefficients using Pseudoinverse:\")\n",
        "print(beta_pinv)\n",
        "\n",
        "# 10. Comparing All Methods\n",
        "df_comparison = pd.DataFrame({\n",
        "    'Feature': ['Intercept'] + [f'X{i}' for i in range(1, n_features + 1)],\n",
        "    'True beta': beta_true,\n",
        "    'Cholesky': beta_cholesky if 'beta_cholesky' in locals() else np.nan,\n",
        "    'QR Decomposition': beta_qr,\n",
        "    'SVD': beta_svd,\n",
        "    'Pseudoinverse': beta_pinv,\n",
        "})\n",
        "\n",
        "print(\"\\nComparison of Estimated Coefficients:\")\n",
        "print(df_comparison)\n",
        "\n",
        "# 11. Plotting the Coefficients Comparison\n",
        "plt.figure(figsize=(12, 8))\n",
        "methods = ['True beta', 'Cholesky', 'QR Decomposition', 'SVD', 'Pseudoinverse']\n",
        "x = np.arange(len(beta_true))\n",
        "\n",
        "for method in methods:\n",
        "    plt.plot(x, df_comparison[method], marker='o', linestyle='--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features + 1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Comparison of Estimated Coefficients by Different Methods')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Additional Note: VIF Calculation Details\n",
        "# The VIF for each predictor X_i is calculated as: VIF(X_i) = 1 / (1 - R_i^2)\n",
        "# where R_i^2 is the coefficient of determination obtained by regressing X_i on all other predictors.\n",
        "\n",
        "# Final Note on Numerical Stability\n",
        "# Cholesky decomposition requires X^T X to be positive definite. If not, it will raise an exception.\n",
        "# QR decomposition and SVD are more stable options when dealing with ill-conditioned matrices.\n",
        "# The pseudoinverse method handles any matrix, including rank-deficient ones, but can be computationally expensive.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAe1fzQqm2bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFu5iEIsHofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL-b-_6iIuSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6C83VLsIuVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "ht5fKzGKIuYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hat matrix H"
      ],
      "metadata": {
        "id": "-UyNYcAhLMBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute X^T X\n",
        "XtX = X.T @ X\n",
        "\n",
        "# Calculate the hat matrix H\n",
        "H = X @ np.linalg.inv(XtX) @ X.T\n",
        "\n",
        "print(\"Dimensions of H:\", H.shape)\n",
        "print(\"Dimensions of X:\", X.shape)"
      ],
      "metadata": {
        "id": "hMSkKmQ1LOZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eigenvalues of H\n",
        "eigenvalues = np.linalg.eigvals(H)\n",
        "print(\"Eigenvalues of H:\", np.round(eigenvalues, 10))\n",
        "\n",
        "# Check if H is idempotent\n",
        "idempotent_diff = np.sum(np.round(H @ H - H, 5))\n",
        "print(\"Difference between H^2 and H:\", idempotent_diff)\n",
        "\n",
        "# Check if H is symmetric\n",
        "symmetry_diff = np.round(H.T - H, 10)\n",
        "print(\"Difference between H^T and H:\", symmetry_diff)\n",
        "\n",
        "# Predicted values\n",
        "hat_Y = H @ Y"
      ],
      "metadata": {
        "id": "BcQkei5CLO31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(Y, X)\n",
        "results = model.fit()\n",
        "\n",
        "# Predicted values from statsmodels\n",
        "Y_hat_sm = results.predict(X)\n",
        "# (X @ results.params)\n",
        "\n",
        "\n",
        "# Compare predicted values\n",
        "difference = hat_Y - Y_hat_sm\n",
        "print(\"Difference between predicted values from Hat matrix and statsmodels predict():\")\n",
        "print(np.round(difference, 10).sum())\n",
        "max_difference = np.max(np.abs(difference))\n",
        "print(f\"Maximum absolute difference: {max_difference}\")"
      ],
      "metadata": {
        "id": "9Il72m4HLj9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((X @ results.params).mean())\n",
        "print(results.fittedvalues.mean())\n",
        "print((H @ Y).mean())"
      ],
      "metadata": {
        "id": "JCg0gZ9ZL7j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# M matrix: I - H\n",
        "M = np.identity(H.shape[0]) - H\n",
        "e = (M @ Y)\n",
        "e"
      ],
      "metadata": {
        "id": "rld4a0gSL84k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals computed using M matrix\n",
        "residuals_M = M @ Y\n",
        "\n",
        "# Residuals from statsmodels\n",
        "residuals_statsmodels = results.resid\n",
        "\n",
        "# Compare the two sets of residuals\n",
        "difference = residuals_M - residuals_statsmodels\n",
        "print(\"Difference between residuals from M matrix and statsmodels:\")\n",
        "print(np.round(difference, 10).sum())\n",
        "\n",
        "# Maximum absolute difference\n",
        "max_difference = np.max(np.abs(difference))\n",
        "print(f\"Maximum absolute difference: {max_difference}\")\n"
      ],
      "metadata": {
        "id": "ueuICa-BM1gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_hBFQVYNN40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual student work\n",
        "\n",
        "# **Exercise: Developing a Marketing Plan Based on Advertising Data**\n",
        "\n",
        "Imagine that you are statistical consultants tasked with building a marketing plan for the next year to maximize product sales. You have access to a dataset that contains information on the advertising budget allocated to three different media channels—**TV**, **Radio**, and **Newspaper**—and the corresponding **Sales** figures.\n",
        "\n",
        "## **Dataset Description**\n",
        "\n",
        "- **Variables:**\n",
        "  - **TV**: Advertising budget allocated to TV (in thousands of dollars)\n",
        "  - **Radio**: Advertising budget allocated to Radio (in thousands of dollars)\n",
        "  - **Newspaper**: Advertising budget allocated to Newspaper (in thousands of dollars)\n",
        "  - **Sales**: Product sales (in thousands of units)\n",
        "\n",
        "## **Tasks**\n",
        "\n",
        "Based on this data and your final regression model, answer the following questions:\n",
        "\n",
        "1. **Relationship Between Advertising Budget and Sales**\n",
        "   - Is there a statistically significant relationship between the advertising budget and sales?\n",
        "\n",
        "2. **Contribution of Each Media**\n",
        "   - Do all three media channels—TV, Radio, and Newspaper—contribute to sales?\n",
        "   - Which media have significant effects on sales?\n",
        "\n",
        "3. **Media Generating the Biggest Boost in Sales**\n",
        "   - Which advertising medium generates the largest increase in sales per unit increase in budget?\n",
        "\n",
        "4. **Strength of the Relationship**\n",
        "   - How strong is the relationship between the advertising budget and sales?\n",
        "   - What is the coefficient of determination (R-squared) of your model?\n",
        "\n",
        "5. **Effect of TV Advertising**\n",
        "   - How much increase in sales is associated with a given increase in TV advertising budget?\n",
        "\n",
        "6. **Effect of Radio Advertising**\n",
        "   - How much increase in sales is associated with a given increase in Radio advertising budget?\n",
        "\n",
        "7. **Accuracy of Estimated Effects**\n",
        "   - How accurately can we estimate the effect of each medium on sales?\n",
        "   - Provide the confidence intervals for the coefficients of each medium.\n",
        "\n",
        "8. **Predicting Future Sales**\n",
        "   - How accurately can we predict future sales based on the advertising budgets?\n",
        "   - What is the standard error of the estimate?\n",
        "\n",
        "9. **Optimal Allocation of Advertising Budget**\n",
        "    - Imagine you have a budget of $100,000. What is the best strategy to allocate this budget among TV, Radio, and Newspaper advertising to maximize sales?\n",
        "\n",
        "10. **Predicting Sales for Specific Budget Allocation**\n",
        "    - If you spend $10,000 on TV advertising and $20,000 on Radio advertising, how much increase in sales can you expect?\n",
        "\n",
        "11. **Confidence Interval for Predicted Sales**\n",
        "    - What is the 95% confidence interval for the predicted sales in the previous question?\n",
        "\n",
        "12. **Checking Correlation Between Independent Variables**\n",
        "    - Are there significant correlations between the advertising budgets for different media?\n",
        "    - How might multicollinearity affect your regression model?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6aBkIp1uNSfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "Advert = pd.read_csv(\"https://raw.githubusercontent.com/francji1/01RAD/main/data/Advert.csv\", sep=\",\")\n",
        "Advert.head()"
      ],
      "metadata": {
        "id": "7ROONrvbNUur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raA0HpeBP86k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}