{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_Ex08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression: Diagnostics and Influence Measures\n",
        "\n",
        "In this lecture, we explore:\n",
        "1. **Data Generation**: Simulating a dataset with multiple predictors and a response variable.\n",
        "2. **Regression Modeling**: Fitting a linear regression model using ordinary least squares (OLS).\n",
        "3. **Visualization**: Scatterplots for data exploration and regression diagnostic plots.\n",
        "4. **Diagnostics**: Examining residuals, leverage, and influence measures to evaluate model assumptions and detect outliers or influential observations.\n",
        "\n",
        "The goal is to understand how linear regression assumptions can be validated and how to identify problematic data points.\n"
      ],
      "metadata": {
        "id": "PS4YwqHLLi6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr8RRFaEi5DF"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.graphics.regressionplots import plot_ccpr_grid, plot_partregress_grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed for reproducibility\n",
        "np.random.seed(4242)\n",
        "\n",
        "\n",
        "# Generate data for regression\n",
        "# Sample size and predictors\n",
        "n = 100\n",
        "p = 4\n",
        "\n",
        "# Error term\n",
        "e = np.random.normal(0, 4, n)\n",
        "\n",
        "# Coefficients\n",
        "beta0 = np.array([5, 3, 2, -5]).reshape(4, 1)\n",
        "\n",
        "# Predictors\n",
        "X0 = np.ones(n)\n",
        "X1 = np.random.normal(20, 3, n)\n",
        "X2 = 10 + np.random.exponential(1 / 0.1, n)\n",
        "X3 = 5 + np.random.binomial(15, 0.2, n)\n",
        "\n",
        "# Response variable Y\n",
        "Y = np.dot(np.column_stack((X0, X1, X2, X3)), beta0).flatten() + e\n",
        "\n",
        "# Create DataFrame\n",
        "data0 = pd.DataFrame({'X0': X0, 'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
        "\n",
        "# Selecting the variables\n",
        "X = data0[['X1', 'X2', 'X3']]\n",
        "Y = data0['Y']\n",
        "\n",
        "# Display basic information about the data\n",
        "print(data0.head())\n",
        "print(data0.describe())\n"
      ],
      "metadata": {
        "id": "fZ6J4xeCLruH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoN5OK3ilwmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatterplots for Data Exploration\n",
        "\n",
        "Scatterplots help visualize relationships between predictors and the response variable (`Y`). These plots provide an initial understanding of the data and potential linear relationships.\n"
      ],
      "metadata": {
        "id": "4errL1-sL3ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot for exploratory data visualization\n",
        "sns.pairplot(data0, vars=['X1', 'X2', 'X3', 'Y'], diag_kind='kde')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nu2lpBT1lwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsgFqZ0IlwrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting a Linear Regression Model\n",
        "\n",
        "The dataset includes three predictors (`X1`, `X2`, `X3`) and one response variable (`Y`). We'll fit an ordinary least squares (OLS) regression model to examine the relationship between the predictors and the response.\n"
      ],
      "metadata": {
        "id": "p9H5MNKVMNAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the regression model\n",
        "model = smf.ols('Y ~ X1 + X2 + X3', data=data0).fit()\n",
        "\n",
        "#X = sm.add_constant(X)  # Add an intercept term to the predictor variables\n",
        "#model = sm.OLS(Y, X).fit()\n",
        "\n",
        "# Displaying the summary of the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "2tjG51PHnG-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OohwzqGHo-8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnostic Plots\n",
        "\n",
        "Regression diagnostic plots provide insights into:\n",
        "- **Residual Behavior**: Check for non-linearity, heteroscedasticity, and outliers.\n",
        "- **Normality**: Evaluate the distribution of residuals.\n",
        "- **Influence and Leverage**: Identify observations that disproportionately affect the model.\n",
        "\n",
        "The key plots include:\n",
        "1. Fitted Values vs. Residuals\n",
        "2. Scale-Location Plot (Spread-Location plot)\n",
        "3. Normal Q-Q Plot\n",
        "4. Component-Residual (Partial Residual) Plots\n",
        "5. Added Variable (Partial Regression) Plots\n"
      ],
      "metadata": {
        "id": "_vIQXR5cMhMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regression_diagnostics(model):\n",
        "    \"\"\"\n",
        "    Generate diagnostic plots for a regression model with spline smoothing.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the diagnostic plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    standardized_residuals = model.get_influence().resid_studentized_internal\n",
        "    fitted_values = model.fittedvalues\n",
        "    scale_residuals = np.sqrt(np.abs(standardized_residuals))\n",
        "\n",
        "    # Plot of Fitted Values vs Residuals with LOESS smoothing\n",
        "    plt.subplot(2, 3, 1)\n",
        "    sns.regplot(\n",
        "        x=fitted_values,\n",
        "        y=model.resid,\n",
        "        lowess=True,\n",
        "        scatter_kws={'alpha': 0.7},\n",
        "        line_kws={'color': 'red'}\n",
        "    )\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Fitted Values vs Residuals')\n",
        "\n",
        "\n",
        "    # Scale-Location plot with spline smoothing\n",
        "    plt.subplot(2, 3, 2)\n",
        "    sns.regplot(\n",
        "        x=model.fittedvalues,\n",
        "        y=np.sqrt(np.abs(standardized_residuals)),\n",
        "        lowess=True,\n",
        "        scatter_kws={'alpha': 0.7},\n",
        "        line_kws={'color': 'red'}\n",
        "    )\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel(r'$\\sqrt{|Standardized\\ Residuals|}$')  # LaTeX format\n",
        "    plt.title('Scale-Location')\n",
        "\n",
        "\n",
        "    # Normal Q-Q plot\n",
        "    plt.subplot(2, 3, 3)\n",
        "    sm.qqplot(model.resid, line='s', ax=plt.gca())\n",
        "    plt.title('Normal Q-Q')\n",
        "\n",
        "    # Response vs Residuals for each regressor\n",
        "    for i, col in enumerate(model.model.exog_names[1:], 2):\n",
        "        plt.subplot(2, 3, 2+i)\n",
        "        plt.scatter(model.model.exog[:, i - 1], model.resid, alpha=0.7)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'Response vs Residuals: {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "rLQjo7eUPB3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plot_regression_diagnostics(model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C5BTGVkdTIz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Component-Residual (Partial Residual) Plots\n",
        "\n",
        "Component-Residual Plots (Partial Residual Plots) are a useful diagnostic tool in regression analysis. They help to visualize the relationship between a predictor and the response variable while accounting for the effect of other predictors in the model.\n",
        "\n",
        "- Helps determine if a transformation of a predictor is necessary.\n",
        "- A **linear pattern** in the plot suggests that the relationship between the predictor and response is well-modeled.\n",
        "- Deviations from linearity (e.g., curvature) may indicate that the predictor's relationship with the response is non-linear.\n",
        "- **Outliers or influential points** may appear as points far away from the general pattern.\n",
        "\n"
      ],
      "metadata": {
        "id": "BeuRu-PlUcnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_ccpr_grid\n",
        "\n",
        "def plot_component_residuals(model):\n",
        "    \"\"\"\n",
        "    Generate Component-Residual Plots (Partial Residual Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Component-Residual Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_ccpr_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "3jjzv6P3ogXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig1 = plot_component_residuals(model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SvesRxeUaCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Added Variable Plots\n",
        "Added Variable Plots are a useful diagnostic tool in regression. They help to visualize the contribution of each predictor variable to the response variable after accounting for other predictors.\n",
        "\n",
        "### Key Points:\n",
        "- **Purpose**: Show the partial relationship between a predictor and the response.\n",
        "- **Usage**: Identify whether a variable has a significant relationship with the response after adjusting for others.\n",
        "- **Interpretation**:\n",
        "  - A strong linear pattern indicates a significant relationship.\n",
        "  - Outliers or curvature may indicate a poor model fit or influential points.\n"
      ],
      "metadata": {
        "id": "Qk4l3uDSKTRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
        "\n",
        "def plot_added_variable(model):\n",
        "    \"\"\"\n",
        "    Generate Added Variable Plots (Partial Regression Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Added Variable Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_partregress_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "0MYaaSX_ogZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = plot_added_variable(model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oQrD5rXAoeW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Residuals in Linear Regression (Recap from Ex4)\n",
        "\n",
        "$$\n",
        "Y_i = X_i \\beta + e_i, \\ \\text{where} \\ e_i \\sim N(0, \\sigma^2)\n",
        "$$\n",
        "\n",
        "Residuals measure the difference between observed and predicted values.\n",
        "\n",
        "#### 1. Raw Residuals\n",
        "\n",
        "The raw residuals are simply the differences between each observed value $ Y_i $ and its corresponding predicted value $\\hat{Y}_i $:\n",
        "$$\n",
        "\\hat{e}_i = Y_i - \\hat{Y}_i\n",
        "$$\n",
        "\n",
        "#### 2. Internally Studentized Residuals (unknown sigma)\n",
        "\n",
        "Internally studentized residuals adjust each residual to account for the leverage $ h_{ii} $ of each observation.\n",
        "\n",
        "$$\n",
        "\\hat{r_i} = \\frac{\\hat{e}_i}{s \\sqrt{1 - h_{ii}}}\n",
        "$$\n",
        "\n",
        "and $s^2 = \\hat{\\sigma}^2 = \\frac{1}{n - p}\\sum_{j=1}^n \\hat{e}_j^2 $ is the variance estimate from OLS, using all $n$ observations.\n",
        "\n",
        "\n",
        "Studentized Residuals better reflects the influence of each observation on the fit by normalizing based on individual variances. Internally studentized residuals do not fully assess an observation's influence if removed from the model.\n",
        "\n",
        "#### 3. Externally Studentized Residuals\n",
        "\n",
        "Externally Studentized Residuals $\\hat{r}_{(-i)}$\n",
        " - taking the PRESS residuals, or leave-one-out residuals (the residuals when each observation is left out of the model fit) and dividing by a scaled estimate of their standard deviation.\n",
        "$$\n",
        "\\hat{r}_{(-i)} =  \\frac{\\hat{e}_{(-i)}}{s_{(-i)} \\sqrt{1 - h_{ii}}}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "s_{(-i)} = \\sqrt{\\frac{(n - p - 1)s^2 - \\frac{\\hat{e}_i^2}{1 - h_{ii}}}{n - p - 1}}\n",
        "$$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UQwPxep1ssv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract model details\n",
        "residuals = model.resid  # Classical residuals\n",
        "h_ii = model.get_influence().hat_matrix_diag  # Leverage values (h_ii)\n",
        "n = int(model.nobs)  # Number of observations (ensure it's an integer)\n",
        "p = int(model.df_model)  # Number of predictors\n",
        "mse = model.mse_resid  # Mean squared error (s^2)\n",
        "\n",
        "# 2. Internal Studentized Residuals (matches resid_studentized in statsmodels)\n",
        "s_squared = np.sum(residuals**2) / (n - p - 1)  # OLS variance estimate\n",
        "studentized_residuals_internal = residuals / np.sqrt(s_squared * (1 - h_ii))\n",
        "\n",
        "# 3. External Studentized Residuals (matches resid_studentized_external in statsmodels)\n",
        "studentized_residuals_external = np.zeros_like(residuals)\n",
        "for i in range(n):\n",
        "    # PRESS residuals (leave-one-out residuals)\n",
        "    e_i = residuals[i]\n",
        "    h_ii_i = h_ii[i]\n",
        "\n",
        "    # Leave-one-out standard deviation (s_{(-i)})\n",
        "    s_minus_i = np.sqrt(((n - p - 1) * mse - (e_i**2) / (1 - h_ii_i)) / (n - p - 2))\n",
        "\n",
        "    # Externally studentized residual\n",
        "    studentized_residuals_external[i] = e_i / (s_minus_i * np.sqrt(1 - h_ii_i))\n",
        "\n",
        "# Residuals from statsmodels for comparison\n",
        "model_studentized_residuals_internal = model.get_influence().resid_studentized  # Internal\n",
        "model_studentized_residuals_external = model.get_influence().resid_studentized_external  # External\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "residuals_df = pd.DataFrame({\n",
        "    'Classical Residuals (StatsModels)': residuals,\n",
        "    'Studentized Residuals (Internal - Hand)': studentized_residuals_internal,\n",
        "    'Studentized Residuals (External - Hand)': studentized_residuals_external,\n",
        "    'Studentized Residuals (Internal - StatsModels)': model_studentized_residuals_internal,\n",
        "    'Studentized Residuals (External - StatsModels)': model_studentized_residuals_external\n",
        "})\n",
        "\n",
        "# Display the first few rows\n",
        "residuals_df.head()\n"
      ],
      "metadata": {
        "id": "P3x2bQYmcV3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S2y3o6gOtwm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Influence Measures in Linear Regression\n",
        "\n",
        "In linear regression, influence measures are used to identify observations that have a disproportionate impact on the model. These measures help in diagnosing the model's robustness and identifying outliers or influential points. Below are key influence measures commonly used:\n",
        "\n",
        "### 1. DFBETAS\n",
        "DFBETAS measures the difference in each coefficient estimate when an observation is omitted.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFBETAS_{ij} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{\\hat{\\sigma}^2_{(i)} (X^T X)^{-1}_{jj}}}\n",
        "$$\n",
        "where $ \\hat{\\beta}_j $ is the estimated coefficient, $ \\hat{\\beta}_{j(i)} $ is the estimated coefficient with the \\( i \\)-th observation omitted, and $(X^T X)^{-1}_{jj} $ is the \\( j \\)-th diagonal element of the inverse of $X^T X $.\n",
        "\n",
        "### 2. DFFITS\n",
        "DFFITS is an influence statistic that measures the effect of deleting a single observation.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}_{(i)} \\sqrt{h_{ii}}}\n",
        "$$\n",
        "where $ \\hat{y}_i $ is the predicted value with all observations, $ \\hat{y}_{i(i)}$is the predicted value with the \\( i \\)-th observation omitted, and $ h_{ii} $is the leverage of the $ i $-th observation.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "Leverage values measure the influence of each observation on its own fitted value. High leverage points can significantly alter the position of the regression line.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "h_{ii} = X_i (X^T X)^{-1} X_i^T\n",
        "$$\n",
        "where $X_i$ is the \\( i \\)-th row of the matrix of predictors \\( X \\).\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "Covariance ratios compare the determinants of the covariance matrices with and without each observation. They help identify observations that influence the variance of the parameter estimates.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "CR_i = \\frac{\\det(\\hat{\\Sigma}_{(i)})}{\\det(\\hat{\\Sigma})}\n",
        "$$\n",
        "where $ \\hat{\\Sigma}_{(i)} $is the covariance matrix with the \\( i \\)-th observation omitted and $ \\hat{\\Sigma} $ is the covariance matrix with all observations.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "Cook's distance measures the effect of deleting a single observation on the entire regression model. It is a commonly used metric to identify influential observations.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2}\n",
        "$$\n",
        "where $ \\hat{y}_j $ is the predicted value for the $ j $-th observation, $ \\hat{y}_{j(i)} $ is the predicted value with the \\( i \\)-th observation omitted, \\( p \\) is the number of predictors, and $ \\hat{\\sigma}^2 $ is the estimated variance of the residuals.\n",
        "\n"
      ],
      "metadata": {
        "id": "23HSszBstyFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\n",
        "\n",
        "### 1. DFBETAS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFBETAS for any coefficient exceeds $ \\frac{2}{\\sqrt{n}} $, where $ n $ is the number of observations.\n",
        "\n",
        "### 2. DFFITS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFFITS is larger than $ 2 \\sqrt{\\frac{p+1}{n}} $, where \\( p \\) is the number of predictors and \\( n \\) is the number of observations.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "\n",
        "**Rule of Thumb:** An observation is considered to have high leverage if its leverage value exceeds $ \\frac{2(p+1)}{n} $, where \\( p \\) is the number of predictors and \\( n \\) is the number of observations.\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "\n",
        "**Rule of Thumb:** There is no widely accepted rule of thumb for covariance ratios, but observations with values far from 1 (either much larger or smaller) are generally considered influential.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if its Cook's distance $D_i > \\frac{4}{n} $, where \\( n \\) is the number of observations.\n"
      ],
      "metadata": {
        "id": "DporrqiPuZY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Old version of influence measures data frame\n",
        "def create_influence_dataframe(model):\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extracting the influence measures\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    covariance_ratios = influence.cov_ratio\n",
        "    cooks_distances = influence.cooks_distance[0]\n",
        "\n",
        "    # Creating the DataFrame\n",
        "    influence_df = pd.DataFrame({\n",
        "        'DFFITS': dffits,\n",
        "        'Leverage': leverage,\n",
        "        'Covariance Ratio': covariance_ratios,\n",
        "        'Cook\\'s Distance': cooks_distances\n",
        "    })\n",
        "\n",
        "    # Adding DFBETAS columns for each predictor\n",
        "    for i in range(dfbetas.shape[1]):\n",
        "        influence_df[f'DFBETA_{i}'] = dfbetas[:, i]\n",
        "\n",
        "    return influence_df\n",
        "\n",
        "influence_df = create_influence_dataframe(model)\n",
        "influence_df\n"
      ],
      "metadata": {
        "id": "y7JI4lfrrA8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_influence_measures(model):\n",
        "    \"\"\"\n",
        "    Summarize influence measures and flag observations as potential outliers.\n",
        "\n",
        "    :param model: Fitted regression model object from statsmodels.\n",
        "    :return: DataFrame summarizing influence measures and flagged outliers.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extract measures\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    cooks_distance = influence.cooks_distance[0]\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    cov_ratios = influence.cov_ratio\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n = int(model.nobs)\n",
        "    p = int(model.df_model)\n",
        "\n",
        "    # Rule of Thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Summarize outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        'High Cook\\'s Distance': cooks_distance > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(dfbetas.shape[1]):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        'Cook\\'s Distance': cooks_distance,\n",
        "        'DFFITS': dffits,\n",
        "        'Covariance Ratio': cov_ratios\n",
        "    })\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "summary = summarize_influence_measures(model)\n",
        "summary\n"
      ],
      "metadata": {
        "id": "0JS8y6EwdbhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_influence_measures_with_data(model, data):\n",
        "    \"\"\"\n",
        "    Summarize influence measures, flag outliers, and include original data columns.\n",
        "\n",
        "    :param model: Fitted regression model object from statsmodels.\n",
        "    :param data: DataFrame used to fit the regression model.\n",
        "    :return: DataFrame summarizing influence measures, flagged outliers, and original data.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extract measures\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    cooks_distance = influence.cooks_distance[0]\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    cov_ratios = influence.cov_ratio\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n = int(model.nobs)\n",
        "    p = int(model.df_model)\n",
        "\n",
        "    # Rule of Thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Summarize outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        'High Cook\\'s Distance': cooks_distance > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(dfbetas.shape[1]):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        'Cook\\'s Distance': cooks_distance,\n",
        "        'DFFITS': dffits,\n",
        "        'Covariance Ratio': cov_ratios\n",
        "    })\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    # Combine summary DataFrame with original data\n",
        "    summary_with_data = pd.concat([data.reset_index(drop=True), summary_df], axis=1)\n",
        "\n",
        "    #  Select rows where any flag is True\n",
        "    flagged_observations = summary_with_data.loc[summary_with_data.iloc[:, len(data.columns) + 4:].any(axis=1)]\n",
        "    return summary_with_data, flagged_observations\n",
        "\n",
        "# Example usage:\n",
        "all_observations_with_im, flagged_observations = summarize_influence_measures_with_data(model, data0)\n",
        "flagged_observations\n"
      ],
      "metadata": {
        "id": "hmAtUv42eYJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_leverage(model):\n",
        "    X = model.model.exog  # Extract design matrix\n",
        "    H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
        "    return np.diag(H)\n"
      ],
      "metadata": {
        "id": "8FjlWKiEikWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_dfbetas(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    betas = model.params  # Coefficients\n",
        "    sigma = np.sqrt(model.mse_resid)  # Residual standard deviation\n",
        "    n, p = X.shape\n",
        "    dfbetas = np.zeros((n, p))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute betas excluding observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "\n",
        "        # Compute DFBETAS for each predictor\n",
        "        for j in range(p):\n",
        "            dfbetas[i, j] = (betas[j] - betas_exclude_i[j]) / (sigma * np.sqrt(np.linalg.inv(X.T @ X)[j, j]))\n",
        "\n",
        "    return dfbetas\n"
      ],
      "metadata": {
        "id": "ZFUcmF1likYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_dffits(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    sigma = np.sqrt(model.mse_resid)  # Residual standard deviation\n",
        "    n, p = X.shape\n",
        "    dffits = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute predicted y for observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new_i = X[i] @ betas_exclude_i\n",
        "\n",
        "        # Compute DFFITS\n",
        "        h_ii = X[i] @ np.linalg.inv(X.T @ X) @ X[i].T  # Leverage for observation i\n",
        "        dffits[i] = (y_hat[i] - y_hat_new_i) / (sigma * np.sqrt(h_ii))\n",
        "\n",
        "    return dffits\n"
      ],
      "metadata": {
        "id": "c44lgjQtikdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_dffits(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    residuals = model.resid  # Residuals\n",
        "    n, p = X.shape\n",
        "    dffits = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute betas and predicted y for observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new_i = X[i] @ betas_exclude_i\n",
        "\n",
        "        # Compute leverage for observation i\n",
        "        h_ii = X[i] @ np.linalg.inv(X.T @ X) @ X[i].T\n",
        "\n",
        "        # Compute sigma^2_{(-i)} using the leave-one-out residuals\n",
        "        sse = np.sum(residuals**2)  # Sum of squared residuals\n",
        "        sigma_sq_minus_i = (sse - (residuals[i]**2 / (1 - h_ii))) / (n - p - 2)\n",
        "        sigma_minus_i = np.sqrt(sigma_sq_minus_i)\n",
        "\n",
        "        # Compute DFFITS using sigma_{(-i)} and leverage\n",
        "        dffits[i] = (y_hat[i] - y_hat_new_i) / (sigma_minus_i * np.sqrt(h_ii))\n",
        "\n",
        "    return dffits\n"
      ],
      "metadata": {
        "id": "CAyvOZFvoFQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_cooks_distances(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    sigma = np.sqrt(model.mse_resid)  # Residual standard deviation\n",
        "    n, p = X.shape\n",
        "    cooks_d = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute predicted y for all observations\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new = X @ betas_exclude_i\n",
        "\n",
        "        # Compute Cook's Distance\n",
        "        cooks_d[i] = np.sum((y_hat - y_hat_new) ** 2) / (p * sigma**2)\n",
        "\n",
        "    return cooks_d\n"
      ],
      "metadata": {
        "id": "IqyZe9UFipVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute manual influence measures\n",
        "leverage = manual_leverage(model)\n",
        "dfbetas = manual_dfbetas(model)\n",
        "dffits = manual_dffits(model)\n",
        "cooks_distances = manual_cooks_distances(model)\n",
        "\n",
        "# Compare with statsmodels\n",
        "influence = model.get_influence()\n",
        "statsmodels_leverage = influence.hat_matrix_diag\n",
        "statsmodels_dfbetas = influence.dfbetas\n",
        "statsmodels_dffits = influence.dffits[0]\n",
        "statsmodels_cooks = influence.cooks_distance[0]\n",
        "\n",
        "# Print comparisons\n",
        "print(\"Manual Leverage vs Statsmodels Leverage:\")\n",
        "print(np.allclose(leverage, statsmodels_leverage,atol=1e-02))\n",
        "\n",
        "print(\"Manual DFBETAS vs Statsmodels DFBETAS:\")\n",
        "print(np.allclose(dfbetas, statsmodels_dfbetas,atol=1e-02))\n",
        "\n",
        "print(\"Manual DFFITS vs Statsmodels DFFITS:\")\n",
        "print(np.allclose(dffits, statsmodels_dffits,atol=1e-02))\n",
        "\n",
        "print(\"Manual Cook's Distance vs Statsmodels Cook's Distance:\")\n",
        "print(np.allclose(cooks_distances, statsmodels_cooks,atol=1e-02))"
      ],
      "metadata": {
        "id": "HaU-TYGgipav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_manual_influence_measures(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    data = pd.DataFrame(model.model.data.frame)  # Original data as a DataFrame\n",
        "\n",
        "    # Compute influence measures using manual functions\n",
        "    leverage = manual_leverage(model)\n",
        "    dfbetas = manual_dfbetas(model)\n",
        "    dffits = manual_dffits(model)\n",
        "    cooks_distances = manual_cooks_distances(model)\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n, p = X.shape\n",
        "\n",
        "    # Rule-of-thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Flag outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        'High Cook\\'s Distance': cooks_distances > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(p):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        'Cook\\'s Distance': cooks_distances,\n",
        "        'DFFITS': dffits,\n",
        "    })\n",
        "\n",
        "    # Add DFBETAS for each predictor\n",
        "    for j in range(p):\n",
        "        summary_df[f'DFBETAS (Predictor {j})'] = dfbetas[:, j]\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    # Combine with original data\n",
        "    summary_with_data = pd.concat([data.reset_index(drop=True), summary_df], axis=1)\n",
        "\n",
        "    # Select flagged observations\n",
        "    flagged_columns = [col for col in summary_with_data.columns if col.startswith('High')]\n",
        "    flagged_observations = summary_with_data.loc[summary_with_data[flagged_columns].any(axis=1)]\n",
        "\n",
        "    return flagged_observations, summary_with_data\n"
      ],
      "metadata": {
        "id": "HdIRYM1skYGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flagged_observations_manual, summary_with_data_manual = summarize_manual_influence_measures(model)\n",
        "flagged_observations_manual"
      ],
      "metadata": {
        "id": "IOZ1yHWNkaeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flagged_observations"
      ],
      "metadata": {
        "id": "6g8NZRVgl3UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcjnabUPtwvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a good outlying point to predictors\n",
        "outlier = pd.DataFrame({'X1': [max(data0['X1']) + 25],\n",
        "                        'X2': [max(data0['X2']) + 35],\n",
        "                        'X3': [max(data0['X3']) + 25]})\n",
        "X_with_outlier = pd.concat([data0[['X1', 'X2', 'X3']], outlier], ignore_index=True)\n",
        "\n",
        "# Recalculating Y with the new outlying point\n",
        "# Create the design matrix for the model including the intercept\n",
        "X_design = sm.add_constant(X_with_outlier)\n",
        "# Calculate Y values including the outlier\n",
        "Y_with_outlier = np.dot(X_design, beta0).flatten() + np.append(e, np.random.normal(0, 4))\n",
        "\n",
        "# Simple Regression - only X2 as independent variable\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_with_outlier['X2'], Y_with_outlier)\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression with at least one influential point')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lEFSC3nbx5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UuzUcHYyJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Playground"
      ],
      "metadata": {
        "id": "DE14OnGgsQ4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n = 100\n",
        "X1 = np.random.normal(10, 2, n)\n",
        "X2 = np.random.normal(20, 5, n)\n",
        "X3 = np.random.normal(30, 3, n)\n",
        "e = np.random.normal(0, 4, n)\n",
        "beta0 = [5, 2, -1, 3]  # Intercept and slopes for X1, X2, X3\n",
        "X_design = sm.add_constant(pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3}))  # Add intercept\n",
        "Y = np.dot(X_design, beta0) + e\n",
        "\n",
        "data0 = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
        "\n",
        "# Step 2: Function to add outliers or leverage points\n",
        "def add_outliers(data, n_outliers=1, leverage=False, extreme_y=False):\n",
        "    \"\"\"\n",
        "    Add outliers or high-leverage points to the data.\n",
        "\n",
        "    :param data: Original data as a DataFrame.\n",
        "    :param n_outliers: Number of outliers to add.\n",
        "    :param leverage: Whether to add high-leverage points (extreme predictors).\n",
        "    :param extreme_y: Whether to add extreme Y values.\n",
        "    :return: Updated DataFrame with added outliers.\n",
        "    \"\"\"\n",
        "    new_data = data.copy()\n",
        "    for _ in range(n_outliers):\n",
        "        if leverage:\n",
        "            # Add high-leverage points (extreme predictor values)\n",
        "            outlier = {\n",
        "                'X1': max(data['X1']) + np.random.uniform(20, 30),\n",
        "                'X2': max(data['X2']) + np.random.uniform(30, 40),\n",
        "                'X3': max(data['X3']) + np.random.uniform(20, 30),\n",
        "                'Y': np.random.uniform(min(data['Y']), max(data['Y']))\n",
        "            }\n",
        "        elif extreme_y:\n",
        "            # Add extreme Y values\n",
        "            outlier = {\n",
        "                'X1': np.random.uniform(min(data['X1']), max(data['X1'])),\n",
        "                'X2': np.random.uniform(min(data['X2']), max(data['X2'])),\n",
        "                'X3': np.random.uniform(min(data['X3']), max(data['X3'])),\n",
        "                'Y': max(data['Y']) + np.random.uniform(20, 40)\n",
        "            }\n",
        "        else:\n",
        "            # Add a general outlier (moderately extreme values in both X and Y)\n",
        "            outlier = {\n",
        "                'X1': max(data['X1']) + np.random.uniform(10, 20),\n",
        "                'X2': max(data['X2']) + np.random.uniform(15, 25),\n",
        "                'X3': max(data['X3']) + np.random.uniform(10, 20),\n",
        "                'Y': max(data['Y']) + np.random.uniform(10, 20)\n",
        "            }\n",
        "        new_data = pd.concat([new_data, pd.DataFrame([outlier])], ignore_index=True)\n",
        "    return new_data\n"
      ],
      "metadata": {
        "id": "J-ZUKnfOqVx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Add outliers or leverage points\n",
        "data_with_outliers = add_outliers(data0, n_outliers=3, leverage=True,extreme_y=True)\n",
        "\n",
        "# Step 4: Fit a regression model and calculate influence measures\n",
        "X_with_outliers = sm.add_constant(data_with_outliers[['X1', 'X2', 'X3']])\n",
        "model = sm.OLS(data_with_outliers['Y'], X_with_outliers).fit()"
      ],
      "metadata": {
        "id": "ise78u5aqnnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Visualize scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(data_with_outliers['X2'], data_with_outliers['Y'], label=\"Data Points\", alpha=0.7)\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression with Added Outliers')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Plot regression diagnostics\n",
        "fig = model.get_influence().summary_frame().plot(kind='scatter', x='hat_diag', y='student_resid', alpha=0.7)\n",
        "plt.title('Regression Diagnostics: Leverage vs Studentized Residuals')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Highlight flagged observations\n",
        "influence = model.get_influence()\n",
        "summary_frame = influence.summary_frame()\n",
        "summary_frame['index'] = range(len(summary_frame))\n",
        "flagged_obs = summary_frame[\n",
        "    (summary_frame['cooks_d'] > 4 / n) | (summary_frame['hat_diag'] > 2 * (X_with_outliers.shape[1] / n))\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(data_with_outliers['X2'], data_with_outliers['Y'], label=\"Data Points\", alpha=0.7)\n",
        "plt.scatter(\n",
        "    data_with_outliers.iloc[flagged_obs['index']]['X2'],\n",
        "    data_with_outliers.iloc[flagged_obs['index']]['Y'],\n",
        "    color='red',\n",
        "    label='Flagged Observations',\n",
        "    s=100\n",
        ")\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Flagged Observations Highlighted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JnAj3-KCsxq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1mzBCcKqWSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}