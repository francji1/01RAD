{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_ex03_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SL2gaKMqH9M"
      },
      "source": [
        "# 01RAD Exercise 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5796ccd9"
      },
      "source": [
        "\n",
        "This exercise consolidates simple linear regression, shows two common ways to include a categorical predictor (as parallel lines and with interaction), and revisits linear algebra views of OLS including multicollinearity diagnostics and the hat matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhNKTe8qJPh"
      },
      "source": [
        "Last exercise: simple linear regression + different approaches how to add categorical varaible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNwFWm9ArpLo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from statsmodels.datasets import get_rdataset\n",
        "from scipy.stats import t,norm\n",
        "\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGUUC5HarrhP"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "cars = sns.load_dataset('mpg').dropna()  # Dropping rows with missing values\n",
        "\n",
        "# Check the first few rows\n",
        "print(cars.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "204ed38b"
      },
      "source": [
        "\n",
        "The `mpg` dataset contains fuel efficiency and car attributes. We focus on `mpg` (response), `weight` (numeric regressor), and `origin` (categorical: USA, Europe, Japan).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCnUk4WQrmZL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fe13431"
      },
      "source": [
        "\n",
        "## Model A: mpg ~ weight\n",
        "A single line for all cars. Intercept is the mean `mpg` at zero weight (not meaningful physically), slope is the expected change in `mpg` per unit weight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE08YXhZruvq"
      },
      "outputs": [],
      "source": [
        "# OLS model: mpg ~ weight (single slope and intercept)\n",
        "model1 = smf.ols('mpg ~ weight', data=cars)\n",
        "results1 = model1.fit()\n",
        "print(results1.summary())\n",
        "\n",
        "\n",
        "# Scatter plot + regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='weight', y='mpg', data=cars, color='blue')\n",
        "plt.plot(cars['weight'], results1.fittedvalues, color='red', label='Regression line')\n",
        "\n",
        "plt.title('Simple Linear Regression (mpg ~ weight)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ee389a"
      },
      "source": [
        "\n",
        "## Model B: mpg ~ weight + origin\n",
        "Parallel lines per origin: one common slope for `weight`, separate intercepts by origin. Encoded via `C(origin)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E3FAlF1sntk"
      },
      "outputs": [],
      "source": [
        "# OLS model: mpg ~ weight + origin (three intercepts, one slope)\n",
        "model2 = smf.ols('mpg ~ weight + C(origin)', data=cars)\n",
        "results2 = model2.fit()\n",
        "print(results2.summary())\n",
        "\n",
        "# Scatter plot with points colored by origin\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = sns.scatterplot(x='weight', y='mpg', hue='origin', data=cars, palette='Set1')\n",
        "\n",
        "# Get the color palette used in the scatterplot\n",
        "palette = dict(zip(cars['origin'].unique(), scatter.legend_.get_texts()))\n",
        "\n",
        "# Plot regression lines for each origin group (same slope, different intercepts)\n",
        "for origin_level in cars['origin'].unique():\n",
        "    subset = cars[cars['origin'] == origin_level]\n",
        "    color = scatter.legend_.get_lines()[list(cars['origin'].unique()).index(origin_level)].get_color()\n",
        "    plt.plot(subset['weight'], results2.predict(subset), label=f'Origin {origin_level}', color=color)\n",
        "\n",
        "plt.title('Multiple Intercepts (mpg ~ weight + origin)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend(title='Origin')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25173531"
      },
      "source": [
        "\n",
        "## Model C: mpg ~ weight * origin\n",
        "Interaction allows both intercepts and slopes to differ by origin: three distinct lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39B_epW1snv2"
      },
      "outputs": [],
      "source": [
        "# OLS model: mpg ~ weight + origin + weight:origin (three intercepts, three slopes)\n",
        "model3 = smf.ols('mpg ~ weight * C(origin)', data=cars)\n",
        "results3 = model3.fit()\n",
        "print(results3.summary())\n",
        "\n",
        "# Scatter plot with points colored by origin\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = sns.scatterplot(x='weight', y='mpg', hue='origin', data=cars, palette='Set1')\n",
        "\n",
        "# Plot regression lines for each origin group (different slopes and intercepts)\n",
        "for origin_level in cars['origin'].unique():\n",
        "    subset = cars[cars['origin'] == origin_level]\n",
        "    color = scatter.legend_.get_lines()[list(cars['origin'].unique()).index(origin_level)].get_color()\n",
        "    plt.plot(subset['weight'], results3.predict(subset), label=f'Origin {origin_level}', color=color)\n",
        "\n",
        "plt.title('Multiple Intercepts and Slopes (mpg ~ weight * origin)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend(title='Origin')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "167be7c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Alternative plot with a shared x-grid per origin for smoother lines\n",
        "weight_grid = np.linspace(cars['weight'].min(), cars['weight'].max(), 100)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.scatterplot(data=cars, x='weight', y='mpg', hue='origin', alpha=0.5, ax=ax)\n",
        "for origin_level in cars['origin'].unique():\n",
        "    df_line = pd.DataFrame({'weight': weight_grid, 'origin': origin_level})\n",
        "    ax.plot(weight_grid, results3.predict(df_line), label=f'{origin_level} fit')\n",
        "ax.set_title('Fitted lines by origin (shared grid)')\n",
        "ax.legend(title='Origin')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYTcZHrFvy_K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXDihzH2wvMM"
      },
      "source": [
        "**The confidence interval for the mean predicted value** $\\hat{y}_i$ at a given value of $x_i$ is calculated as:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{y}_i)}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\hat{y}_i$ is the predicted value at $x_i$, computed from the regression equation $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$,\n",
        "- $z_{\\alpha/2,}$ is the critical value from the tnormal distribution with (based on the desired confidence level, typically 95%),\n",
        "- $\\text{Var}(\\hat{y}_i)$is the variance of the predicted value $\\hat{y}_i$, given by:\n",
        "\n",
        "$\n",
        "\\text{Var}(\\hat{y}_i) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} \\right)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\sigma^2$ is the residual variance,\n",
        "- $n$ is the number of observations,\n",
        "- $x_i$ is the specific value of the independent variable for which the confidence interval is being calculated,\n",
        "- $\\bar{x}$ is the mean of the independent variable values.\n",
        "\n",
        "\n",
        "**The prediction interval for an individual predicted value** $y_i$ at a given $x_i$ is computed as:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{y}_i) + \\sigma^2}\n",
        "$\n",
        "\n",
        "## Qeuestions:\n",
        "\n",
        "* Is this computation correct?\n",
        "* If so, can I use it in practice?\n",
        "* How can we derive the formula for $\\hat{y}_i$ in general?\n",
        "\n",
        "<!--\n",
        "\n",
        "\n",
        "The confidence interval for the mean predicted value with unknown $\\sigma$:\n",
        "\n",
        "$\n",
        "\\hat{y}_i \\pm t_{\\alpha/2, n-m-1} \\cdot \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} \\right)}\n",
        "$\n",
        "\n",
        "Where the unbiased estimate of the residual variance $\\hat{\\sigma}^2$ in a regression model is given by:\n",
        "\n",
        "$\n",
        "s_n^2 = \\hat{\\sigma}^2 = \\frac{1}{n - m - 1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$\n",
        "\n",
        "$\n",
        "\\text{Var}(\\hat{y}_i) = \\sigma^2 \\cdot \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i\n",
        "$\n",
        "\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKdThixqvzHK"
      },
      "outputs": [],
      "source": [
        "# Generate new data for weight (for a smooth line plot)\n",
        "weight_range = np.linspace(cars['weight'].min(), cars['weight'].max(), 100)\n",
        "new_data = pd.DataFrame({'weight': weight_range})\n",
        "\n",
        "# Predict the mean mpg and get confidence and prediction intervals\n",
        "predictions = results1.get_prediction(new_data)\n",
        "prediction_summary = predictions.summary_frame(alpha=0.05)  # 95% intervals\n",
        "prediction_summary.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL2zpz1p6zKW"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Scatter plot of original data\n",
        "sns.scatterplot(x='weight', y='mpg', data=cars, color='blue', label='Data')\n",
        "\n",
        "# Plot the regression line (mean prediction)\n",
        "plt.plot(weight_range, prediction_summary['mean'], color='red', label='Regression line')\n",
        "\n",
        "# Plot the confidence interval\n",
        "plt.fill_between(weight_range,\n",
        "                 prediction_summary['mean_ci_lower'],\n",
        "                 prediction_summary['mean_ci_upper'],\n",
        "                 color='red', alpha=0.3, label='Confidence interval')\n",
        "\n",
        "# Plot the prediction interval\n",
        "plt.fill_between(weight_range,\n",
        "                 prediction_summary['obs_ci_lower'],\n",
        "                 prediction_summary['obs_ci_upper'],\n",
        "                 color='green', alpha=0.2, label='Prediction interval')\n",
        "\n",
        "plt.title('Regression Line with Confidence and Prediction Intervals')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6deb3177"
      },
      "source": [
        "\n",
        "Confidence bands quantify uncertainty in the mean response; prediction bands are wider because they add the variance of new observations around the regression line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDmY7OAB69VM"
      },
      "source": [
        "Manual computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaKL8AOJvzJ8"
      },
      "outputs": [],
      "source": [
        "# Extracting the regression coefficients and residuals from statsmodels\n",
        "intercept, slope = results1.params\n",
        "y_hat = results1.fittedvalues\n",
        "residuals = results1.resid\n",
        "n = len(residuals)\n",
        "\n",
        "# Estimate of sigma^2 (unbiased residual variance) using statsmodels result\n",
        "sigma_squared_hat = results1.mse_resid\n",
        "sigma_hat = np.sqrt(sigma_squared_hat)\n",
        "\n",
        "# Variance-covariance matrix of the coefficients\n",
        "var_beta_hat = results1.cov_params()\n",
        "\n",
        "# Generate new data for weight for smooth line plotting\n",
        "weight_range = np.linspace(cars['weight'].min(), cars['weight'].max(), 100)\n",
        "X_range_with_intercept = sm.add_constant(weight_range)\n",
        "\n",
        "# Predicted mean mpg for the new data (regression line)  - classic way\n",
        "y_hat_range = X_range_with_intercept @ results1.params\n",
        "\n",
        "# You can use predict function instead of manually calculating\n",
        "# y_hat_range = results1.predict(new_data)\n",
        "\n",
        "# Standard error of the predicted mean (for confidence interval)\n",
        "se_mean_prediction = np.sqrt(np.sum(X_range_with_intercept @ var_beta_hat * X_range_with_intercept, axis=1))\n",
        "\n",
        "# Confidence interval (95%)\n",
        "alpha = 0.05\n",
        "t_value = t.ppf(1 - alpha / 2, df=n - 2)  # Critical t-value for 95% confidence interval\n",
        "confidence_interval_lower = y_hat_range - t_value * se_mean_prediction\n",
        "confidence_interval_upper = y_hat_range + t_value * se_mean_prediction\n",
        "\n",
        "# Standard error for the prediction interval (includes variance of errors)\n",
        "se_prediction_interval = np.sqrt(se_mean_prediction**2 + sigma_squared_hat)\n",
        "\n",
        "# Prediction interval (95%)\n",
        "prediction_interval_lower = y_hat_range - t_value * se_prediction_interval\n",
        "prediction_interval_upper = y_hat_range + t_value * se_prediction_interval\n",
        "\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Scatter plot of original data\n",
        "sns.scatterplot(x=cars['weight'], y=cars['mpg'], color='blue', label='Data')\n",
        "\n",
        "# Plot the regression line (mean prediction)\n",
        "plt.plot(weight_range, y_hat_range, color='red', label='Regression line')\n",
        "\n",
        "# Plot the confidence interval\n",
        "plt.fill_between(weight_range, confidence_interval_lower, confidence_interval_upper,\n",
        "                 color='red', alpha=0.3, label='Confidence interval')\n",
        "\n",
        "# Plot the prediction interval\n",
        "plt.fill_between(weight_range, prediction_interval_lower, prediction_interval_upper,\n",
        "                 color='green', alpha=0.2, label='Prediction interval')\n",
        "\n",
        "plt.title('Regression Line with Confidence and Prediction Intervals (Manual Calculation)')\n",
        "plt.xlabel('Weight')\n",
        "plt.ylabel('MPG')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print estimated sigma and standard errors for reference\n",
        "print(f\"Estimated sigma^2 (residual variance): {sigma_squared_hat}\")\n",
        "print(f\"Estimated sigma (residual standard deviation): {sigma_hat}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FCAcx1FvzMo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcX_wCVcuEKJ"
      },
      "source": [
        "## Introduction into multivarible regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4c2672"
      },
      "source": [
        "\n",
        "We compare several numerically-stable ways to solve $\\hat{\\beta}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNHGqAO1uKji"
      },
      "source": [
        "Recap: $\\hat{\\beta}^{OLS} = argmin_{\\beta \\in \\mathrm{R^p}} \\sum_{i=1}^n (Y_i - X_i \\beta)^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-UpWjWuvdC"
      },
      "source": [
        "From the Lecture: $\\hat{\\beta}^{OLS}  = (X^TX)^{-1}X^TY$\n",
        "\n",
        "Question\n",
        "* When it holds?\n",
        "* How many solutions do we have?\n",
        "* What should we check and how?\n",
        "* How do we compute $\\hat{\\beta}^{OLS}$ in practice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlwpwBdJuuns"
      },
      "outputs": [],
      "source": [
        "# Generating multivariate data X with an intercept\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Generating random explanatory variables\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Adding intercept (column of ones)\n",
        "X_intercept = np.hstack((np.ones((n_samples, 1)), X))\n",
        "\n",
        "# True coefficients beta (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])\n",
        "\n",
        "# Generating noise epsilon\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# Calculating the response Y\n",
        "Y = X_intercept @ beta_true + epsilon\n",
        "\n",
        "# Applying various methods to estimate coefficients\n",
        "\n",
        "## a) Analytical solution using normal equations\n",
        "beta_hat_norm_eq = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "\n",
        "## b) Using numpy.linalg.lstsq\n",
        "beta_hat_lstsq, residuals, rank, s = np.linalg.lstsq(X_intercept, Y, rcond=None)\n",
        "\n",
        "## c) Linear regression using sklearn\n",
        "model_sk = LinearRegression(fit_intercept=False)\n",
        "model_sk.fit(X_intercept, Y)\n",
        "beta_hat_sk = model_sk.coef_\n",
        "\n",
        "## d) Linear regression using statsmodels\n",
        "model_sm = sm.OLS(Y, X_intercept).fit()\n",
        "beta_hat_sm = model_sm.params\n",
        "\n",
        "##### Comparing the results\n",
        "\n",
        "# Creating a DataFrame for comparison\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Normal equations': beta_hat_norm_eq,\n",
        "    'Numpy lstsq': beta_hat_lstsq,\n",
        "    'Sklearn': beta_hat_sk,\n",
        "    'Statsmodels': beta_hat_sm\n",
        "})\n",
        "\n",
        "print(df_results)\n",
        "\n",
        "# Graphical comparison\n",
        "methods = ['Normal equations', 'Numpy lstsq', 'Sklearn', 'Statsmodels']\n",
        "x = np.arange(len(beta_true))  # Indices of coefficients\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, beta_true, 'o-', label='True beta', linewidth=3)\n",
        "for method in methods:\n",
        "    plt.plot(x, df_results[method], 'x--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features+1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Comparison of Estimated Coefficients by Different Methods')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQc8zs8u99hu"
      },
      "source": [
        "Overview:\n",
        "\n",
        "* NumPy (`numpy.linalg.lstsq`): Uses Singular Value Decomposition (SVD)\n",
        "\n",
        "* SciPy (`scipy.linalg.lstsq`): Offers methods using QR decomposition and SVD\n",
        "\n",
        "* scikit-learn (`LinearRegression`): Uses SVD via numpy.linalg.lstsq\n",
        "\n",
        "* statsmodels (`OLS`): Uses QR decomposition by default\n",
        "\n",
        "Methods:\n",
        "* **Cholesky Decomposition** $(X^T X = L L^T)$ is efficient but sensitive to data conditions. Use it when you are confident that $(X^TX)$ is positive definite.\n",
        "\n",
        "* **QR Decomposition** ($X = QR$) is a stable method suitable for most linear regression problems, especially when multicollinearity is a concern.\n",
        "\n",
        "* **SVD** ($X = U \\Sigma V^T$ and ($(X^TX)^{-1}X = X^{+}$) provides the most robust solution, particularly in the presence of multicollinearity or rank deficiency, higher computational cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYOmXjdw91q7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT91CGeQuJhq"
      },
      "outputs": [],
      "source": [
        "# Generating multivariate data X with an intercept\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Function to generate correlated features\n",
        "def generate_correlated_data(n_samples, n_features, correlation):\n",
        "    \"\"\"\n",
        "    Generates a dataset with specified correlation between features.\n",
        "\n",
        "    Parameters:\n",
        "    - n_samples: Number of samples\n",
        "    - n_features: Number of features (excluding intercept)\n",
        "    - correlation: Desired correlation between features (between 0 and 1)\n",
        "\n",
        "    Returns:\n",
        "    - X: Generated data matrix with an intercept column\n",
        "    \"\"\"\n",
        "    # Create a covariance matrix with the desired correlation\n",
        "    cov_matrix = np.full((n_features, n_features), correlation)\n",
        "    np.fill_diagonal(cov_matrix, 1)\n",
        "\n",
        "    # Generate multivariate normal data\n",
        "    mean = np.zeros(n_features)\n",
        "    X_no_intercept = np.random.multivariate_normal(mean, cov_matrix, size=n_samples)\n",
        "\n",
        "    # Add intercept (column of ones)\n",
        "    X = np.hstack((np.ones((n_samples, 1)), X_no_intercept))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Set the desired correlation level (e.g., 0.9 for high correlation)\n",
        "correlation_level = 0.99\n",
        "\n",
        "# Generate correlated data\n",
        "X = generate_correlated_data(n_samples, n_features, correlation_level)\n",
        "\n",
        "# True coefficients beta (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])  # Length should be n_features + 1\n",
        "\n",
        "# Generating noise epsilon\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# Calculating the response Y\n",
        "Y = X @ beta_true + epsilon\n",
        "\n",
        "# Calculating the condition number of X^T X\n",
        "condition_number = np.linalg.cond(X.T @ X)\n",
        "print(f\"Condition number of X^T X: {condition_number:.2e}\")\n",
        "\n",
        "##### Applying various methods to estimate coefficients\n",
        "\n",
        "## a) Analytical solution using normal equations\n",
        "try:\n",
        "    beta_hat_norm_eq = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "except np.linalg.LinAlgError:\n",
        "    beta_hat_norm_eq = np.linalg.pinv(X.T @ X) @ X.T @ Y\n",
        "    print(\"Used pseudo-inverse due to singular matrix in normal equations.\")\n",
        "\n",
        "## b) Using numpy.linalg.lstsq\n",
        "beta_hat_lstsq, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "## c) Linear regression using sklearn\n",
        "model_sk = LinearRegression(fit_intercept=False)\n",
        "model_sk.fit(X, Y)\n",
        "beta_hat_sk = model_sk.coef_\n",
        "\n",
        "## d) Linear regression using statsmodels\n",
        "model_sm = sm.OLS(Y, X).fit()\n",
        "beta_hat_sm = model_sm.params\n",
        "\n",
        "#### Comparing the results\n",
        "\n",
        "# Creating a DataFrame for comparison\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Normal equations': beta_hat_norm_eq,\n",
        "    'Numpy lstsq': beta_hat_lstsq,\n",
        "    'Sklearn': beta_hat_sk,\n",
        "    'Statsmodels': beta_hat_sm\n",
        "})\n",
        "\n",
        "print(df_results)\n",
        "\n",
        "# Graphical comparison\n",
        "methods = ['Normal equations', 'Numpy lstsq', 'Sklearn', 'Statsmodels']\n",
        "x = np.arange(len(beta_true))  # Indices of coefficients\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, beta_true, 'o-', label='True beta', linewidth=3)\n",
        "for method in methods:\n",
        "    plt.plot(x, df_results[method], 'x--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features+1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title(f'Comparison of Estimated Coefficients (Correlation={correlation_level})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jzFsvQCFd50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3936e445"
      },
      "source": [
        "\n",
        "###  diagnostics and numerical solvers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjB9StveFd85"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of samples and features\n",
        "n_samples = 100\n",
        "n_features = 3\n",
        "\n",
        "# Function to generate correlated features\n",
        "def generate_correlated_data(n_samples, n_features, correlation):\n",
        "    \"\"\"\n",
        "    Generates a dataset with specified correlation between features.\n",
        "\n",
        "    Parameters:\n",
        "    - n_samples: Number of samples\n",
        "    - n_features: Number of features (excluding intercept)\n",
        "    - correlation: Desired correlation between features (between 0 and 1)\n",
        "\n",
        "    Returns:\n",
        "    - X: Generated data matrix with an intercept column\n",
        "    \"\"\"\n",
        "    # Mean and covariance matrix\n",
        "    mean = np.zeros(n_features)\n",
        "    cov = np.full((n_features, n_features), correlation)\n",
        "    np.fill_diagonal(cov, 1)\n",
        "\n",
        "    # Generate multivariate normal data\n",
        "    X_no_intercept = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
        "\n",
        "    # Add intercept (column of ones)\n",
        "    X = np.hstack((np.ones((n_samples, 1)), X_no_intercept))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Generate data with high correlation to test ill-conditioned scenarios\n",
        "correlation_level = 0.99\n",
        "X = generate_correlated_data(n_samples, n_features, correlation_level)\n",
        "\n",
        "# True coefficients (including intercept)\n",
        "beta_true = np.array([5, 2, -3, 1])  # Length should be n_features + 1\n",
        "\n",
        "# Generate noise\n",
        "epsilon = np.random.randn(n_samples)\n",
        "\n",
        "# Calculate response variable\n",
        "Y = X @ beta_true + epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JZrJwHpFd_w"
      },
      "outputs": [],
      "source": [
        "def linear_regression_cholesky(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using normal equations and Cholesky decomposition.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute X^T X and X^T Y\n",
        "    XtX = X.T @ X\n",
        "    XtY = X.T @ Y\n",
        "\n",
        "    # Perform Cholesky decomposition of XtX\n",
        "    try:\n",
        "        L = np.linalg.cholesky(XtX)\n",
        "    except np.linalg.LinAlgError:\n",
        "        raise np.linalg.LinAlgError(\"Matrix X^T X is not positive definite.\")\n",
        "\n",
        "    # Solve L * z = X^T Y\n",
        "    z = np.linalg.solve(L, XtY)\n",
        "\n",
        "    # Solve L^T * beta_hat = z\n",
        "    beta_hat = np.linalg.solve(L.T, z)\n",
        "\n",
        "    return beta_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGpOpGOUFeCS"
      },
      "outputs": [],
      "source": [
        "def linear_regression_qr(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using QR decomposition.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the QR decomposition of X\n",
        "    Q, R = np.linalg.qr(X)\n",
        "\n",
        "    # Compute Q^T Y\n",
        "    QtY = Q.T @ Y\n",
        "\n",
        "    # Solve R * beta_hat = Q^T Y\n",
        "    beta_hat = np.linalg.solve(R, QtY)\n",
        "\n",
        "    return beta_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNixULy_FeE6"
      },
      "outputs": [],
      "source": [
        "def linear_regression_svd(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using Singular Value Decomposition (SVD).\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the SVD of X\n",
        "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "    # Compute beta_hat = V * S_inv * U^T * Y\n",
        "    S_inv = np.diag(1 / S)\n",
        "    beta_hat = Vt.T @ S_inv @ U.T @ Y\n",
        "\n",
        "    return beta_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhf8_2qiFeHY"
      },
      "outputs": [],
      "source": [
        "def linear_regression_pinv(X, Y):\n",
        "    \"\"\"\n",
        "    Solves the linear regression problem using the Moore-Penrose pseudoinverse.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Design matrix (n_samples x n_features)\n",
        "    - Y: Response vector (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - beta_hat: Estimated coefficients (n_features,)\n",
        "    \"\"\"\n",
        "    # Compute the pseudoinverse of X\n",
        "    X_pinv = np.linalg.pinv(X)\n",
        "\n",
        "    # Compute beta_hat = X_pinv * Y\n",
        "    beta_hat = X_pinv @ Y\n",
        "\n",
        "    return beta_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8Aw-QzFeJ1"
      },
      "outputs": [],
      "source": [
        "# Estimate coefficients using Cholesky decomposition\n",
        "try:\n",
        "    beta_cholesky = linear_regression_cholesky(X, Y)\n",
        "except np.linalg.LinAlgError as e:\n",
        "    print(f\"Cholesky method failed: {e}\")\n",
        "    beta_cholesky = np.full(beta_true.shape, np.nan)\n",
        "\n",
        "# Estimate coefficients using QR decomposition\n",
        "beta_qr = linear_regression_qr(X, Y)\n",
        "\n",
        "# Estimate coefficients using SVD\n",
        "beta_svd = linear_regression_svd(X, Y)\n",
        "\n",
        "# Estimate coefficients using the pseudoinverse\n",
        "beta_pinv = linear_regression_pinv(X, Y)\n",
        "\n",
        "# For reference, use NumPy's lstsq method\n",
        "beta_lstsq, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    'True beta': beta_true,\n",
        "    'Cholesky': beta_cholesky,\n",
        "    'QR Decomposition': beta_qr,\n",
        "    'SVD': beta_svd,\n",
        "    'Pseudoinverse': beta_pinv,\n",
        "    'NumPy lstsq': beta_lstsq\n",
        "})\n",
        "\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAe1fzQqm2bn"
      },
      "outputs": [],
      "source": [
        "# Graphical comparison\n",
        "methods = ['Cholesky', 'QR Decomposition', 'SVD', 'Pseudoinverse', 'NumPy lstsq']\n",
        "x = np.arange(len(beta_true))  # Indices of coefficients\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, beta_true, 'o-', label='True beta', linewidth=3)\n",
        "for method in methods:\n",
        "    plt.plot(x, df_results[method], 'x--', label=method)\n",
        "\n",
        "plt.xticks(x, ['Intercept'] + [f'X{i}' for i in range(1, n_features+1)])\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Value')\n",
        "plt.title(f'Comparison of Estimated Coefficients (Correlation={correlation_level})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFu5iEIsHofD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL-b-_6iIuSw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6C83VLsIuVc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht5fKzGKIuYW"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UyNYcAhLMBo"
      },
      "source": [
        "## Hat matrix H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac0161a4"
      },
      "source": [
        "\n",
        "The hat matrix $H = X(X^\top X)^{-1}X^\top$ maps observed $Y$ to fitted values $\\hat{Y}=HY$. It is symmetric and idempotent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMSkKmQ1LOZc"
      },
      "outputs": [],
      "source": [
        "# Compute X^T X\n",
        "XtX = X.T @ X\n",
        "\n",
        "# Calculate the hat matrix H\n",
        "H = X @ np.linalg.inv(XtX) @ X.T\n",
        "\n",
        "print(\"Dimensions of H:\", H.shape)\n",
        "print(\"Dimensions of X:\", X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcQkei5CLO31"
      },
      "outputs": [],
      "source": [
        "# Eigenvalues of H\n",
        "eigenvalues = np.linalg.eigvals(H)\n",
        "print(\"Eigenvalues of H:\", np.round(eigenvalues, 10))\n",
        "\n",
        "# Check if H is idempotent\n",
        "idempotent_diff = np.sum(np.round(H @ H - H, 5))\n",
        "print(\"Difference between H^2 and H:\", idempotent_diff)\n",
        "\n",
        "# Check if H is symmetric\n",
        "symmetry_diff = np.round(H.T - H, 10)\n",
        "print(\"Difference between H^T and H:\", symmetry_diff)\n",
        "\n",
        "# Predicted values\n",
        "hat_Y = H @ Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Il72m4HLj9M"
      },
      "outputs": [],
      "source": [
        "model = sm.OLS(Y, X)\n",
        "results = model.fit()\n",
        "\n",
        "# Predicted values from statsmodels\n",
        "Y_hat_sm = results.predict(X)\n",
        "# (X @ results.params)\n",
        "\n",
        "\n",
        "# Compare predicted values\n",
        "difference = hat_Y - Y_hat_sm\n",
        "print(\"Difference between predicted values from Hat matrix and statsmodels predict():\")\n",
        "print(np.round(difference, 10).sum())\n",
        "max_difference = np.max(np.abs(difference))\n",
        "print(f\"Maximum absolute difference: {max_difference}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCg0gZ9ZL7j0"
      },
      "outputs": [],
      "source": [
        "print((X @ results.params).mean())\n",
        "print(results.fittedvalues.mean())\n",
        "print((H @ Y).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rld4a0gSL84k"
      },
      "outputs": [],
      "source": [
        "# M matrix: I - H\n",
        "M = np.identity(H.shape[0]) - H\n",
        "e = (M @ Y)\n",
        "e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueuICa-BM1gk"
      },
      "outputs": [],
      "source": [
        "# Residuals computed using M matrix\n",
        "residuals_M = M @ Y\n",
        "\n",
        "# Residuals from statsmodels\n",
        "residuals_statsmodels = results.resid\n",
        "\n",
        "# Compare the two sets of residuals\n",
        "difference = residuals_M - residuals_statsmodels\n",
        "print(\"Difference between residuals from M matrix and statsmodels:\")\n",
        "print(np.round(difference, 10).sum())\n",
        "\n",
        "# Maximum absolute difference\n",
        "max_difference = np.max(np.abs(difference))\n",
        "print(f\"Maximum absolute difference: {max_difference}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_hBFQVYNN40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aBkIp1uNSfq"
      },
      "source": [
        "# Individual student work\n",
        "\n",
        "# **Exercise: Developing a Marketing Plan Based on Advertising Data**\n",
        "\n",
        "Imagine that you are statistical consultants tasked with building a marketing plan for the next year to maximize product sales. You have access to a dataset that contains information on the advertising budget allocated to three different media channels—**TV**, **Radio**, and **Newspaper**—and the corresponding **Sales** figures.\n",
        "\n",
        "## **Dataset Description**\n",
        "\n",
        "- **Variables:**\n",
        "  - **TV**: Advertising budget allocated to TV (in thousands of dollars)\n",
        "  - **Radio**: Advertising budget allocated to Radio (in thousands of dollars)\n",
        "  - **Newspaper**: Advertising budget allocated to Newspaper (in thousands of dollars)\n",
        "  - **Sales**: Product sales (in thousands of units)\n",
        "\n",
        "## **Tasks**\n",
        "\n",
        "Based on this data and your final regression model, answer the following questions:\n",
        "\n",
        "1. **Relationship Between Advertising Budget and Sales**\n",
        "   - Is there a statistically significant relationship between the advertising budget and sales?\n",
        "\n",
        "2. **Contribution of Each Media**\n",
        "   - Do all three media channels—TV, Radio, and Newspaper—contribute to sales?\n",
        "   - Which media have significant effects on sales?\n",
        "\n",
        "3. **Media Generating the Biggest Boost in Sales**\n",
        "   - Which advertising medium generates the largest increase in sales per unit increase in budget?\n",
        "\n",
        "4. **Strength of the Relationship**\n",
        "   - How strong is the relationship between the advertising budget and sales?\n",
        "   - What is the coefficient of determination (R-squared) of your model?\n",
        "\n",
        "5. **Effect of TV Advertising**\n",
        "   - How much increase in sales is associated with a given increase in TV advertising budget?\n",
        "\n",
        "6. **Effect of Radio Advertising**\n",
        "   - How much increase in sales is associated with a given increase in Radio advertising budget?\n",
        "\n",
        "7. **Accuracy of Estimated Effects**\n",
        "   - How accurately can we estimate the effect of each medium on sales?\n",
        "   - Provide the confidence intervals for the coefficients of each medium.\n",
        "\n",
        "8. **Predicting Future Sales**\n",
        "   - How accurately can we predict future sales based on the advertising budgets?\n",
        "   - What is the standard error of the estimate?\n",
        "\n",
        "9. **Optimal Allocation of Advertising Budget**\n",
        "    - Imagine you have a budget of $100,000. What is the best strategy to allocate this budget among TV, Radio, and Newspaper advertising to maximize sales?\n",
        "\n",
        "10. **Predicting Sales for Specific Budget Allocation**\n",
        "    - If you spend $10,000 on TV advertising and $20,000 on Radio advertising, how much increase in sales can you expect?\n",
        "\n",
        "11. **Confidence Interval for Predicted Sales**\n",
        "    - What is the 95% confidence interval for the predicted sales in the previous question?\n",
        "\n",
        "12. **Checking Correlation Between Independent Variables**\n",
        "    - Are there significant correlations between the advertising budgets for different media?\n",
        "    - How might multicollinearity affect your regression model?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ROONrvbNUur"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "Advert = pd.read_csv(\"https://raw.githubusercontent.com/francji1/01RAD/main/data/Advert.csv\", sep=\",\")\n",
        "Advert.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raA0HpeBP86k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}