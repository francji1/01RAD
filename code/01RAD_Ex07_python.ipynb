{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_Ex07_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01RAD: Exercise 07\n",
        "\n",
        "- Recap Stepwise regression pros/cons\n",
        "- AIC/BIC criteria\n",
        "- Transformation, Box cox transformation of dependent response variable\n",
        "- Trasformation of independent variable"
      ],
      "metadata": {
        "id": "H3d9YGb952gr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe3WsTiJoIaL"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stepwise regression completion from the last exercise"
      ],
      "metadata": {
        "id": "dguutuVZVeU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and clean the dataset\n",
        "data = sns.load_dataset('mpg').dropna()\n",
        "\n",
        "# Drop the redundant \"name\" column\n",
        "data = data.drop(columns=['name'])\n",
        "\n",
        "# Check the structure of data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "KnRAlVvSOkUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHYlQ1lOVtUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take model with all features and second order interactions"
      ],
      "metadata": {
        "id": "ojplI-SdVunj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define interaction terms in the formula\n",
        "formula = 'mpg ~ (cylinders + displacement + horsepower + weight + acceleration + model_year)**2'\n",
        "\n",
        "# Fit an OLS model with interactions\n",
        "model = smf.ols(formula, data=data).fit()\n",
        "\n",
        "# Display model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "cRPws_HKKhHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract t-scores and p-values for individual variables\n",
        "t_scores = model.tvalues\n",
        "p_values = model.pvalues\n",
        "\n",
        "# Combine into a DataFrame for clarity\n",
        "significant_features = pd.DataFrame({\n",
        "    'Feature': t_scores.index,\n",
        "    't-Score': t_scores.values,\n",
        "    'p-Value': p_values.values\n",
        "}).sort_values(by='p-Value', ascending=True)\n",
        "\n",
        "# Display significant features with their t-scores and p-values\n",
        "from IPython.display import display\n",
        "display(significant_features)"
      ],
      "metadata": {
        "id": "qNzm5_VCOfND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion of usefulness or not usefulness **SelectKBest**"
      ],
      "metadata": {
        "id": "XHJsFJVLV6SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stepwise selection with SelectKBest\n",
        "# Prepare data for scikit-learn\n",
        "# Extract predictors and response\n",
        "X = data[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']]\n",
        "y = data['mpg']\n",
        "\n",
        "# Generate polynomial and interaction terms\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Get feature names for interaction terms\n",
        "feature_names = poly.get_feature_names_out(input_features=X.columns)\n",
        "\n",
        "# Use SelectKBest with F-regression\n",
        "selector = SelectKBest(score_func=f_regression, k=8)  # Use 'k' to limit the number of features if needed\n",
        "X_selected = selector.fit_transform(X_poly, y)\n",
        "\n",
        "# Get selected features and their scores\n",
        "selected_features = feature_names[selector.get_support()]\n",
        "scores = selector.scores_[selector.get_support()]\n",
        "\n",
        "# Print significant features\n",
        "significant_features = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'F-Score': scores\n",
        "}).sort_values(by='F-Score', ascending=False)\n",
        "\n",
        "# Display significant features with their t-scores and p-values\n",
        "from IPython.display import display\n",
        "display(significant_features)\n"
      ],
      "metadata": {
        "id": "7lUfWLvUOeiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* good/bad\n",
        "* univariate/multivariate\n",
        "* Another ML appraoch\n",
        "* What to use instead !\n"
      ],
      "metadata": {
        "id": "1k4c-pxj4OHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Criteria: AIC and BIC\n",
        "\n",
        "\n",
        "## AIC: Akaike Information Criterion\n",
        "\n",
        "The **AIC** is defined as:\n",
        "\n",
        "$$\n",
        "\\text{AIC} = -2 \\ln(\\hat{L}) + 2k\n",
        "$$\n",
        "\n",
        "- $ \\ln(\\hat{L}) $: The log-likelihood of the model.\n",
        "- $ k $: The number of estimated parameters in the model (including the intercept).\n",
        "\n",
        "The AIC penalizes model complexity by adding $ 2k $. Lower AIC values indicate a better trade-off between model fit and complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## BIC: Bayesian Information Criterion\n",
        "\n",
        "The **BIC** is defined as:\n",
        "\n",
        "$$\n",
        "\\text{BIC} = -2 \\ln(\\hat{L}) + k \\ln(n)\n",
        "$$\n",
        "\n",
        "- $ \\ln(\\hat{L}) $: The log-likelihood of the model.\n",
        "- $ k $: The number of estimated parameters.\n",
        "- $ n $: The number of observations in the dataset.\n",
        "\n",
        "The BIC penalizes complexity more heavily than the AIC, as the penalty term $ k \\ln(n) $ grows with the size of the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Differences Between AIC and BIC\n",
        "\n",
        "| Criterion | Penalizes Complexity | Suitable When |\n",
        "|-----------|-----------------------|---------------|\n",
        "| **AIC**   | $ 2k $             | predictive models |\n",
        "| **BIC**   | $ k \\ln(n) $       | simpler, interpretable models with fewer predictors |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QDsjHUIFTrWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate interaction and polynomial terms\n",
        "def generate_interaction_terms(predictors):\n",
        "    interaction_terms = [f\"{a}:{b}\" for a, b in itertools.combinations(predictors, 2)]\n",
        "    quadratic_terms = [f\"I({var}**2)\" for var in predictors]\n",
        "    return predictors + interaction_terms + quadratic_terms\n",
        "\n",
        "# Modified stepwise selection function\n",
        "def stepwise_selection(data, response, predictors):\n",
        "    \"\"\"\n",
        "    Perform stepwise regression using AIC and BIC.\n",
        "\n",
        "    Args:\n",
        "    - data: DataFrame containing the dataset.\n",
        "    - response: The dependent variable.\n",
        "    - predictors: List of predictors (independent variables).\n",
        "\n",
        "    Returns:\n",
        "    - aic_values: List of minimum AIC values at each step.\n",
        "    - bic_values: List of minimum BIC values at each step.\n",
        "    - best_aic_model: Final model selected using AIC.\n",
        "    - best_bic_model: Final model selected using BIC.\n",
        "    \"\"\"\n",
        "    selected_predictors = []  # Start with no predictors\n",
        "    remaining_predictors = set(predictors)\n",
        "    current_aic = float('inf')\n",
        "    current_bic = float('inf')\n",
        "\n",
        "    aic_values = []\n",
        "    bic_values = []\n",
        "    best_aic_model = None\n",
        "    best_bic_model = None\n",
        "\n",
        "    while remaining_predictors:\n",
        "        scores_with_candidates = []\n",
        "        for candidate in remaining_predictors:\n",
        "            formula = f\"{response} ~ {' + '.join(selected_predictors + [candidate])}\"\n",
        "            model = smf.ols(formula, data=data).fit()\n",
        "            scores_with_candidates.append((model.aic, model.bic, candidate, model))\n",
        "\n",
        "        # Sort candidates by AIC\n",
        "        scores_with_candidates.sort(key=lambda x: x[0])\n",
        "        best_aic, best_bic, best_candidate, best_model = scores_with_candidates[0]\n",
        "\n",
        "        # Update AIC and BIC\n",
        "        if best_aic < current_aic:\n",
        "            current_aic = best_aic\n",
        "            best_aic_model = best_model\n",
        "        if best_bic < current_bic:\n",
        "            current_bic = best_bic\n",
        "            best_bic_model = best_model\n",
        "\n",
        "        # Record current minimum AIC/BIC\n",
        "        aic_values.append(current_aic)\n",
        "        bic_values.append(current_bic)\n",
        "\n",
        "        # Add the best candidate to selected predictors and remove from remaining\n",
        "        selected_predictors.append(best_candidate)\n",
        "        remaining_predictors.remove(best_candidate)\n",
        "\n",
        "    return aic_values, bic_values, best_aic_model, best_bic_model\n"
      ],
      "metadata": {
        "id": "hD4AQB_uLPZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "data = sns.load_dataset('mpg').dropna()\n",
        "data = data.drop(columns=['name'])  # Drop redundant \"name\" column\n",
        "\n",
        "# Define response and predictors\n",
        "response = 'mpg'\n",
        "predictors = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']\n",
        "\n",
        "# Generate interaction and polynomial terms\n",
        "expanded_predictors = generate_interaction_terms(predictors)\n",
        "\n",
        "# Fit an OLS large model with all interactions and quadratic terms\n",
        "large_formula = f\"{response} ~ {' + '.join(expanded_predictors)}\"\n",
        "large_model = smf.ols(large_formula, data=data).fit()\n",
        "print(large_model.summary())\n"
      ],
      "metadata": {
        "id": "ZODVmNhOTman"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run stepwise selection with expanded predictors\n",
        "aic_values, bic_values, best_aic_model, best_bic_model = stepwise_selection(data, response, expanded_predictors)\n",
        "\n",
        "# Print the minimum AIC and BIC and their corresponding models\n",
        "print(\"Minimum AIC:\", min(aic_values))\n",
        "print(\"Selected Predictors for AIC Model:\")\n",
        "print(best_aic_model.summary())\n",
        "\n",
        "print(\"\\nMinimum BIC:\", min(bic_values))\n",
        "print(\"Selected Predictors for BIC Model:\")\n",
        "print(best_bic_model.summary())\n"
      ],
      "metadata": {
        "id": "IjLEYgMfSYfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot AIC and BIC values\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(1, len(aic_values) + 1)\n",
        "plt.plot(x, aic_values, marker='o', label='AIC')\n",
        "plt.plot(x, bic_values, marker='s', label='BIC')\n",
        "plt.axvline(np.argmin(aic_values) + 1, color='blue', linestyle='--', label='Min AIC')\n",
        "plt.axvline(np.argmin(bic_values) + 1, color='orange', linestyle='--', label='Min BIC')\n",
        "plt.xlabel('Number of Features')\n",
        "plt.ylabel('Criterion Value')\n",
        "plt.title('Stepwise Selection: AIC and BIC over Number of Features (Including Interactions)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vRS_8CLpSKPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52ZVa_-ePz0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations in Linear regression"
      ],
      "metadata": {
        "id": "6L5FW5KkWRYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the initial model\n",
        "model = smf.ols('mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Plot residuals vs. fitted values to check for heteroscedasticity\n",
        "residuals = model.resid\n",
        "fitted = model.fittedvalues\n",
        "\n",
        "plt.scatter(fitted, residuals)\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Fitted Values (Checking for Heteroscedasticity)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MA-YlUBNo39N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1fnbeldZdFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logarithmic Transformation of the Response\n",
        "\n",
        "\n",
        "\n",
        "When the response variable $Y$ is transformed using the natural logarithm, the regression model changes from:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\epsilon\n",
        "$$\n",
        "\n",
        "to:\n",
        "\n",
        "$$\n",
        "\\ln(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\epsilon\n",
        "$$\n",
        "\n",
        "\n",
        "The Logarithmic transformation is particularly useful when:\n",
        "- Non-linearity between the predictors and response.\n",
        "- Heteroscedasticity (non-constant variance of residuals).\n",
        "- Skewness in the response distribution.\n",
        "- The relationship between predictors and the response is **multiplicative** rather than **additive**.\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "Suppose you're modeling the relationship between `mpg` ($Y$) and `weight` of the car ($X$):\n",
        "\n",
        "1. **Additive Error**: `mpg` increases by a fixed amount of `weight`.\n",
        "   $$\n",
        "   \\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{weight} + \\epsilon\n",
        "   $$\n",
        "\n",
        "2. **Multiplicative Error**: `mpg` increases by a fixed percentage for each additional unit of `weight`.\n",
        "   $$\n",
        "   \\ln(\\text{mpg}) = \\beta_0 + \\beta_1 \\cdot \\text{weight} + \\epsilon\n",
        "   $$\n",
        "\n",
        "In the second model, $\\beta_1$ represents the **percentage increase in income** per year of education.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KJ6kHu7ZdaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add log-transformed mpg to the dataset\n",
        "data['log_mpg'] = np.log(data['mpg'])\n",
        "\n",
        "# Fit the model with log-transformed mpg\n",
        "model_log = smf.ols('log_mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Extract residuals and fitted values\n",
        "residuals_log = model_log.resid\n",
        "fitted_log = model_log.fittedvalues\n",
        "\n",
        "# Plot residuals vs. fitted values for log-transformed response\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(fitted_log, residuals_log, alpha=0.7)\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.xlabel(\"Fitted Values (Log MPG)\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Fitted Values (Log Transformation)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8PYPstgjo3_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box-Cox Transformation\n",
        "\n",
        "The **Box-Cox transformation** is a statistical technique used to stabilize variance and make data more normally distributed. It is particularly useful when the response variable exhibits non-constant variance (heteroscedasticity) or skewness.\n",
        "\n",
        "---\n",
        "\n",
        "## Definition of the Box-Cox Transformation\n",
        "\n",
        "The Box-Cox transformation is defined as:\n",
        "\n",
        "$$\n",
        "Y(\\lambda) =\n",
        "\\begin{cases}\n",
        "\\frac{Y^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
        "\\ln(Y), & \\text{if } \\lambda = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "- $Y$: The response variable (must be positive).\n",
        "- $\\lambda$: The transformation parameter.\n",
        "\n",
        "---\n",
        "\n",
        "The  Box-Cox transformation is particularly useful to:\n",
        "-  **Stabilize Variance**: The transformation reduces heteroscedasticity, ensuring that residual variance remains constant across the range of predicted values.\n",
        "- **Normalize the Data**: By finding the optimal $\\lambda$, the transformation makes the distribution of the response variable closer to normal, which is often an assumption of regression models.\n",
        "\n",
        "---\n",
        "\n",
        "## Selecting the Optimal Lambda\n",
        "\n",
        "The optimal value of $\\lambda$ is typically chosen to maximize the log-likelihood function. For each candidate $\\lambda$, the likelihood is computed, and the value that maximizes the likelihood is selected as the optimal $\\lambda$.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4AeBKET7c5xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log-Likelihood Function for Box-Cox Transformation\n",
        "\n",
        "In the context of the Box-Cox transformation, the likelihood function assumes that the transformed response variable $Y(\\lambda)$ follows a normal distribution:\n",
        "\n",
        "$$\n",
        "L(\\lambda) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(Y_i(\\lambda) - \\hat{Y}_i(\\lambda))^2}{2\\sigma^2}\\right),\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $Y_i(\\lambda) $ is the transformed response for observation \\( i \\),\n",
        "- $ \\hat{Y}_i(\\lambda) = X_i \\hat{\\beta} $ is the predicted value based on the transformed response,\n",
        "- $ \\sigma^2 $ is the variance of the residuals.\n",
        "\n",
        "---\n",
        "\n",
        "### Taking the Natural Logarithm of the Likelihood Function\n",
        "\n",
        "The natural logarithm of the likelihood function becomes:\n",
        "\n",
        "$$\n",
        "\\ln L(\\lambda) = \\sum_{i=1}^n \\ln \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(Y_i(\\lambda) - \\hat{Y}_i(\\lambda))^2}{2\\sigma^2}\\right) \\right).\n",
        "$$\n",
        "\n",
        "Expanding the logarithm and simplifying:\n",
        "\n",
        "\n",
        "$$\n",
        "\\ln L(\\lambda) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i(\\lambda) - \\hat{Y}_i(\\lambda))^2.\n",
        "$$\n",
        "\n",
        "\n",
        "Define the sum of squared residuals in the transformed space as\n",
        "\n",
        "$$\n",
        "SSE(\\lambda) = \\sum_{i=1}^n (Y_i(\\lambda) - \\hat{Y}_i(\\lambda))^2.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\sigma^2 = \\frac{SSE(\\lambda)}{n},\n",
        "$$\n",
        "\n",
        "the log-likelihood simplifies to:\n",
        "\n",
        "$$\n",
        "\\ln L(\\lambda) = -\\frac{n}{2} \\ln\\left(\\frac{1}{n} SSE(\\lambda)\\right) + (\\lambda - 1) \\sum_{i=1}^n \\ln(Y_i).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "To find the optimal $ \\lambda $, maximize $\\ln L(\\lambda) $ over a range of candidate $ \\lambda $ values.\n"
      ],
      "metadata": {
        "id": "DqCRfzPifa8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure mpg has no zero or negative values before Box-Cox transformation\n",
        "# data['mpg'] = data['mpg'] + 0.01  # Adding 0.01 to ensure positivity\n",
        "data['mpg_bc'], fitted_lambda = stats.boxcox(data['mpg'])\n",
        "print(f\"Optimal Lambda for Box-Cox: {fitted_lambda}\")\n",
        "\n",
        "# Fit the model with Box-Cox transformed mpg\n",
        "model_boxcox = smf.ols('mpg_bc ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Extract residuals and fitted values\n",
        "residuals_boxcox = model_boxcox.resid\n",
        "fitted_boxcox = model_boxcox.fittedvalues\n",
        "\n",
        "# Plot residuals vs. fitted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(fitted_boxcox, residuals_boxcox, alpha=0.7)\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.xlabel(\"Fitted Values (Box-Cox Transformed MPG)\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Fitted Values (Box-Cox Transformation)\")\n",
        "plt.show()\n",
        "# Check if the Box-Cox lambda is close to 0, which would imply a log transformation\n"
      ],
      "metadata": {
        "id": "G9SPdYVsdgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "\n",
        "# Recalculating the confidence interval for the optimal lambda\n",
        "_, lmbda_optimal, (lmbda_ci_lower, lmbda_ci_upper) = stats.boxcox(data['mpg'], alpha=0.05)\n",
        "\n",
        "# Generate log-likelihood values for a range of lambda\n",
        "lmbdas = np.linspace(-2, 2, 100)\n",
        "llf = [stats.boxcox_llf(lmbda, data['mpg']) for lmbda in lmbdas]\n",
        "\n",
        "# Replotting the log-likelihood as a function of lambda with the correct confidence interval\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(lmbdas, llf, 'b.-', label='Log-Likelihood')\n",
        "ax.axhline(stats.boxcox_llf(lmbda_optimal, data['mpg']), color='r', linestyle='-', label=f'Optimal LL: {lmbda_optimal:.2f}')\n",
        "ax.axvline(lmbda_optimal, color='r', linestyle='--', label=f'Optimal Lambda: {lmbda_optimal:.2f}')\n",
        "ax.axvline(lmbda_ci_lower, color='g', linestyle='--', label=f'CI Lower: {lmbda_ci_lower:.2f}')\n",
        "ax.axvline(lmbda_ci_upper, color='g', linestyle='--', label=f'CI Upper: {lmbda_ci_upper:.2f}')\n",
        "ax.set_xlabel('Lambda parameter')\n",
        "ax.set_ylabel('Box-Cox log-likelihood')\n",
        "ax.legend()\n",
        "\n",
        "# Insert QQ-plots for selected lambda values\n",
        "selected_lambdas = [-1, lmbda_optimal, 1]  # Include the optimal lambda and other extremes\n",
        "locations = ['lower left', 'center', 'lower right']  # Locations for QQ-plots\n",
        "\n",
        "for lmbda, loc in zip(selected_lambdas, locations):\n",
        "    transformed_data = stats.boxcox(data['mpg'], lmbda=lmbda)\n",
        "    (osm, osr), (slope, intercept, r_sq) = stats.probplot(transformed_data, dist=\"norm\")\n",
        "    ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
        "    ax_inset.plot(osm, osr, 'c.', label='Data')\n",
        "    ax_inset.plot(osm, slope * osm + intercept, 'k-', label='Fit')\n",
        "    ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
        "    ax_inset.set_xticks([])\n",
        "    ax_inset.set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fit the model with Box-Cox transformed mpg\n",
        "model_boxcox = smf.ols('mpg_bc ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Extract residuals and fitted values\n",
        "residuals_boxcox = model_boxcox.resid\n",
        "fitted_boxcox = model_boxcox.fittedvalues\n",
        "\n",
        "# Print the optimal lambda and its confidence interval\n",
        "print(f\"Optimal Lambda: {lmbda_optimal}\")\n",
        "print(f\"95% Confidence Interval for Lambda: ({lmbda_ci_lower}, {lmbda_ci_upper})\")\n"
      ],
      "metadata": {
        "id": "Ch-Ldji0oE7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Square Root Transformation\n",
        "data['sqrt_mpg'] = np.sqrt(data['mpg'])\n",
        "model_sqrt = smf.ols('sqrt_mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Reciprocal Transformation\n",
        "data['reciprocal_mpg'] = 1 / data['mpg']\n",
        "model_reciprocal = smf.ols('reciprocal_mpg ~ weight + horsepower + displacement', data=data).fit()\n"
      ],
      "metadata": {
        "id": "v8mQHZtNo4E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Ftar7jLFqpOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBOSZPcWiMwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Create transformed variables\n",
        "data['log_mpg'] = np.log(data['mpg'])\n",
        "data['sqrt_mpg'] = np.sqrt(data['mpg'])\n",
        "data['mpg_bc'], fitted_lambda = stats.boxcox(data['mpg'])\n",
        "\n",
        "# Fit models with different transformations\n",
        "model_original = smf.ols('mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "model_log = smf.ols('log_mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "model_sqrt = smf.ols('sqrt_mpg ~ weight + horsepower + displacement', data=data).fit()\n",
        "model_boxcox = smf.ols('mpg_bc ~ weight + horsepower + displacement', data=data).fit()\n",
        "\n",
        "# Calculate studentized residuals for each model\n",
        "residuals_original = model_original.get_influence().resid_studentized_internal\n",
        "residuals_log = model_log.get_influence().resid_studentized_internal\n",
        "residuals_sqrt = model_sqrt.get_influence().resid_studentized_internal\n",
        "residuals_boxcox = model_boxcox.get_influence().resid_studentized_internal\n",
        "\n",
        "# Combine residuals in a DataFrame for easy plotting\n",
        "residuals_df = pd.DataFrame({\n",
        "    'Original': residuals_original,\n",
        "    'Log': residuals_log,\n",
        "    'Square Root': residuals_sqrt,\n",
        "    'Box-Cox': residuals_boxcox\n",
        "})\n"
      ],
      "metadata": {
        "id": "YFh_0ThqpBS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot studentized residuals for all transformations in separate subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(residuals_df.columns):\n",
        "    axes[i].plot(residuals_df.index, residuals_df[col], marker='o', linestyle='', alpha=0.5)\n",
        "    axes[i].axhline(0, color='gray', linestyle='--')\n",
        "    axes[i].set_title(f\"Studentized Residuals: {col} Transformation\")\n",
        "    axes[i].set_xlabel(\"Index\")\n",
        "    axes[i].set_ylabel(\"Studentized Residuals\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "10EY67t3qKkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot studentized residuals for all transformations\n",
        "plt.figure(figsize=(12, 6))\n",
        "for col in residuals_df.columns:\n",
        "    plt.plot(residuals_df.index, residuals_df[col], marker='o', linestyle='', alpha=0.5, label=col)\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Studentized Residuals\")\n",
        "plt.title(\"Studentized Residuals for Different Transformations\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ynLkMd6aqJ31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# QQ-plots for studentized residuals\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "for i, col in enumerate(residuals_df.columns):\n",
        "    sm.qqplot(residuals_df[col], line='45', ax=axes[i // 2, i % 2])\n",
        "    axes[i // 2, i % 2].set_title(f\"QQ-plot: {col} Transformation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bb28SvxuqLWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histograms of studentized residuals\n",
        "residuals_df.plot(kind='hist', bins=20, alpha=0.5, subplots=True, layout=(2, 2), figsize=(12, 10), legend=False)\n",
        "plt.suptitle(\"Histograms of Studentized Residuals for Different Transformations\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_NgPMweiqLw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUFxFixGpvaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificially generated dataset\n",
        "\n",
        "### Simple linear model"
      ],
      "metadata": {
        "id": "PSwt3pn1jJgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "6oZaHRwljKvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y depends on X^2\n",
        "# Generate data\n",
        "X = np.linspace(0, 10, 100)\n",
        "Y1 = X**2 + np.random.normal(0, 5, size=100)\n",
        "data1 = pd.DataFrame({'X': X, 'Y': Y1})\n",
        "\n",
        "# Model Y ~ X (incorrect)\n",
        "model1_incorrect = smf.ols('Y ~ X', data=data1).fit()\n",
        "\n",
        "# Model Y ~ X^2 (correct)\n",
        "data1['X2'] = X**2\n",
        "model1_correct = smf.ols('Y ~ X2', data=data1).fit()\n"
      ],
      "metadata": {
        "id": "_iAKjinojXY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y depends on sqrt(X)\n",
        "# Generate data\n",
        "Y2 = np.sqrt(X) + np.random.normal(0, 0.1, size=100)\n",
        "data2 = pd.DataFrame({'X': X, 'Y': Y2})\n",
        "\n",
        "# Model Y ~ X (incorrect)\n",
        "model2_incorrect = smf.ols('Y ~ X', data=data2).fit()\n",
        "\n",
        "# Model Y ~ sqrt(X) (correct)\n",
        "data2['sqrt_X'] = np.sqrt(X)\n",
        "model2_correct = smf.ols('Y ~ sqrt_X', data=data2).fit()\n"
      ],
      "metadata": {
        "id": "gX4H1q6tjSXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y ~ X^2 vs Y ~ X\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(data1['X'], data1['Y'], label='True Data', alpha=0.6)\n",
        "plt.plot(data1['X'], model1_incorrect.fittedvalues, color='red', label='Y ~ X (Incorrect)')\n",
        "plt.plot(data1['X'], model1_correct.fittedvalues, color='green', label='Y ~ X^2 (Correct)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Case 1: True Relationship Y ~ X^2')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "_RzPDTRcjYv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y ~ sqrt(X) vs Y ~ X\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(data2['X'], data2['Y'], label='True Data', alpha=0.6)\n",
        "plt.plot(data2['X'], model2_incorrect.fittedvalues, color='red', label='Y ~ X (Incorrect)')\n",
        "plt.plot(data2['X'], model2_correct.fittedvalues, color='green', label='Y ~ sqrt(X) (Correct)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Case 2: True Relationship Y ~ sqrt(X)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aY_je5n3jbBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summaries for Case 1\n",
        "print(\"Case 1: Y ~ X^2 vs Y ~ X\")\n",
        "print(\"Incorrect Model (Y ~ X):\")\n",
        "print(model1_incorrect.summary())\n",
        "print(\"\\nCorrect Model (Y ~ X^2):\")\n",
        "print(model1_correct.summary())\n",
        "\n",
        "# Print model summaries for Case 2\n",
        "print(\"\\nCase 2: Y ~ sqrt(X) vs Y ~ X\")\n",
        "print(\"Incorrect Model (Y ~ X):\")\n",
        "print(model2_incorrect.summary())\n",
        "print(\"\\nCorrect Model (Y ~ sqrt(X)):\")\n",
        "print(model2_correct.summary())\n"
      ],
      "metadata": {
        "id": "HlP1pcnLjbbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.graphics.api as smg\n",
        "\n",
        "# Function to plot partial regression and partial residual plots using smf models\n",
        "def plot_partial_plots_smf(model, data, predictor, case_title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Partial regression plot\n",
        "    smg.plot_ccpr(model, predictor, ax=axes[0])\n",
        "    axes[0].set_title(f\"Partial Regression Plot ({case_title})\")\n",
        "\n",
        "    # Partial residual plot\n",
        "    smg.plot_partregress('Y', predictor, ['X'], data=data, ax=axes[1])\n",
        "    axes[1].set_title(f\"Partial Residual Plot ({case_title})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#  Y depends on X^2 but modeled as Y ~ X\n",
        "print(\"Case 1: Y depends on X^2 but modeled as Y ~ X\")\n",
        "plot_partial_plots_smf(model1_incorrect, data1, 'X', \"Case 1: Y ~ X\")\n",
        "\n",
        "#  Y depends on sqrt(X) but modeled as Y ~ X\n",
        "print(\"Case 2: Y depends on sqrt(X) but modeled as Y ~ X\")\n",
        "plot_partial_plots_smf(model2_incorrect, data2, 'X', \"Case 2: Y ~ X\")\n"
      ],
      "metadata": {
        "id": "m_ACPbj-jNBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add more variables"
      ],
      "metadata": {
        "id": "LY3Mb6PFpkcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.graphics.api as smg\n",
        "\n",
        "# Y depends on X1^2\n",
        "# Generate data\n",
        "X1 = np.linspace(0, 10, 100)  # Main variable of interest\n",
        "X2 = np.random.uniform(0, 10, 100)  # Additional variable\n",
        "X3 = np.random.normal(5, 2, 100)  # Another additional variable\n",
        "Y1 = X1**2 + 2*X2 + 3*X3 + np.random.normal(0, 5, size=100)  # True relationship\n",
        "\n",
        "# Create a DataFrame\n",
        "data1 = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y1})\n",
        "\n",
        "# Fit incorrect and correct models\n",
        "model1_incorrect = smf.ols('Y ~ X1 + X2 + X3', data=data1).fit()\n",
        "data1['X1_squared'] = X1**2\n",
        "model1_correct = smf.ols('Y ~ X1_squared + X2 + X3', data=data1).fit()\n",
        "\n",
        "# Y depends on sqrt(X1)\n",
        "# Generate data\n",
        "Y2 = np.sqrt(X1) + 2*X2 + 3*X3 + np.random.normal(0, 0.1, size=100)  # True relationship\n",
        "\n",
        "# Create a DataFrame\n",
        "data2 = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y2})\n",
        "\n",
        "# Fit incorrect and correct models\n",
        "model2_incorrect = smf.ols('Y ~ X1 + X2 + X3', data=data2).fit()\n",
        "data2['sqrt_X1'] = np.sqrt(X1)\n",
        "model2_correct = smf.ols('Y ~ sqrt_X1 + X2 + X3', data=data2).fit()\n",
        "\n",
        "# Function to plot partial regression and partial residual plots\n",
        "def plot_partial_plots_smf(model, data, predictor, case_title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Partial regression plot\n",
        "    smg.plot_ccpr(model, predictor, ax=axes[0])\n",
        "    axes[0].set_title(f\"Partial Regression Plot ({case_title})\")\n",
        "\n",
        "    # Partial residual plot\n",
        "    smg.plot_partregress('Y', predictor, ['X1', 'X2', 'X3'], data=data, ax=axes[1])\n",
        "    axes[1].set_title(f\"Partial Residual Plot ({case_title})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Y depends on X1^2 but modeled as Y ~ X1\n",
        "print(\"Case 1: Y depends on X1^2 but modeled as Y ~ X1\")\n",
        "plot_partial_plots_smf(model1_incorrect, data1, 'X1', \"Case 1: Y ~ X1\")\n",
        "print(\"Correct model: Partial plots not needed as transformation is applied.\")\n",
        "\n",
        "# Y depends on sqrt(X1) but modeled as Y ~ X1\n",
        "print(\"Case 2: Y depends on sqrt(X1) but modeled as Y ~ X1\")\n",
        "plot_partial_plots_smf(model2_incorrect, data2, 'X1', \"Case 2: Y ~ X1\")\n",
        "print(\"Correct model: Partial plots not needed as transformation is applied.\")\n"
      ],
      "metadata": {
        "id": "YHyhSvbxj2WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UY_Wd0ozmNzV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}