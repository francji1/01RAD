{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_Ex06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "01RAD Exercise 06"
      ],
      "metadata": {
        "id": "IySLvMRC2IO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets use the same dataset from the last exercise"
      ],
      "metadata": {
        "id": "-QNdlNCjnxsf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binLmBw41zGH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import f,t,norm\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n"
      ],
      "metadata": {
        "id": "kcuAltnr13Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cars_all = pd.read_csv(\"https://raw.githubusercontent.com/francji1/01RAD/main/data/carsdata2.csv\", sep=\";\")\n",
        "cars_all.head()"
      ],
      "metadata": {
        "id": "IXlL6QzQ13RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cars_all"
      ],
      "metadata": {
        "id": "b4imaZUj3zXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cars_all.isna().sum()\n"
      ],
      "metadata": {
        "id": "yaZfc6n213Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define car type and wheel drive conditions\n",
        "sedan_condition = cars_all['Sedan'] == 1\n",
        "sport_condition = cars_all['Sports'] == 1\n",
        "suv_condition = cars_all['SUV'] == 1\n",
        "minivan_condition = (cars_all['Wagon'] == 1) | (cars_all['Minivan'] == 1) | (cars_all['Pickup'] == 1)\n",
        "awd_condition = cars_all['AWD'] == 1\n",
        "rwd_condition = cars_all['RWD'] == 1\n",
        "\n",
        "# Create new DataFrame with car_type and other derived columns in a single step\n",
        "cars_all = cars_all.assign(\n",
        "    car_type=np.select(\n",
        "        [sedan_condition, sport_condition, suv_condition, minivan_condition],\n",
        "        ['sedan', 'sport', 'suv', 'minivan'],\n",
        "        default='Unknown'\n",
        "    ),\n",
        "    wheel_drive=np.select(\n",
        "        [awd_condition, rwd_condition],\n",
        "        ['AWD', 'RWD'],\n",
        "        default='FWD'\n",
        "    ),\n",
        "    consumption=100 / (1.60934 * ((cars_all['CityMPG'] + cars_all['HwyMPG']) / 2) / 3.7854)\n",
        ").astype({\n",
        "    'car_type': 'category',\n",
        "    'wheel_drive': 'category'\n",
        "}).filter([\n",
        "    'RetailPrice', 'car_type', 'consumption', 'wheel_drive',\n",
        "    'DealerCost', 'EngineSize', 'Cyl', 'HP', 'Weight', 'WheelBase', 'Len', 'Width'\n",
        "])\n",
        "\n",
        "cars_all.head()\n"
      ],
      "metadata": {
        "id": "FvIEAmX846v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txrPJP__39t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop redundant columns and rows with NA values\n",
        "cars = cars_all.drop(columns = ['Cyl','DealerCost']).copy()\n",
        "cars.dropna(inplace=True)\n",
        "cars.isna().sum()\n"
      ],
      "metadata": {
        "id": "D3DUeTPiisAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show how to handle with formula with/without one hot encoded varialbes."
      ],
      "metadata": {
        "id": "ReM8Cn11p4a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding for categorical variables\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Building the full model with all predictors and their second-order interactions\n",
        "\n",
        "predictors = cars_data_encoded.columns.drop('Weight')\n",
        "interaction_terms = ['{}:{}'.format(a, b) for a, b in combinations(predictors, 2)]\n",
        "formula_full = 'Weight ~ ' + ' + '.join(predictors) + ' + ' + ' + '.join(interaction_terms) + '-' +  'type_sport:type_suv'\n",
        "# not work: formula_full = 'Weight ~ (.)^2 ' , * works\n",
        "formula_full"
      ],
      "metadata": {
        "id": "pL_6O_Mz0vnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the full model\n",
        "full_model = smf.ols(formula=formula_full, data=cars_data_encoded).fit()\n",
        "\n",
        "# Display the summary of the full model\n",
        "full_model_summary = full_model.summary()\n",
        "full_model_aic = full_model.aic\n",
        "full_model_bic = full_model.bic\n",
        "\n",
        "print(full_model_summary)"
      ],
      "metadata": {
        "id": "3UrQiDlwoTtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for fitting a model and getting AIC and BIC\n",
        "def fit_model(formula, data):\n",
        "    model = smf.ols(formula, data=data).fit()\n",
        "    return model.aic, model.bic, model\n",
        "\n",
        "# Stepwise Regression with column validation and dynamic name matching\n",
        "def stepwise_selection(data, response, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "\n",
        "        # Forward step\n",
        "        excluded = list(set(data.columns) - set(included) - {response})\n",
        "        new_pval = pd.Series(index=excluded, dtype=float)\n",
        "        for new_column in excluded:\n",
        "            try:\n",
        "                model = smf.ols(f'{response} ~ ' + ' + '.join(included + [new_column]), data=data).fit()\n",
        "                new_pval[new_column] = model.pvalues[new_column]\n",
        "            except KeyError:\n",
        "                # In case the predictor isn't in the dataset, skip to avoid error\n",
        "                continue\n",
        "        if not new_pval.empty:\n",
        "            best_pval = new_pval.min()\n",
        "            if best_pval < threshold_in:\n",
        "                best_feature = new_pval.idxmin()\n",
        "                included.append(best_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Backward step\n",
        "        model = smf.ols(f'{response} ~ ' + ' + '.join(included), data=data).fit()\n",
        "        # Use all p-values except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max()  # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ensure categorical variables are properly encoded\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Run stepwise selection\n",
        "predictors_stepwise = stepwise_selection(cars_data_encoded, 'Weight')\n",
        "\n",
        "# Fit the model with selected predictors\n",
        "formula_stepwise = 'Weight ~ ' + ' + '.join(predictors_stepwise)\n",
        "aic_stepwise, bic_stepwise, reduced_model_t = fit_model(formula_stepwise, cars_data_encoded)\n",
        "\n",
        "# Output the selected predictors, AIC, BIC, and the final formula\n",
        "predictors_stepwise, aic_stepwise, bic_stepwise, formula_stepwise\n",
        "\n",
        "print(reduced_model_t.summary())"
      ],
      "metadata": {
        "id": "b3eyut3DHzze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcUFV2a_UMKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fit and compare models using F-test\n",
        "def fit_and_compare_models(data, full_formula, sub_formula):\n",
        "    # Fit both the full model and the sub-model\n",
        "    full_model = smf.ols(full_formula, data=data).fit()\n",
        "    sub_model = smf.ols(sub_formula, data=data).fit()\n",
        "    # Perform ANOVA to compare the models and get the p-value\n",
        "    anova_results = anova_lm(sub_model, full_model)\n",
        "    f_pvalue = anova_results[\"Pr(>F)\"][1]  # p-value for the comparison\n",
        "    return f_pvalue, full_model\n",
        "\n",
        "# Stepwise Selection using F-tests and ANOVA\n",
        "def stepwise_selection(data, response, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "\n",
        "        # Forward step: try adding each excluded variable and test significance with F-test\n",
        "        excluded = list(set(data.columns) - set(included) - {response})\n",
        "        new_pvalues = pd.Series(index=excluded, dtype=float)\n",
        "        for new_column in excluded:\n",
        "            # Only proceed if included is non-empty\n",
        "            formula_with = f'{response} ~ ' + ' + '.join(included + [new_column])\n",
        "            formula_without = f'{response} ~ ' + ' + '.join(included) if included else f'{response} ~ 1'\n",
        "            try:\n",
        "                f_pvalue, _ = fit_and_compare_models(data, formula_with, formula_without)\n",
        "                new_pvalues[new_column] = f_pvalue\n",
        "            except Exception as e:\n",
        "                print(f\"Error fitting model with {new_column}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Add the variable with the lowest F-test p-value if below threshold_in\n",
        "        if not new_pvalues.empty:\n",
        "            best_pvalue = new_pvalues.min()\n",
        "            if best_pvalue < threshold_in:\n",
        "                best_feature = new_pvalues.idxmin()\n",
        "                included.append(best_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Backward step: try removing each variable in the model and test significance with F-test\n",
        "        if included:\n",
        "            pvalues = pd.Series(index=included, dtype=float)\n",
        "            for column in included:\n",
        "                formula_with = f'{response} ~ ' + ' + '.join(included)\n",
        "                remaining_columns = [col for col in included if col != column]\n",
        "                formula_without = f'{response} ~ ' + ' + '.join(remaining_columns) if remaining_columns else f'{response} ~ 1'\n",
        "                try:\n",
        "                    f_pvalue, _ = fit_and_compare_models(data, formula_with, formula_without)\n",
        "                    pvalues[column] = f_pvalue\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fitting model without {column}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Remove the variable with the highest p-value if above threshold_out\n",
        "            worst_pvalue = pvalues.max()\n",
        "            if worst_pvalue > threshold_out:\n",
        "                worst_feature = pvalues.idxmax()\n",
        "                included.remove(worst_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Stop if no predictors were added or removed\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ensure categorical variables are properly encoded\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Run stepwise selection\n",
        "predictors_stepwise = stepwise_selection(cars_data_encoded, 'Weight')\n",
        "\n",
        "# Fit the model with selected predictors\n",
        "if predictors_stepwise:  # Ensure we have predictors before fitting the model\n",
        "    formula_stepwise = 'Weight ~ ' + ' + '.join(predictors_stepwise)\n",
        "    aic_stepwise, bic_stepwise, reduced_model_F = fit_model(formula_stepwise, cars_data_encoded)\n",
        "else:\n",
        "    print(\"No predictors were selected.\")\n",
        "\n",
        "print(reduced_model_F.summary())"
      ],
      "metadata": {
        "id": "egb1PPL8IXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 years old package\n",
        "# !pip install stepwise-regression\n",
        "\n",
        "# Install mlxtend if not already installed\n",
        "!pip install mlxtend\n"
      ],
      "metadata": {
        "id": "gdm0sIREVE6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old appraoch with mlxtend\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "# Prepare data for mlxtend\n",
        "X = cars_data_encoded.drop(columns=['Weight']) # no need for constant\n",
        "y = cars_data_encoded['Weight']\n",
        "\n",
        "# Initialize SequentialFeatureSelector for forward stepwise selection\n",
        "lr = LinearRegression()\n",
        "sfs = SFS(lr,\n",
        "          k_features=\"best\",\n",
        "          forward=True,\n",
        "          floating=False,\n",
        "          scoring='r2', # r2 neg_mean_squared_error\n",
        "          cv=5)  # Optional cross-validation\n",
        "\n",
        "# Fit the selector\n",
        "sfs = sfs.fit(X, y)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_features = list(sfs.k_feature_names_)\n",
        "\n",
        "# Construct the formula for statsmodels\n",
        "formula_stepwise = 'Weight ~ ' + ' + '.join(selected_features)\n",
        "final_model_mlx = smf.ols(formula=formula_stepwise, data=cars_data_encoded).fit()\n",
        "\n",
        "# Display the final model summary\n",
        "print(final_model_mlx.summary())\n"
      ],
      "metadata": {
        "id": "vH98mtLXbAi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach with sklearn directly\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "# Prepare data\n",
        "X = cars_data_encoded.drop(columns=['Weight'])\n",
        "y = cars_data_encoded['Weight']\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Initialize Sequential Feature Selector\n",
        "sfs = SequentialFeatureSelector(\n",
        "    estimator=lr,\n",
        "    n_features_to_select=\"auto\",  # Automatically determine the optimal number of features\n",
        "    direction=\"forward\",          # Perform forward selection\n",
        "    scoring=\"r2\",                 # (other options: 'neg_mean_squared_error')\n",
        "    cv=5                          # 5-fold cross-validation\n",
        ")\n",
        "\n",
        "# Fit the Sequential Feature Selector\n",
        "sfs.fit(X, y)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features = X.columns[sfs.get_support()]\n",
        "\n",
        "# Construct the formula for statsmodels\n",
        "formula_stepwise = 'Weight ~ ' + ' + '.join(selected_features)\n",
        "final_model_sk = smf.ols(formula=formula_stepwise, data=cars_data_encoded).fit()\n",
        "\n",
        "# Display the final model summary\n",
        "print(final_model_sk.summary())\n"
      ],
      "metadata": {
        "id": "lc19NO5afZuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POgch3JjaSEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GF9PnohhaSL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld82YRftaSOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.summary()\n"
      ],
      "metadata": {
        "id": "32dby2bGaSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conduct ANOVA (F-test) to compare the full model and the reduced model\n",
        "anova_results = anova_lm(final_model_sk, final_model_mlx)\n",
        "anova_results\n"
      ],
      "metadata": {
        "id": "fRrgFY6nBa0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gORCqJPg3dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Diagnostics and Plots\n",
        "\n",
        "Residual analysis is critical for validating model assumptions. We focus on normality, linearity, and constant variance assumptions. Key diagnostic tools include:\n",
        "\n",
        "### 1. **Q-Q Plot for Residual Normality**\n",
        "\n",
        "Plots the quantiles of the residuals against theoretical quantiles of a normal distribution.\n",
        "\n",
        "### 2. **Residuals vs. Fitted Values**\n",
        "\n",
        "- **Homoscedasticity**: Residuals should be evenly scattered around zero.\n",
        "- **Non-linearity**: A pattern in residuals suggests that the relationship between predictors and response may not be linear.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-UkCcWNQjTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Residuals plots**\n",
        "\n",
        "##Component-Residual Plot (Partial Residual Plots):\n",
        "\n",
        "* What to See: These plots show the relationship between each predictor and the response variable while controlling for the effect of other variables. They are useful for checking linearity and identifying outliers or influential points.\n",
        "It visualize the isolated effect of each predictor by adjusting for other variables.\n",
        "* Why to Plot: To verify the assumption that the relationship between predictors and the response is linear, and to spot any non-linear patterns, outliers, or points that might have a disproportionate impact on the regression model.\n",
        "\n",
        "##Added Variable Plot (Partial Regression Plots):\n",
        "\n",
        "* What to See: These plots display the relationship between the response and a given predictor, after removing the effect of all other predictors. They help in understanding the individual contribution of a predictor to the model.\n",
        "* Why to Plot: To assess the unique impact of each predictor on the response, checking for linearity, and identifying potential outliers or influential observations that might affect the slope of the regression line.\n",
        "\n",
        "## Spread-Level Plot:\n",
        "\n",
        "* What to See: This plot shows the spread or variance of the residuals against the predicted values or a predictor. It's used to check the assumption of homoscedasticity (constant variance of errors).\n",
        "* Why to Plot: To ensure that the error variance is constant across all levels of the predictors. Non-constant variance (heteroscedasticity) can indicate that the model is not capturing some aspect of the data, possibly violating regression assumptions."
      ],
      "metadata": {
        "id": "P0fIkcOWq2JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_model = reduced_model_F"
      ],
      "metadata": {
        "id": "X49cHM5ljRFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_partregress_grid, plot_ccpr_grid\n"
      ],
      "metadata": {
        "id": "rMA5MNX7Ba3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spread-Level Plot (Residuals vs Predicted)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=reduced_model.fittedvalues, y=reduced_model.resid)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Spread-Level Plot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7klFN-WMjV4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Component-Residual Plot (Partial Residual Plots)\n",
        "plot_ccpr_grid(reduced_model)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lx0mQG4zjXSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Added Variable Plot (Partial Regression Plots)\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "plot_partregress_grid(reduced_model, fig=fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1P-DLFC3jU1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_regression_diagnostics(model):\n",
        "    \"\"\"\n",
        "    Generate diagnostic plots for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the diagnostic plots.\n",
        "    \"\"\"\n",
        "\n",
        "    #residuals = model.resid\n",
        "    residuals = model.get_influence().resid_studentized  # internal studentized residuals\n",
        "\n",
        "    num_regressors = len(model.model.exog_names) - 1  # Exclude intercept\n",
        "    total_plots = num_regressors + 4  # Total plots needed (1 plot per regressor + 4 diagnostics)\n",
        "    rows = (total_plots + 2) // 3  # Calculate rows needed to fit all plots in 3 columns\n",
        "\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
        "    axes = axes.flatten()  # Flatten to iterate easily\n",
        "\n",
        "    # Plot Fitted Values vs Residuals\n",
        "    axes[0].scatter(model.fittedvalues, residuals)\n",
        "    axes[0].axhline(0, color='red', linestyle='--')\n",
        "    axes[0].set_xlabel('Fitted Values')\n",
        "    axes[0].set_ylabel('Residuals')\n",
        "    axes[0].set_title('Fitted Values vs Residuals')\n",
        "\n",
        "    # Plot Response vs Residuals for each regressor\n",
        "    for i, col in enumerate(model.model.exog_names[1:], start=1):\n",
        "        ax = axes[i]\n",
        "        ax.scatter(model.model.exog[:, i], residuals)\n",
        "        ax.axhline(0, color='red', linestyle='--')\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Residuals')\n",
        "        ax.set_title(f'Response vs Residuals: {col}')\n",
        "\n",
        "    # Normal Q-Q plot\n",
        "    sm.qqplot(residuals, line='s', ax=axes[num_regressors + 1])\n",
        "    axes[num_regressors + 1].set_title('Normal Q-Q')\n",
        "\n",
        "    # Scale-Location plot\n",
        "    axes[num_regressors + 2].scatter(model.fittedvalues, np.sqrt(np.abs(residuals)))\n",
        "    axes[num_regressors + 2].axhline(0, color='red', linestyle='--')\n",
        "    axes[num_regressors + 2].set_xlabel('Fitted Values')\n",
        "    axes[num_regressors + 2].set_ylabel('Standardized Residuals')\n",
        "    axes[num_regressors + 2].set_title('Scale-Location')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_regressors + 3, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Generate the diagnostic plots\n",
        "fig0 = plot_regression_diagnostics(reduced_model)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WR2YDzBFkV8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oahPGUXbjucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lP3lCnjijugA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_regress_exog\n",
        "\n",
        "# Check residuals against each independent variable using plot_regress_exog\n",
        "key_predictors = ['consumption', 'WheelBase', 'Width']\n",
        "\n",
        "for predictor in key_predictors:\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    plot_regress_exog(reduced_model, predictor, fig=fig)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "F-OKLGmjBa57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract predictors from the reduced model\n",
        "predictors_stepwise = reduced_model.model.exog_names\n",
        "predictors_stepwise.remove('Intercept')  # Remove the intercept from the list\n"
      ],
      "metadata": {
        "id": "yYDF1GRlBa_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the formula based on the variables provided\n",
        "formula = 'Weight ~ consumption + WheelBase + Width + RetailPrice + HP'\n",
        "\n",
        "# Fit the OLS model using statsmodels with the defined formula\n",
        "reduced_model = smf.ols(formula=formula, data=cars_data_encoded).fit()\n",
        "\n",
        "# Print the summary to see the coefficients and confirm they match\n",
        "print(reduced_model.summary())"
      ],
      "metadata": {
        "id": "p5tRAbxemw1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Log Transformation of the Response\n",
        "cars_data_encoded['log_Weight'] = np.log(cars_data_encoded['Weight'])\n",
        "formula_log = 'log_Weight ~ consumption + WheelBase + Width + RetailPrice + HP'\n",
        "model_log = smf.ols(formula=formula_log, data=cars_data_encoded).fit()\n",
        "print(model_log.summary())"
      ],
      "metadata": {
        "id": "RoxWGN-lmU1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Extract predictors from the reduced model, excluding 'Intercept' if present\n",
        "predictors_stepwise = [name for name in reduced_model_F.model.exog_names if name != 'Intercept']\n",
        "\n",
        "# Define the formula dynamically for both transformations\n",
        "formula = 'Weight ~ consumption + WheelBase + Width + RetailPrice + HP'\n",
        "\n",
        "# Log Transformation of the Response\n",
        "cars_data_encoded['log_Weight'] = np.log(cars_data_encoded['Weight'])\n",
        "model_log = smf.ols(formula=formula.replace(\"Weight\", \"log_Weight\"), data=cars_data_encoded).fit()\n",
        "\n",
        "# Box-Cox Transformation of the Response\n",
        "box_cox_transformed, best_lambda = stats.boxcox(cars_data_encoded['Weight'])\n",
        "cars_data_encoded['box_cox_Weight'] = box_cox_transformed\n",
        "model_box_cox = smf.ols(formula=formula.replace(\"Weight\", \"box_cox_Weight\"), data=cars_data_encoded).fit()\n",
        "\n",
        "# Collecting and printing summary statistics for comparison\n",
        "print(\"Best Lambda for Box-Cox Transformation:\", best_lambda)\n",
        "print(\"\\nLog-Transformed Model Summary:\\n\", model_log.summary())\n",
        "print(\"\\nBox-Cox Transformed Model Summary:\\n\", model_box_cox.summary())\n"
      ],
      "metadata": {
        "id": "gmO-po2zn0j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from matplotlib import gridspec\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "\n",
        "# Using the 'Weight' variable from the cars_data_encoded dataframe\n",
        "x = cars_data_encoded['Weight']\n",
        "\n",
        "# Lambda range and corresponding log-likelihood values\n",
        "lmbdas = np.linspace(-2, 2, 400)\n",
        "llf = [stats.boxcox_llf(lmbda, x) for lmbda in lmbdas]\n",
        "\n",
        "# Finding the lambda that maximizes the log-likelihood\n",
        "lmbda_optimal = lmbdas[np.argmax(llf)]\n",
        "\n",
        "# Plotting the log-likelihood as a function of lambda\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "gs = gridspec.GridSpec(1, 1)\n",
        "ax = fig.add_subplot(gs[0])\n",
        "ax.plot(lmbdas, llf, 'b.-')\n",
        "ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n",
        "ax.set_xlabel('Lambda parameter')\n",
        "ax.set_ylabel('Box-Cox log-likelihood')\n",
        "\n",
        "# Inset plots for different lambda values\n",
        "locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
        "for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
        "    xt = stats.boxcox(x, lmbda=lmbda)\n",
        "    (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
        "    ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
        "    ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
        "    ax_inset.set_xticklabels([])\n",
        "    ax_inset.set_yticklabels([])\n",
        "    ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "lmbda_optimal\n"
      ],
      "metadata": {
        "id": "-9n9q0G6IJLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scipy box cox functions:\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox_llf.html\n"
      ],
      "metadata": {
        "id": "Fn5M-myQfCqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculating the confidence interval for the optimal lambda using the correct method\n",
        "_, lmbda_optimal, (lmbda_ci_lower, lmbda_ci_upper) = stats.boxcox(x, alpha=0.05)\n",
        "\n",
        "# Replotting the log-likelihood as a function of lambda with the correct confidence interval\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "gs = gridspec.GridSpec(1, 1)\n",
        "ax = fig.add_subplot(gs[0])\n",
        "ax.plot(lmbdas, llf, 'b.-')\n",
        "ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n",
        "ax.axvline(lmbda_optimal, color='r', linestyle='--', label=f'Optimal Lambda: {lmbda_optimal:.2f}')\n",
        "ax.axvline(lmbda_ci_lower, color='g', linestyle='--', label=f'CI Lower: {lmbda_ci_lower:.2f}')\n",
        "ax.axvline(lmbda_ci_upper, color='g', linestyle='--', label=f'CI Upper: {lmbda_ci_upper:.2f}')\n",
        "ax.set_xlabel('Lambda parameter')\n",
        "ax.set_ylabel('Box-Cox log-likelihood')\n",
        "ax.legend()\n",
        "\n",
        "# Insert plots for different lambda values\n",
        "locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
        "for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
        "    xt = stats.boxcox(x, lmbda=lmbda)\n",
        "    (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
        "    ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
        "    ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
        "    ax_inset.set_xticklabels([])\n",
        "    ax_inset.set_yticklabels([])\n",
        "    ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "(lmbda_optimal, lmbda_ci_lower, lmbda_ci_upper)\n"
      ],
      "metadata": {
        "id": "W09sYX1WIJN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRoBxFL9IJQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MX-7OQXIJTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}