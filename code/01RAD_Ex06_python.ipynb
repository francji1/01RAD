{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/code/01RAD_Ex06_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IySLvMRC2IO4"
      },
      "source": [
        "01RAD Exercise 06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QNdlNCjnxsf"
      },
      "source": [
        "Lets use the same dataset from the last exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binLmBw41zGH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcuAltnr13Nu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import f,t,norm\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXlL6QzQ13RB"
      },
      "outputs": [],
      "source": [
        "cars_all = pd.read_csv(\"https://raw.githubusercontent.com/francji1/01RAD/refs/heads/main/data/carsdata2.csv\", sep=\";\")\n",
        "cars_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4imaZUj3zXo"
      },
      "outputs": [],
      "source": [
        "cars_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaZfc6n213Tb"
      },
      "outputs": [],
      "source": [
        "cars_all.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvIEAmX846v0"
      },
      "outputs": [],
      "source": [
        "# Define car type and wheel drive conditions\n",
        "sedan_condition = cars_all['Sedan'] == 1\n",
        "sport_condition = cars_all['Sports'] == 1\n",
        "suv_condition = cars_all['SUV'] == 1\n",
        "minivan_condition = (cars_all['Wagon'] == 1) | (cars_all['Minivan'] == 1) | (cars_all['Pickup'] == 1)\n",
        "awd_condition = cars_all['AWD'] == 1\n",
        "rwd_condition = cars_all['RWD'] == 1\n",
        "\n",
        "# Create new DataFrame with car_type and other derived columns in a single step\n",
        "cars_all = cars_all.assign(\n",
        "    car_type=np.select(\n",
        "        [sedan_condition, sport_condition, suv_condition, minivan_condition],\n",
        "        ['sedan', 'sport', 'suv', 'minivan'],\n",
        "        default='Unknown'\n",
        "    ),\n",
        "    wheel_drive=np.select(\n",
        "        [awd_condition, rwd_condition],\n",
        "        ['AWD', 'RWD'],\n",
        "        default='FWD'\n",
        "    ),\n",
        "    consumption=100 / (1.60934 * ((cars_all['CityMPG'] + cars_all['HwyMPG']) / 2) / 3.7854)\n",
        ").astype({\n",
        "    'car_type': 'category',\n",
        "    'wheel_drive': 'category'\n",
        "}).filter([\n",
        "    'RetailPrice', 'car_type', 'consumption', 'wheel_drive',\n",
        "    'DealerCost', 'EngineSize', 'Cyl', 'HP', 'Weight', 'WheelBase', 'Len', 'Width'\n",
        "])\n",
        "\n",
        "cars_all.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txrPJP__39t4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3DUeTPiisAA"
      },
      "outputs": [],
      "source": [
        "# Drop redundant columns and rows with NA values\n",
        "cars = cars_all.drop(columns = ['Cyl','DealerCost']).copy()\n",
        "cars.dropna(inplace=True)\n",
        "cars.isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReM8Cn11p4a0"
      },
      "source": [
        "Show how to handle with formula with/without one hot encoded varialbes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL_6O_Mz0vnn"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding for categorical variables\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Building the full model with all predictors and their second-order interactions\n",
        "predictors = cars_data_encoded.columns.drop('Weight')\n",
        "interaction_terms = ['{}:{}'.format(a, b) for a, b in combinations(predictors, 2)]\n",
        "formula_full = 'Weight ~ ' + ' + '.join(predictors) + ' + ' + ' + '.join(interaction_terms) + '-' +  'type_sport:type_suv'\n",
        "# not work: formula_full = 'Weight ~ (.)^2 ' , * works\n",
        "formula_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UrQiDlwoTtE"
      },
      "outputs": [],
      "source": [
        "# Fit the full model\n",
        "full_model = smf.ols(formula=formula_full, data=cars_data_encoded).fit()\n",
        "\n",
        "# Display the summary of the full model\n",
        "full_model_summary = full_model.summary()\n",
        "full_model_aic = full_model.aic\n",
        "full_model_bic = full_model.bic\n",
        "\n",
        "print(full_model_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX--6rKd-qpf"
      },
      "outputs": [],
      "source": [
        "full_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3eyut3DHzze"
      },
      "outputs": [],
      "source": [
        "# Function for fitting a model and getting AIC and BIC\n",
        "def fit_model(formula, data):\n",
        "    model = smf.ols(formula, data=data).fit()\n",
        "    return model.aic, model.bic, model\n",
        "\n",
        "# Stepwise Regression with column validation and dynamic name matching\n",
        "def stepwise_selection(data, response, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "\n",
        "        # Forward step\n",
        "        excluded = list(set(data.columns) - set(included) - {response})\n",
        "        new_pval = pd.Series(index=excluded, dtype=float)\n",
        "        for new_column in excluded:\n",
        "            try:\n",
        "                model = smf.ols(f'{response} ~ ' + ' + '.join(included + [new_column]), data=data).fit()\n",
        "                new_pval[new_column] = model.pvalues[new_column]\n",
        "            except KeyError:\n",
        "                # In case the predictor isn't in the dataset, skip to avoid error\n",
        "                continue\n",
        "        if not new_pval.empty:\n",
        "            best_pval = new_pval.min()\n",
        "            if best_pval < threshold_in:\n",
        "                best_feature = new_pval.idxmin()\n",
        "                included.append(best_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Backward step\n",
        "        model = smf.ols(f'{response} ~ ' + ' + '.join(included), data=data).fit()\n",
        "        # Use all p-values except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max()  # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ensure categorical variables are properly encoded\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Run stepwise selection\n",
        "predictors_stepwise = stepwise_selection(cars_data_encoded, 'Weight')\n",
        "\n",
        "# Fit the model with selected predictors\n",
        "formula_stepwise = 'Weight ~ ' + ' + '.join(predictors_stepwise)\n",
        "aic_stepwise, bic_stepwise, reduced_model_t = fit_model(formula_stepwise, cars_data_encoded)\n",
        "\n",
        "# Output the selected predictors, AIC, BIC, and the final formula\n",
        "predictors_stepwise, aic_stepwise, bic_stepwise, formula_stepwise\n",
        "\n",
        "print(reduced_model_t.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUFV2a_UMKh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egb1PPL8IXpP"
      },
      "outputs": [],
      "source": [
        "# Function to fit and compare models using F-test\n",
        "def fit_and_compare_models(data, full_formula, sub_formula):\n",
        "    # Fit both the full model and the sub-model\n",
        "    full_model = smf.ols(full_formula, data=data).fit()\n",
        "    sub_model = smf.ols(sub_formula, data=data).fit()\n",
        "    # Perform ANOVA to compare the models and get the p-value\n",
        "    anova_results = anova_lm(sub_model, full_model)\n",
        "    f_pvalue = anova_results[\"Pr(>F)\"][1]  # p-value for the comparison\n",
        "    return f_pvalue, full_model\n",
        "\n",
        "# Stepwise Selection using F-tests and ANOVA\n",
        "def stepwise_selection(data, response, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "\n",
        "        # Forward step: try adding each excluded variable and test significance with F-test\n",
        "        excluded = list(set(data.columns) - set(included) - {response})\n",
        "        new_pvalues = pd.Series(index=excluded, dtype=float)\n",
        "        for new_column in excluded:\n",
        "            # Only proceed if included is non-empty\n",
        "            formula_with = f'{response} ~ ' + ' + '.join(included + [new_column])\n",
        "            formula_without = f'{response} ~ ' + ' + '.join(included) if included else f'{response} ~ 1'\n",
        "            try:\n",
        "                f_pvalue, _ = fit_and_compare_models(data, formula_with, formula_without)\n",
        "                new_pvalues[new_column] = f_pvalue\n",
        "            except Exception as e:\n",
        "                print(f\"Error fitting model with {new_column}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Add the variable with the lowest F-test p-value if below threshold_in\n",
        "        if not new_pvalues.empty:\n",
        "            best_pvalue = new_pvalues.min()\n",
        "            if best_pvalue < threshold_in:\n",
        "                best_feature = new_pvalues.idxmin()\n",
        "                included.append(best_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Backward step: try removing each variable in the model and test significance with F-test\n",
        "        if included:\n",
        "            pvalues = pd.Series(index=included, dtype=float)\n",
        "            for column in included:\n",
        "                formula_with = f'{response} ~ ' + ' + '.join(included)\n",
        "                remaining_columns = [col for col in included if col != column]\n",
        "                formula_without = f'{response} ~ ' + ' + '.join(remaining_columns) if remaining_columns else f'{response} ~ 1'\n",
        "                try:\n",
        "                    f_pvalue, _ = fit_and_compare_models(data, formula_with, formula_without)\n",
        "                    pvalues[column] = f_pvalue\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fitting model without {column}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Remove the variable with the highest p-value if above threshold_out\n",
        "            worst_pvalue = pvalues.max()\n",
        "            if worst_pvalue > threshold_out:\n",
        "                worst_feature = pvalues.idxmax()\n",
        "                included.remove(worst_feature)\n",
        "                changed = True\n",
        "\n",
        "        # Stop if no predictors were added or removed\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ensure categorical variables are properly encoded\n",
        "cars_data_encoded = pd.get_dummies(cars, columns=['car_type', 'wheel_drive'], drop_first=True)\n",
        "\n",
        "# Run stepwise selection\n",
        "predictors_stepwise = stepwise_selection(cars_data_encoded, 'Weight')\n",
        "\n",
        "# Fit the model with selected predictors\n",
        "if predictors_stepwise:  # Ensure we have predictors before fitting the model\n",
        "    formula_stepwise = 'Weight ~ ' + ' + '.join(predictors_stepwise)\n",
        "    aic_stepwise, bic_stepwise, reduced_model_F = fit_model(formula_stepwise, cars_data_encoded)\n",
        "else:\n",
        "    print(\"No predictors were selected.\")\n",
        "\n",
        "print(reduced_model_F.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRrgFY6nBa0e"
      },
      "outputs": [],
      "source": [
        "# Conduct ANOVA (F-test) to compare the full model and the reduced model\n",
        "anova_results = anova_lm(reduced_model_F, full_model)\n",
        "anova_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X0560O9-qpg"
      },
      "source": [
        "## Confidence interval for the mean response $E(Y \\mid x_0)$\n",
        "\n",
        "We consider the linear model\n",
        "$$\n",
        "Y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim N_n(0, \\sigma^2 I_n),\n",
        "$$\n",
        "with design matrix $X$ of dimension $n \\times (m+1)$ including intercept.\n",
        "\n",
        "**Least squares estimator:**\n",
        "$$\n",
        "\\hat\\beta = (X^\\top X)^{-1} X^\\top Y.\n",
        "$$\n",
        "\n",
        "For a new point\n",
        "$$\n",
        "x_0 = (1, x_{0,1}, \\dots, x_{0,m})^\\top\n",
        "$$\n",
        "the point prediction of the conditional mean is\n",
        "$$\n",
        "\\hat Y_{x_0} = x_0^\\top \\hat\\beta.\n",
        "$$\n",
        "\n",
        "**Residual sum of squares and variance estimate:**\n",
        "$$\n",
        "\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2, \\qquad\n",
        "s_n^2 = \\frac{\\text{SSE}}{n - m - 1}.\n",
        "$$\n",
        "\n",
        "**Variance of $\\hat Y_{x_0}$:**\n",
        "$$\n",
        "\\operatorname{Var}(\\hat Y_{x_0})\n",
        "= \\sigma^2 x_0^\\top (X^\\top X)^{-1} x_0\n",
        "\\approx s_n^2 \\, x_0^\\top (X^\\top X)^{-1} x_0.\n",
        "$$\n",
        "\n",
        "Using normality and independence of $\\hat\\beta$ and $s_n^2$:\n",
        "$$\n",
        "\\frac{\\hat Y_{x_0} - E(Y \\mid x_0)}\n",
        "     {s_n \\sqrt{x_0^\\top (X^\\top X)^{-1} x_0}}\n",
        "\\sim t_{n-m-1}.\n",
        "$$\n",
        "\n",
        "**$100(1-\\alpha)\\%$ confidence interval for $E(Y \\mid x_0)$:**\n",
        "$$\n",
        "\\boxed{\n",
        "\\hat Y_{x_0}\n",
        "\\;\\pm\\;\n",
        "t_{1-\\alpha/2,\\,n-m-1}\n",
        "\\; s_n \\sqrt{x_0^\\top (X^\\top X)^{-1} x_0}\n",
        "}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Prediction interval for a new observation $Y_{x_0}$\n",
        "\n",
        "Now $Y_{x_0}$ is a new future observation at $x_0$:\n",
        "$$\n",
        "Y_{x_0} = x_0^\\top \\beta + \\varepsilon_0,\n",
        "$$\n",
        "independent of the sample.\n",
        "\n",
        "Point prediction is again\n",
        "$$\n",
        "\\hat Y_{x_0} = x_0^\\top \\hat\\beta.\n",
        "$$\n",
        "\n",
        "**Variance of prediction error:**\n",
        "$$\n",
        "\\operatorname{Var}(\\hat Y_{x_0} - Y_{x_0})\n",
        "= \\operatorname{Var}(\\hat Y_{x_0}) + \\operatorname{Var}(Y_{x_0})\n",
        "= \\sigma^2 \\bigl(1 + x_0^\\top (X^\\top X)^{-1} x_0 \\bigr)\n",
        "\\approx s_n^2 \\bigl(1 + x_0^\\top (X^\\top X)^{-1} x_0 \\bigr).\n",
        "$$\n",
        "\n",
        "Hence\n",
        "$$\n",
        "\\frac{\\hat Y_{x_0} - Y_{x_0}}\n",
        "     {s_n \\sqrt{1 + x_0^\\top (X^\\top X)^{-1} x_0}}\n",
        "\\sim t_{n-m-1}.\n",
        "$$\n",
        "\n",
        "**$100(1-\\alpha)\\%$ prediction interval for $Y_{x_0}$:**\n",
        "$$\n",
        "\\boxed{\n",
        "\\hat Y_{x_0}\n",
        "\\;\\pm\\;\n",
        "t_{1-\\alpha/2,\\,n-m-1}\n",
        "\\; s_n \\sqrt{1 + x_0^\\top (X^\\top X)^{-1} x_0}\n",
        "}\n",
        "$$\n",
        "\n",
        "**Key comparison:**\n",
        "\n",
        "- CI for $E(Y \\mid x_0)$: only model (estimation) uncertainty\n",
        "  $\\Rightarrow$ term $x_0^\\top (X^\\top X)^{-1} x_0$.\n",
        "- PI for $Y_{x_0}$: model uncertainty $+$ new-error variance\n",
        "  $\\Rightarrow$ term $1 + x_0^\\top (X^\\top X)^{-1} x_0$.\n",
        "\n",
        "Prediction interval is always wider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-4jOn1A-qph"
      },
      "outputs": [],
      "source": [
        "reduced_model_F.model.exog_names[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPH5cTj-qph"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 1) Prepare data and fit reduced model\n",
        "# =========================================\n",
        "\n",
        "# Use only needed columns and drop rows with missing values\n",
        "cols = [\"Weight\", \"consumption\", \"WheelBase\", \"Width\", \"RetailPrice\", \"HP\"]\n",
        "cars_model = cars[cols].dropna()\n",
        "\n",
        "model = smf.ols(\n",
        "    \"Weight ~ consumption + WheelBase + Width + RetailPrice + HP\",\n",
        "    data=cars_model\n",
        "    ).fit()\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# =========================================\n",
        "# 2) Build prediction grid\n",
        "#    - x-axis: consumption\n",
        "#    - others fixed at their sample means\n",
        "# =========================================\n",
        "\n",
        "x_col = \"consumption\"\n",
        "\n",
        "x_min, x_max = cars_model[x_col].min(), cars_model[x_col].max()\n",
        "x_grid = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "# Fix other regressors at mean\n",
        "base_vals = {\n",
        "    \"WheelBase\": cars_model[\"WheelBase\"].mean(),\n",
        "    \"Width\": cars_model[\"Width\"].mean(),\n",
        "    \"RetailPrice\": cars_model[\"RetailPrice\"].mean(),\n",
        "    \"HP\": cars_model[\"HP\"].mean(),\n",
        "}\n",
        "\n",
        "pred_df = pd.DataFrame({\n",
        "    \"consumption\": x_grid,\n",
        "    \"WheelBase\": base_vals[\"WheelBase\"],\n",
        "    \"Width\": base_vals[\"Width\"],\n",
        "    \"RetailPrice\": base_vals[\"RetailPrice\"],\n",
        "    \"HP\": base_vals[\"HP\"],\n",
        "})\n",
        "\n",
        "# =========================================\n",
        "# MANUAL CI and PI\n",
        "# =========================================\n",
        "\n",
        "res = model  # shorthand\n",
        "\n",
        "exog_names = res.model.exog_names  # e.g. ['Intercept','consumption',...]\n",
        "n_grid = len(pred_df)\n",
        "\n",
        "# Build design matrix for prediction in the same order as exog_names\n",
        "pred_exog = np.column_stack([\n",
        "    np.ones(n_grid) if name == \"Intercept\" else pred_df[name].to_numpy()\n",
        "    for name in exog_names\n",
        "])\n",
        "\n",
        "beta = res.params.to_numpy()\n",
        "cov_beta = res.cov_params().to_numpy()\n",
        "s2 = float(res.mse_resid)\n",
        "df_resid = int(res.df_resid)\n",
        "t_crit = stats.t.ppf(0.975, df_resid)\n",
        "\n",
        "# Point prediction\n",
        "y_hat_manual = pred_exog @ beta\n",
        "\n",
        "# Var(ŷ | x) = x' cov(β) x\n",
        "var_mean = np.sum((pred_exog @ cov_beta) * pred_exog, axis=1)\n",
        "se_mean = np.sqrt(var_mean)\n",
        "\n",
        "# CI for E(Y | x)\n",
        "ci_lower_manual = y_hat_manual - t_crit * se_mean\n",
        "ci_upper_manual = y_hat_manual + t_crit * se_mean\n",
        "\n",
        "# PI for new Y: Var(error) = s^2 + Var(ŷ)\n",
        "se_pred = np.sqrt(s2 + var_mean)\n",
        "pi_lower_manual = y_hat_manual - t_crit * se_pred\n",
        "pi_upper_manual = y_hat_manual + t_crit * se_pred\n",
        "\n",
        "# =========================================\n",
        "# BUILT-IN CI and PI (statsmodels)\n",
        "# =========================================\n",
        "\n",
        "pred_res = res.get_prediction(pred_df)\n",
        "sf = pred_res.summary_frame(alpha=0.05)\n",
        "\n",
        "y_hat_sm = sf[\"mean\"].to_numpy()\n",
        "ci_lower_sm = sf[\"mean_ci_lower\"].to_numpy()\n",
        "ci_upper_sm = sf[\"mean_ci_upper\"].to_numpy()\n",
        "pi_lower_sm = sf[\"obs_ci_lower\"].to_numpy()\n",
        "pi_upper_sm = sf[\"obs_ci_upper\"].to_numpy()\n",
        "\n",
        "# Optional numeric sanity check\n",
        "print(\"Max |manual - sm| mean:\", np.max(np.abs(y_hat_manual - y_hat_sm)))\n",
        "print(\"Max |manual - sm| CI lower:\", np.max(np.abs(ci_lower_manual - ci_lower_sm)))\n",
        "print(\"Max |manual - sm| PI lower:\", np.max(np.abs(pi_lower_manual - pi_lower_sm)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1rZO6fg-qph"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =========================================\n",
        "# PLOTS with Generated Data\n",
        "# =========================================\n",
        "\n",
        "# Plot 1: Manual CI and PI\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x_grid, y_hat_manual, label=\"Fit (manual)\")\n",
        "plt.plot(x_grid, ci_lower_manual, \"--\", label=\"CI lower (manual)\")\n",
        "plt.plot(x_grid, ci_upper_manual, \"--\", label=\"CI upper (manual)\")\n",
        "plt.plot(x_grid, pi_lower_manual, \":\", label=\"PI lower (manual)\")\n",
        "plt.plot(x_grid, pi_upper_manual, \":\", label=\"PI upper (manual)\")\n",
        "plt.xlabel(x_col)\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.title(\"Manual CI and PI for Weight vs consumption\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot 2: Statsmodels CI and PI\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x_grid, y_hat_sm, label=\"Fit (statsmodels)\")\n",
        "plt.plot(x_grid, ci_lower_sm, \"--\", label=\"CI lower (sm)\")\n",
        "plt.plot(x_grid, ci_upper_sm, \"--\", label=\"CI upper (sm)\")\n",
        "plt.plot(x_grid, pi_lower_sm, \":\", label=\"PI lower (sm)\")\n",
        "plt.plot(x_grid, pi_upper_sm, \":\", label=\"PI upper (sm)\")\n",
        "plt.xlabel(x_col)\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.title(\"Statsmodels CI and PI for Weight vs consumption\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYRwLppO-qph"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# PLOTS USING MEASURED DATA\n",
        "# =========================================\n",
        "\n",
        "res = model\n",
        "\n",
        "# Design matrix used in the fit (matches params order)\n",
        "X_all = res.model.exog\n",
        "beta = res.params.to_numpy()\n",
        "cov_beta = res.cov_params().to_numpy()\n",
        "s2 = float(res.mse_resid)\n",
        "df_resid = int(res.df_resid)\n",
        "t_crit = stats.t.ppf(0.975, df_resid)\n",
        "\n",
        "# Observed values\n",
        "x_obs = cars_model[\"consumption\"].to_numpy()\n",
        "y_obs = cars_model[\"Weight\"].to_numpy()\n",
        "\n",
        "# ----- Manual CI/PI for each observed row -----\n",
        "\n",
        "y_hat_manual_all = X_all @ beta\n",
        "var_mean_all = np.sum((X_all @ cov_beta) * X_all, axis=1)\n",
        "se_mean_all = np.sqrt(var_mean_all)\n",
        "\n",
        "ci_lower_manual_all = y_hat_manual_all - t_crit * se_mean_all\n",
        "ci_upper_manual_all = y_hat_manual_all + t_crit * se_mean_all\n",
        "\n",
        "se_pred_all = np.sqrt(s2 + var_mean_all)\n",
        "pi_lower_manual_all = y_hat_manual_all - t_crit * se_pred_all\n",
        "pi_upper_manual_all = y_hat_manual_all + t_crit * se_pred_all\n",
        "\n",
        "# Sort by consumption for nice bands\n",
        "idx = np.argsort(x_obs)\n",
        "x_sorted = x_obs[idx]\n",
        "y_obs_sorted = y_obs[idx]\n",
        "y_hat_manual_sorted = y_hat_manual_all[idx]\n",
        "ci_l_man_sorted = ci_lower_manual_all[idx]\n",
        "ci_u_man_sorted = ci_upper_manual_all[idx]\n",
        "pi_l_man_sorted = pi_lower_manual_all[idx]\n",
        "pi_u_man_sorted = pi_upper_manual_all[idx]\n",
        "\n",
        "# ----- Built-in CI/PI via get_prediction for each observed row -----\n",
        "\n",
        "pred_res_all = res.get_prediction(cars_model)\n",
        "sf_all = pred_res_all.summary_frame(alpha=0.05)\n",
        "\n",
        "y_hat_sm_all = sf_all[\"mean\"].to_numpy()\n",
        "ci_lower_sm_all = sf_all[\"mean_ci_lower\"].to_numpy()\n",
        "ci_upper_sm_all = sf_all[\"mean_ci_upper\"].to_numpy()\n",
        "pi_lower_sm_all = sf_all[\"obs_ci_lower\"].to_numpy()\n",
        "pi_upper_sm_all = sf_all[\"obs_ci_upper\"].to_numpy()\n",
        "\n",
        "y_hat_sm_sorted = y_hat_sm_all[idx]\n",
        "ci_l_sm_sorted = ci_lower_sm_all[idx]\n",
        "ci_u_sm_sorted = ci_upper_sm_all[idx]\n",
        "pi_l_sm_sorted = pi_lower_sm_all[idx]\n",
        "pi_u_sm_sorted = pi_upper_sm_all[idx]\n",
        "\n",
        "# =========================================\n",
        "# Plot 1: Manual CI and PI with measured data\n",
        "# =========================================\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x_obs, y_obs, alpha=0.4, label=\"Observed Weight\")\n",
        "plt.plot(x_sorted, y_hat_manual_sorted, label=\"Fit (manual)\")\n",
        "plt.plot(x_sorted, ci_l_man_sorted, \"--\", label=\"CI lower (manual)\")\n",
        "plt.plot(x_sorted, ci_u_man_sorted, \"--\", label=\"CI upper (manual)\")\n",
        "plt.plot(x_sorted, pi_l_man_sorted, \":\", label=\"PI lower (manual)\")\n",
        "plt.plot(x_sorted, pi_u_man_sorted, \":\", label=\"PI upper (manual)\")\n",
        "plt.xlabel(\"consumption\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.title(\"Manual CI and PI for Weight vs consumption (all predictors used)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# =========================================\n",
        "# Plot 2: Statsmodels CI and PI with measured data\n",
        "# =========================================\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x_obs, y_obs, alpha=0.4, label=\"Observed Weight\")\n",
        "plt.plot(x_sorted, y_hat_sm_sorted, label=\"Fit (statsmodels)\")\n",
        "plt.plot(x_sorted, ci_l_sm_sorted, \"--\", label=\"CI lower (sm)\")\n",
        "plt.plot(x_sorted, ci_u_sm_sorted, \"--\", label=\"CI upper (sm)\")\n",
        "plt.plot(x_sorted, pi_l_sm_sorted, \":\", label=\"PI lower (sm)\")\n",
        "plt.plot(x_sorted, pi_u_sm_sorted, \":\", label=\"PI upper (sm)\")\n",
        "plt.xlabel(\"consumption\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.title(\"Statsmodels CI and PI for Weight vs consumption (all predictors used)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Io5Ue_G-qpi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-UkCcWNQjTN"
      },
      "source": [
        "## Residual Diagnostics and Plots\n",
        "\n",
        "Residual analysis is critical for validating model assumptions. We focus on normality, linearity, and constant variance assumptions. Key diagnostic tools include:\n",
        "\n",
        "### 1. **Q-Q Plot for Residual Normality**\n",
        "\n",
        "Plots the quantiles of the residuals against theoretical quantiles of a normal distribution.\n",
        "\n",
        "### 2. **Residuals vs. Fitted Values**\n",
        "\n",
        "- **Homoscedasticity**: Residuals should be evenly scattered around zero.\n",
        "- **Non-linearity**: A pattern in residuals suggests that the relationship between predictors and response may not be linear.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0fIkcOWq2JS"
      },
      "source": [
        "### 3. **Residuals plots**\n",
        "\n",
        "**Component-Residual Plot (Partial Residual Plots):**\n",
        "\n",
        "* What to See: These plots show the relationship between each predictor and the response variable while controlling for the effect of other variables. They are useful for checking linearity and identifying outliers or influential points.\n",
        "It visualize the isolated effect of each predictor by adjusting for other variables.\n",
        "* Why to Plot: To verify the assumption that the relationship between predictors and the response is linear, and to spot any non-linear patterns, outliers, or points that might have a disproportionate impact on the regression model.\n",
        "\n",
        "**Added Variable Plot (Partial Regression Plots):**\n",
        "\n",
        "* What to See: These plots display the relationship between the response and a given predictor, after removing the effect of all other predictors. They help in understanding the individual contribution of a predictor to the model.\n",
        "* Why to Plot: To assess the unique impact of each predictor on the response, checking for linearity, and identifying potential outliers or influential observations that might affect the slope of the regression line.\n",
        "\n",
        "**Spread-Level Plot:**\n",
        "\n",
        "* What to See: This plot shows the spread or variance of the residuals against the predicted values or a predictor. It's used to check the assumption of homoscedasticity (constant variance of errors).\n",
        "* Why to Plot: To ensure that the error variance is constant across all levels of the predictors. Non-constant variance (heteroscedasticity) can indicate that the model is not capturing some aspect of the data, possibly violating regression assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X49cHM5ljRFU"
      },
      "outputs": [],
      "source": [
        "reduced_model = reduced_model_F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMA5MNX7Ba3R"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.regressionplots import plot_partregress_grid, plot_ccpr_grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7klFN-WMjV4g"
      },
      "outputs": [],
      "source": [
        "# Spread-Level Plot (Residuals vs Predicted)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=reduced_model.fittedvalues, y=reduced_model.resid)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Spread-Level Plot')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUI45jqc-qpi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import levene\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white, het_goldfeldquandt\n",
        "\n",
        "# Extract residuals and exogenous variables from the reduced model\n",
        "resid = reduced_model.resid\n",
        "exog = reduced_model.model.exog\n",
        "fitted = reduced_model.fittedvalues\n",
        "\n",
        "# 1) Levene’s test: compare variance of residuals across bins of fitted values\n",
        "# (more standard than a single median split)\n",
        "quantiles = np.quantile(fitted, [0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "groups = [\n",
        "    resid[(fitted >= quantiles[i]) & (fitted < quantiles[i + 1])]\n",
        "    for i in range(len(quantiles) - 1)\n",
        "]\n",
        "# Keep only non-empty groups\n",
        "groups = [g for g in groups if len(g) > 0]\n",
        "lev_stat, lev_pvalue = levene(*groups, center=\"median\")\n",
        "print(f\"Levene statistic: {lev_stat:.4f}, p-value: {lev_pvalue:.4g}\")\n",
        "\n",
        "\n",
        "# 2) Breusch–Pagan test (as you had; this is the standard usage)\n",
        "bp_stat, bp_pvalue, bp_f, bp_f_pvalue = het_breuschpagan(resid, exog)\n",
        "print(\n",
        "    f\"Breusch–Pagan: LM={bp_stat:.4f}, p={bp_pvalue:.4g}, \"\n",
        "    f\"F={bp_f:.4f}, F p={bp_f_pvalue:.4g}\"\n",
        ")\n",
        "\n",
        "\n",
        "# 3) White test (general heteroscedasticity, also in statsmodels)\n",
        "w_stat, w_pvalue, w_f, w_f_pvalue = het_white(resid, exog)\n",
        "print(\n",
        "    f\"White test: LM={w_stat:.4f}, p={w_pvalue:.4g}, \"\n",
        "    f\"F={w_f:.4f}, F p={w_f_pvalue:.4g}\"\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx0mQG4zjXSp"
      },
      "outputs": [],
      "source": [
        "# Component-Residual Plot (Partial Residual Plots)\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "sm.graphics.plot_ccpr_grid(reduced_model, fig=fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P-DLFC3jU1z"
      },
      "outputs": [],
      "source": [
        "# Added Variable Plot (Partial Regression Plots)\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "plot_partregress_grid(reduced_model, fig=fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR2YDzBFkV8Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_regression_diagnostics(model):\n",
        "    \"\"\"\n",
        "    Generate diagnostic plots for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the diagnostic plots.\n",
        "    \"\"\"\n",
        "\n",
        "    #residuals = model.resid\n",
        "    residuals = model.get_influence().resid_studentized  # internal studentized residuals\n",
        "\n",
        "    num_regressors = len(model.model.exog_names) - 1  # Exclude intercept\n",
        "    total_plots = num_regressors + 4  # Total plots needed (1 plot per regressor + 4 diagnostics)\n",
        "    rows = (total_plots + 2) // 3  # Calculate rows needed to fit all plots in 3 columns\n",
        "\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
        "    axes = axes.flatten()  # Flatten to iterate easily\n",
        "\n",
        "    # Plot Fitted Values vs Residuals\n",
        "    axes[0].scatter(model.fittedvalues, residuals)\n",
        "    axes[0].axhline(0, color='red', linestyle='--')\n",
        "    axes[0].set_xlabel('Fitted Values')\n",
        "    axes[0].set_ylabel('Studentized residuals')\n",
        "    axes[0].set_title('Fitted Values vs Residuals')\n",
        "\n",
        "    # Plot Response vs Residuals for each regressor\n",
        "    for i, col in enumerate(model.model.exog_names[1:], start=1):\n",
        "        ax = axes[i]\n",
        "        ax.scatter(model.model.exog[:, i], residuals)\n",
        "        ax.axhline(0, color='red', linestyle='--')\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Studentized residuals')\n",
        "        ax.set_title(f'Response vs Residuals: {col}')\n",
        "\n",
        "    # Normal Q-Q plot\n",
        "    sm.qqplot(residuals, line='s', ax=axes[num_regressors + 1])\n",
        "    axes[num_regressors + 1].set_title('Normal Q-Q')\n",
        "\n",
        "    # Scale-Location plot\n",
        "    axes[num_regressors + 2].scatter(model.fittedvalues, np.sqrt(np.abs(residuals)))\n",
        "    axes[num_regressors + 2].axhline(0, color='red', linestyle='--')\n",
        "    axes[num_regressors + 2].set_xlabel('Fitted Values')\n",
        "    axes[num_regressors + 2].set_ylabel('|sqrt(Studentized residuals)|')\n",
        "    axes[num_regressors + 2].set_title('Scale-Location')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_regressors + 3, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Generate the diagnostic plots\n",
        "fig0 = plot_regression_diagnostics(reduced_model)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Tkavk-qpj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIB-RkaB-qpk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oahPGUXbjucI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP3lCnjijugA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-OKLGmjBa57"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.regressionplots import plot_regress_exog\n",
        "\n",
        "# Check residuals against each independent variable using plot_regress_exog\n",
        "key_predictors = ['consumption', 'WheelBase', 'Width']\n",
        "\n",
        "for predictor in key_predictors:\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    plot_regress_exog(reduced_model, predictor, fig=fig)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYDF1GRlBa_n"
      },
      "outputs": [],
      "source": [
        "# Extract predictors from the reduced model\n",
        "# predictors_stepwise = reduced_model.model.exog_names\n",
        "# predictors_stepwise.remove('Intercept')  # Remove the intercept from the list\n",
        "# predictors_stepwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRoBxFL9IJQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MX-7OQXIJTp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}