{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO8BXZXtfwCN8PthDA43r0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01RAD/blob/main/python/01RAD_Ex09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr8RRFaEi5DF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#current_directory = os.getcwd()\n",
        "#print(\"Current Working Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "jNtjzydklwfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed for random number generation using NumPy\n",
        "np.random.seed(4242)"
      ],
      "metadata": {
        "id": "LAfGc-CrlwiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample size and number of predictors\n",
        "n = 100\n",
        "p = 4\n",
        "\n",
        "# Generating the error term\n",
        "e = np.random.normal(0, 4, n)\n",
        "\n",
        "# Defining the beta coefficients\n",
        "beta0 = np.array([5, 3, 2, -5]).reshape(4, 1)\n",
        "\n",
        "# Creating the variables\n",
        "X0 = np.ones(n)\n",
        "X1 = np.random.normal(20, 3, n)\n",
        "X2 = 10 + np.random.exponential(1/0.1, n)\n",
        "X3 = 5 + np.random.binomial(15, 0.2, n)\n",
        "\n",
        "# Calculating Y\n",
        "Y = np.dot(np.column_stack((X0, X1, X2, X3)), beta0).flatten() + e\n",
        "\n",
        "# Creating the data frame\n",
        "data0 = pd.DataFrame({'X0': X0, 'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
        "\n",
        "# Displaying the first few rows and summary of the data frame\n",
        "print(data0.head())\n",
        "print(data0.describe())\n"
      ],
      "metadata": {
        "id": "iqIx9kgUlwkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoN5OK3ilwmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the variables\n",
        "X = data0[['X1', 'X2', 'X3']]\n",
        "Y = data0['Y']\n",
        "\n",
        "# Visualization\n",
        "sns.pairplot(data0, vars=['X1', 'X2', 'X3', 'Y'])"
      ],
      "metadata": {
        "id": "Nu2lpBT1lwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsgFqZ0IlwrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the linear regression model\n",
        "model = smf.ols('Y ~ X1 + X2 + X3', data=data0).fit()\n",
        "\n",
        "# Displaying the summary of the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "2tjG51PHnG-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X = sm.add_constant(X)  # Add an intercept term to the predictor variables\n",
        "#model = sm.OLS(Y, X)  # Create the model object\n",
        "#model_results = model.fit()\n"
      ],
      "metadata": {
        "id": "OohwzqGHo-8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regression_diagnostics(model):\n",
        "    \"\"\"\n",
        "    Generate diagnostic plots for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the diagnostic plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot of Fitted Values vs Residuals\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.scatter(model.fittedvalues, model.resid)\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Fitted Values vs Residuals')\n",
        "\n",
        "    # Response vs Residuals for each regressor\n",
        "    for i, col in enumerate(model.model.exog_names[1:], 2):\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(model.model.exog[:, i - 1], model.resid)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'Response vs Residuals: {col}')\n",
        "\n",
        "    # Normal Q-Q plot\n",
        "    plt.subplot(2, 3, 5)\n",
        "    sm.qqplot(model.resid, line='s', ax=plt.gca())\n",
        "    plt.title('Normal Q-Q')\n",
        "\n",
        "    # Scale-Location plot\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.scatter(model.fittedvalues, np.sqrt(np.abs(model.resid)))\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel('Standardized Residuals')\n",
        "    plt.title('Scale-Location')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "hzlbhebVogSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig0 = plot_regression_diagnostics(model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOVN63FWogVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_ccpr_grid\n",
        "\n",
        "def plot_component_residuals(model):\n",
        "    \"\"\"\n",
        "    Generate Component-Residual Plots (Partial Residual Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Component-Residual Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_ccpr_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "3jjzv6P3ogXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
        "\n",
        "def plot_added_variable(model):\n",
        "    \"\"\"\n",
        "    Generate Added Variable Plots (Partial Regression Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Added Variable Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_partregress_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "0MYaaSX_ogZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig1 = plot_component_residuals(model)\n",
        "plt.show()\n",
        "\n",
        "fig2 = plot_added_variable(model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oQrD5rXAoeW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residuals in Linear Regression\n",
        "\n",
        "In linear regression, residuals are the differences between the observed values and the values predicted by the model.\n",
        "\n",
        "### 1. Classical (Ordinary) Residuals\n",
        "Classical residuals are calculated as the difference between the observed values and the predicted values from the regression model.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "e_i = y_i - \\hat{y}_i\n",
        "$$\n",
        "where \\( e_i \\) is the residual for the $i $th observation, $ y_i $ is the observed value, and $\\hat{y}_i $ is the predicted value by the model.\n",
        "\n",
        "### 2. Standardized Residuals\n",
        "Standardized residuals are classical residuals scaled by an estimate of their standard deviation.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "r_i = \\frac{e_i}{\\hat{\\sigma} \\sqrt{1 - h_{ii}} }\n",
        "$$\n",
        "where \\( r_i \\) is the standardized residual, $ \\hat{\\sigma} $ is the estimated standard deviation of the residuals, and $ h_{ii} $ is the leverage of the $i $-th observation.\n",
        "\n",
        "### 3. Studentized Residuals\n",
        "Studentized residuals are similar to standardized residuals, but they are scaled using a more robust estimate of the standard deviation, one that excludes the \\( i \\)-th observation.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)} \\sqrt{1 - h_{ii}} }\n",
        "$$\n",
        "where \\( t_i \\) is the studentized residual, $ \\hat{\\sigma}_{(i)} $ is the estimated standard deviation of the residuals excluding the $ i $-th observation.\n",
        "\n"
      ],
      "metadata": {
        "id": "UQwPxep1ssv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_classical_residuals(model, data):\n",
        "    \"\"\"\n",
        "    Calculate classical (ordinary) residuals for a regression model.\n",
        "    \"\"\"\n",
        "    observed = data[model.endog_names]\n",
        "    predicted = model.predict()\n",
        "    classical_residuals = observed - predicted\n",
        "    return classical_residuals\n",
        "\n",
        "def manual_classical_residuals(observed, predicted):\n",
        "    return observed - predicted"
      ],
      "metadata": {
        "id": "2FmtF2_Fru5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_standardized_residuals(model):\n",
        "    \"\"\"\n",
        "    Calculate standardized residuals for a regression model.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "    standardized_residuals = influence.resid_studentized_internal\n",
        "    return standardized_residuals\n",
        "\n",
        "def manual_standardized_residuals(observed, predicted, leverage):\n",
        "    residuals = observed - predicted\n",
        "    residual_std = np.sqrt(np.sum(residuals**2) / (len(residuals) - 2))\n",
        "    return residuals / (residual_std * np.sqrt(1 - leverage))\n"
      ],
      "metadata": {
        "id": "4S64ed55ru77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_studentized_residuals(model):\n",
        "    \"\"\"\n",
        "    Calculate studentized residuals for a regression model.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "    studentized_residuals = influence.resid_studentized_external\n",
        "    return studentized_residuals\n",
        "\n",
        "def manual_studentized_residuals(observed, predicted, leverage):\n",
        "    residuals = observed - predicted\n",
        "    studentized_res = np.zeros_like(residuals)\n",
        "\n",
        "    for i in range(len(residuals)):\n",
        "        # Exclude the i-th residual\n",
        "        residuals_without_i = np.delete(residuals, i)\n",
        "        std_without_i = np.sqrt(np.sum(residuals_without_i**2) / (len(residuals_without_i) - 2))\n",
        "        studentized_res[i] = residuals[i] / (std_without_i * np.sqrt(1 - leverage[i]))\n",
        "\n",
        "    return studentized_res\n"
      ],
      "metadata": {
        "id": "fpdKW7pEru-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the influence object\n",
        "influence = model.get_influence()\n",
        "\n",
        "# Extract leverage values\n",
        "leverage = influence.hat_matrix_diagobserved = np.array([...])  # Replace with your observed values\n",
        "observed = data0.Y\n",
        "predicted = model.predict()\n",
        "\n",
        "# Calculate residuals\n",
        "classical_residuals = manual_classical_residuals(observed, predicted)\n",
        "#standardized_residuals = manual_standardized_residuals(observed, predicted, leverage)\n",
        "#studentized_residuals = manual_studentized_residuals(observed, predicted, leverage)\n"
      ],
      "metadata": {
        "id": "fzuDDZC8rvBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data0"
      ],
      "metadata": {
        "id": "53joM4v_rvEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S2y3o6gOtwm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Influence Measures in Linear Regression\n",
        "\n",
        "In linear regression, influence measures are used to identify observations that have a disproportionate impact on the model. These measures help in diagnosing the model's robustness and identifying outliers or influential points. Below are key influence measures commonly used:\n",
        "\n",
        "### 1. DFBETAS\n",
        "DFBETAS measures the difference in each coefficient estimate when an observation is omitted.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFBETAS_{ij} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{\\hat{\\sigma}^2_{(i)} (X^T X)^{-1}_{jj}}}\n",
        "$$\n",
        "where $ \\hat{\\beta}_j $ is the estimated coefficient, $ \\hat{\\beta}_{j(i)} $ is the estimated coefficient with the \\( i \\)-th observation omitted, and $(X^T X)^{-1}_{jj} $ is the \\( j \\)-th diagonal element of the inverse of $X^T X $.\n",
        "\n",
        "### 2. DFFITS\n",
        "DFFITS is an influence statistic that measures the effect of deleting a single observation.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}_{(i)} \\sqrt{h_{ii}}}\n",
        "$$\n",
        "where \\( \\hat{y}_i \\) is the predicted value with all observations, $ \\hat{y}_{i(i)}$is the predicted value with the \\( i \\)-th observation omitted, and $ h_{ii} $is the leverage of the $ i $-th observation.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "Leverage values measure the influence of each observation on its own fitted value. High leverage points can significantly alter the position of the regression line.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "h_{ii} = X_i (X^T X)^{-1} X_i^T\n",
        "$$\n",
        "where $X_i$ is the \\( i \\)-th row of the matrix of predictors \\( X \\).\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "Covariance ratios compare the determinants of the covariance matrices with and without each observation. They help identify observations that influence the variance of the parameter estimates.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "CR_i = \\frac{\\det(\\hat{\\Sigma}_{(i)})}{\\det(\\hat{\\Sigma})}\n",
        "$$\n",
        "where $ \\hat{\\Sigma}_{(i)} $is the covariance matrix with the \\( i \\)-th observation omitted and $ \\hat{\\Sigma} $ is the covariance matrix with all observations.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "Cook's distance measures the effect of deleting a single observation on the entire regression model. It is a commonly used metric to identify influential observations.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2}\n",
        "$$\n",
        "where $ \\hat{y}_j $ is the predicted value for the $ j $-th observation, $ \\hat{y}_{j(i)} $ is the predicted value with the \\( i \\)-th observation omitted, \\( p \\) is the number of predictors, and $ \\hat{\\sigma}^2 $ is the estimated variance of the residuals.\n",
        "\n"
      ],
      "metadata": {
        "id": "23HSszBstyFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\n",
        "\n",
        "### 1. DFBETAS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFBETAS for any coefficient exceeds $ \\frac{2}{\\sqrt{n}} $, where $ n $ is the number of observations.\n",
        "\n",
        "### 2. DFFITS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFFITS is larger than $ 2 \\sqrt{\\frac{p+1}{n}} $, where \\( p \\) is the number of predictors and \\( n \\) is the number of observations.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "\n",
        "**Rule of Thumb:** An observation is considered to have high leverage if its leverage value exceeds $ \\frac{2(p+1)}{n} $, where \\( p \\) is the number of predictors and \\( n \\) is the number of observations.\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "\n",
        "**Rule of Thumb:** There is no widely accepted rule of thumb for covariance ratios, but observations with values far from 1 (either much larger or smaller) are generally considered influential.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if its Cook's distance is greater than$ \\frac{4}{n} $, where \\( n \\) is the number of observations.\n"
      ],
      "metadata": {
        "id": "DporrqiPuZY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_influence_dataframe(model):\n",
        "    \"\"\"\n",
        "    Create a DataFrame with influence statistics for each observation in the model.\n",
        "    The DataFrame includes DFFITS, DFBETAS, Leverage Values (h values), Covariance Ratios, and Cook's Distances.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extracting the influence measures\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    covariance_ratios = influence.cov_ratio\n",
        "    cooks_distances = influence.cooks_distance[0]\n",
        "\n",
        "    # Creating the DataFrame\n",
        "    influence_df = pd.DataFrame({\n",
        "        'DFFITS': dffits,\n",
        "        'Leverage': leverage,\n",
        "        'Covariance Ratio': covariance_ratios,\n",
        "        'Cook\\'s Distance': cooks_distances\n",
        "    })\n",
        "\n",
        "    # Adding DFBETAS columns for each predictor\n",
        "    for i in range(dfbetas.shape[1]):\n",
        "        influence_df[f'DFBETA_{i}'] = dfbetas[:, i]\n",
        "\n",
        "    return influence_df\n",
        "\n",
        "influence_df = create_influence_dataframe(model)\n",
        "print(influence_df)\n"
      ],
      "metadata": {
        "id": "y7JI4lfrrA8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_influence_dataframe_with_violations(model):\n",
        "    \"\"\"\n",
        "    Create a DataFrame with influence statistics for each observation in the model.\n",
        "    Additionally, include a column that lists the names of the statistics where the rule of thumb is violated.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extracting the influence measures\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    covariance_ratios = influence.cov_ratio\n",
        "    cooks_distances = influence.cooks_distance[0]\n",
        "\n",
        "    # Rules of thumb\n",
        "    n = model.nobs\n",
        "    p = model.df_model\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Creating the DataFrame\n",
        "    influence_df = pd.DataFrame({\n",
        "        'DFFITS': dffits,\n",
        "        'Leverage': leverage,\n",
        "        'Covariance Ratio': covariance_ratios,\n",
        "        'Cook\\'s Distance': cooks_distances\n",
        "    })\n",
        "\n",
        "    # Adding DFBETAS columns for each predictor\n",
        "    for i in range(dfbetas.shape[1]):\n",
        "        influence_df[f'DFBETA_{i}'] = dfbetas[:, i]\n",
        "\n",
        "    # Identifying violations of rules of thumb\n",
        "    violations = []\n",
        "    for index, row in influence_df.iterrows():\n",
        "        violated_stats = []\n",
        "        if abs(row['DFFITS']) > dffits_threshold:\n",
        "            violated_stats.append('DFFITS')\n",
        "        if row['Leverage'] > leverage_threshold:\n",
        "            violated_stats.append('Leverage')\n",
        "        if row['Cook\\'s Distance'] > cooks_distance_threshold:\n",
        "            violated_stats.append('Cook\\'s Distance')\n",
        "        for i in range(dfbetas.shape[1]):\n",
        "            if abs(row[f'DFBETA_{i}']) > dfbetas_threshold:\n",
        "                violated_stats.append(f'DFBETA_{i}')\n",
        "        violations.append(', '.join(violated_stats))\n",
        "\n",
        "    influence_df['Violations'] = violations\n",
        "\n",
        "    return influence_df\n",
        "\n",
        "influence_df_with_violations = create_influence_dataframe_with_violations(model)\n",
        "print(influence_df_with_violations.head())\n"
      ],
      "metadata": {
        "id": "mDJXno1wrGyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQKEHa-PvBsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49XBpPOfvBvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNEfSphovByB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrXowG6lvB0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_dfbetas(X, y, betas, sigma):\n",
        "    n, p = X.shape\n",
        "    dfbetas = np.zeros((n, p))\n",
        "\n",
        "    for i in range(n):\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "\n",
        "        for j in range(p):\n",
        "            dfbetas[i, j] = (betas[j] - betas_exclude_i[j]) / (sigma * np.sqrt(np.linalg.inv(X.T @ X)[j, j]))\n",
        "\n",
        "    return dfbetas\n"
      ],
      "metadata": {
        "id": "W4S2tm7SvB3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_dffits(X, y, y_hat, sigma):\n",
        "    n = X.shape[0]\n",
        "    dffits = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "        y_hat_exclude_i = np.delete(y_hat, i)\n",
        "        y_hat_new_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i) @ X[i]\n",
        "\n",
        "        dffits[i] = (y_hat[i] - y_hat_new_i) / (sigma * np.sqrt(np.linalg.inv(X.T @ X)[i, i]))\n",
        "\n",
        "    return dffits\n"
      ],
      "metadata": {
        "id": "WRmaGniMvB5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_leverage(X):\n",
        "    H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
        "    leverage = np.diag(H)\n",
        "    return leverage\n"
      ],
      "metadata": {
        "id": "JlZdGE9TvB8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_cooks_distances(X, y, y_hat, sigma):\n",
        "    n = X.shape[0]\n",
        "    p = X.shape[1]\n",
        "    cooks_d = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "        y_hat_exclude_i = np.delete(y_hat, i)\n",
        "        y_hat_new = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i) @ X\n",
        "\n",
        "        cooks_d[i] = np.sum((y_hat_exclude_i - y_hat_new) ** 2) / (p * sigma**2)\n",
        "\n",
        "    return cooks_d\n"
      ],
      "metadata": {
        "id": "VJDzfT8kvFnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the display option to show all rows (or a specified large number)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "pNpQHJSVvWwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_influence_dataframe_with_violations(model):\n",
        "    \"\"\"\n",
        "    Create a DataFrame with influence statistics for each observation in the model.\n",
        "    Additionally, include a column that lists the names of the statistics where the rule of thumb is violated.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extracting the influence measures\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    covariance_ratios = influence.cov_ratio\n",
        "    cooks_distances = influence.cooks_distance[0]\n",
        "\n",
        "    # Rules of thumb\n",
        "    n = model.nobs\n",
        "    p = model.df_model\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Creating the DataFrame\n",
        "    influence_df = pd.DataFrame({\n",
        "        'DFFITS': dffits,\n",
        "        'Leverage': leverage,\n",
        "        'Covariance Ratio': covariance_ratios,\n",
        "        'Cook\\'s Distance': cooks_distances\n",
        "    })\n",
        "\n",
        "    # Adding DFBETAS columns for each predictor\n",
        "    for i in range(dfbetas.shape[1]):\n",
        "        influence_df[f'DFBETA_{i}'] = dfbetas[:, i]\n",
        "\n",
        "    # Identifying violations of rules of thumb\n",
        "    violations = []\n",
        "    for index, row in influence_df.iterrows():\n",
        "        violated_stats = []\n",
        "        if abs(row['DFFITS']) > dffits_threshold:\n",
        "            violated_stats.append('DFFITS')\n",
        "        if row['Leverage'] > leverage_threshold:\n",
        "            violated_stats.append('Leverage')\n",
        "        if row['Cook\\'s Distance'] > cooks_distance_threshold:\n",
        "            violated_stats.append('Cook\\'s Distance')\n",
        "        for i in range(dfbetas.shape[1]):\n",
        "            if abs(row[f'DFBETA_{i}']) > dfbetas_threshold:\n",
        "                violated_stats.append(f'DFBETA_{i}')\n",
        "        violations.append(', '.join(violated_stats))\n",
        "\n",
        "    influence_df['Violations'] = violations\n",
        "\n",
        "    return influence_df\n",
        "\n",
        "influence_df_with_violations = create_influence_dataframe_with_violations(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "xHuD61M6vFqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "influence_df_with_violations\n"
      ],
      "metadata": {
        "id": "lkrnqDxHvFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mPQPSe1vB-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compare_influence_measures(model, X, y):\n",
        "#     \"\"\"\n",
        "#     Compare manually calculated influence measures with those from statsmodels' built-in functions.\n",
        "\n",
        "#     :param model: The fitted regression model.\n",
        "#     :param X: Design matrix (predictor variables).\n",
        "#     :param y: Response variable.\n",
        "#     :return: DataFrame comparing manual and built-in influence measures.\n",
        "#     \"\"\"\n",
        "#     # Manually calculate influence measures\n",
        "#     y_hat = model.predict(X)\n",
        "#     sigma = np.sqrt(np.sum((y - y_hat) ** 2) / (len(y) - X.shape[1] - 1))\n",
        "#     betas = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "\n",
        "#     manual_dfbetas = manual_dfbetas(X, y, betas, sigma)\n",
        "#     manual_dffits = manual_dffits(X, y, y_hat, sigma)\n",
        "#     manual_leverage = manual_leverage(X)\n",
        "#     manual_cooks_d = manual_cooks_distances(X, y, y_hat, sigma)\n",
        "\n",
        "#     # Create DataFrame for manual calculations\n",
        "#     manual_df = pd.DataFrame({\n",
        "#         'Manual_DFBETAS': np.max(np.abs(manual_dfbetas), axis=1),\n",
        "#         'Manual_DFFITS': np.abs(manual_dffits),\n",
        "#         'Manual_Leverage': manual_leverage,\n",
        "#         'Manual_Cooks_Distance': manual_cooks_d\n",
        "#     })\n",
        "\n",
        "#     # Use built-in functions to get influence measures\n",
        "#     built_in_df = create_influence_dataframe_with_violations(model)\n",
        "\n",
        "#     # Combine the DataFrames for comparison\n",
        "#     comparison_df = pd.concat([manual_df, built_in_df], axis=1)\n",
        "\n",
        "#     return comparison_df\n",
        "\n",
        "# comparison_df = compare_influence_measures(model, X, Y)\n",
        "# print(comparison_df.head())\n"
      ],
      "metadata": {
        "id": "SJEQn2N1twsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcjnabUPtwvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a good outlying point to predictors\n",
        "outlier = pd.DataFrame({'X1': [max(data0['X1']) + 25],\n",
        "                        'X2': [max(data0['X2']) + 35],\n",
        "                        'X3': [max(data0['X3']) + 25]})\n",
        "X_with_outlier = pd.concat([data0[['X1', 'X2', 'X3']], outlier], ignore_index=True)\n",
        "\n",
        "# Recalculating Y with the new outlying point\n",
        "# Create the design matrix for the model including the intercept\n",
        "X_design = sm.add_constant(X_with_outlier)\n",
        "# Calculate Y values including the outlier\n",
        "Y_with_outlier = np.dot(X_design, beta0).flatten() + np.append(e, np.random.normal(0, 4))\n",
        "\n",
        "# Simple Regression - only X2 as independent variable\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_with_outlier['X2'], Y_with_outlier)\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression with at least one influential point')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lEFSC3nbx5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UuzUcHYyJuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}